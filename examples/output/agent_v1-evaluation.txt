Question ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
Gold:
	BIBREF19, BIBREF20
	multilingual NMT (MNMT) BIBREF19
Predicted:
	pivoting and other multilingual approaches
Answer F1:
	0.22222222222222224
Precision:
	0.2
Recall:
	0.25

Question ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
Gold:
	pivoting, pivoting$_{\rm m}$
	firstly translates a source language into the pivot language which is later translated to the target language
Predicted:
	The pivot-based baselines information is not available in the paper with arxiv identifier 1912.01214.
Answer F1:
	0.07692307692307691
Precision:
	0.08333333333333333
Recall:
	0.07142857142857142

Question ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
Gold:
	Europarl, MultiUN
	Europarl BIBREF31, MultiUN BIBREF32
Predicted:
	Europarl and Multi-UN evaluation sets
Answer F1:
	0.5714285714285715
Precision:
	0.4
Recall:
	1.0

Question ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
Gold:
	De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru
	French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation
Predicted:
	Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenarios. The paper explores methods involving a common target language and proposes an effective transfer learning approach based on cross-lingual pre-training. It introduces monolingual and bilingual pre-training methods to obtain a universal encoder for different languages, leading to improved performance in zero-shot translation scenarios.
Answer F1:
	0.15
Precision:
	0.10526315789473684
Recall:
	0.2608695652173913

Question ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
Gold:
	Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer
	Stanford NER, spaCy 2.0, recurrent model with a CRF top layer
Predicted:
	The NER models evaluated in the paper with arxiv identifier 1810.08699 were not explicitly mentioned in the text.
Answer F1:
	0.16
Precision:
	0.13333333333333333
Recall:
	0.2

Question ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
Gold:
	ilur.am
	links between Wikipedia articles to generate sequences of named-entity annotated tokens
Predicted:
	The source of the news sentences in the paper with arxiv identifier 1810.08699 is not explicitly mentioned in the paper.
Answer F1:
	0.07407407407407407
Precision:
	0.0625
Recall:
	0.09090909090909091

Question ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
Gold:
	No
	No
Predicted:
	No, they did not use a crowdsourcing platform for manual annotations.
Answer F1:
	0.18181818181818182
Precision:
	0.1
Recall:
	1.0

Question ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
Gold:
	politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. 
	training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit
Predicted:
	The topics pulled from Reddit are not explicitly mentioned in the paper with arxiv identifier 1609.00425.
Answer F1:
	0.15384615384615383
Precision:
	0.14285714285714285
Recall:
	0.16666666666666666

Question ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
Gold:
	logistic regression models
	logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.
Predicted:
	They build a predictive model to identify dogmatic comments in social media.
Answer F1:
	0
Precision:
	0
Recall:
	0

Question ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
Gold:
	F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data 
	F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)
Predicted:
	The proposed system achieves better scores than strong baseline systems in the paper with arxiv identifier 1801.05147, but the specific accuracy value is not mentioned.
Answer F1:
	0.052631578947368425
Precision:
	0.045454545454545456
Recall:
	0.0625

========================================
Evaluation metrics:
{
    "Answer F1": 0.16429438592596488,
    "Answer F1 by type": {
        "extractive": 0.17606176231176232,
        "abstractive": 0.052631578947368425,
        "boolean": 0.18181818181818182,
        "none": 0.0
    },
    "Missing predictions": 0,
    "Freq of types": {
        "extractive": 8,
        "abstractive": 1,
        "boolean": 1,
        "none": 0
    }
}
