{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "question": "How big is the ANTISCAM dataset? ", "references": [" 3,044 sentences in 100 dialogs", "220 human-human dialogs", "220 human-human dialogs. , 3,044 sentences in 100 dialogs", "220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. ", "220 human-human dialogs", "3,044 sentences in 100 dialogs"], "best_model_answer": " 3,044 sentences in 100 dialogs", "oracle_answer": "220 human-human dialogs. , 3,044 sentences in 100 dialogs", "oracle_answer_rank": 3}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "question": "How is intent annotated?", "references": ["using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations", "Separate on-task and off task intents and annotate on task for data set specific intents, while annotating  off task intents with a fixed set of general intents.", "On-task dialog are annotated as on-task intents , the other dialog are annotated as pre-defined off-task intents.", "separate on-task and off-task intents, on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task, off-task content is too general to design task-specific intents, we choose common dialog acts as the categories", "we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. , In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme, For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns.", "using a hierarchical scheme where on-task intents uses task-related intents for representation and off-task intents chooses dialog acts that convey the syntax information"], "best_model_answer": "using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations", "oracle_answer": "separate on-task and off-task intents, on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task, off-task content is too general to design task-specific intents, we choose common dialog acts as the categories", "oracle_answer_rank": 4}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "question": "What are the baselines outperformed by this work?", "references": ["TransferTransfo and Hybrid ", "TransferTransfo,  hybrid model", "TransferTransfo, Hybrid", "TransferTransfo, Hybrid", "TransferTransfo The vanilla TransferTransfo framework, Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA", "TransferTransfo, Hybrid"], "best_model_answer": "TransferTransfo and Hybrid ", "oracle_answer": "TransferTransfo, Hybrid", "oracle_answer_rank": 3}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "question": "What are the evaluation metrics and criteria used to evaluate the model performance?", "references": ["Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP) , Extended Response-Slot Prediction (ERSP) , Fluency, Coherence , Engagement, Dialog length , Task Success Score (TaskSuc)", "Perplexity , Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), Extended Response-Slot Prediction (ERSP), Fluency , Coherence , Engagement , Dialog length (Length) , Task Success Score (TaskSuc)", "Fluency Fluency is used to explore different models' language generation quality.\n\nCoherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\n\nEngagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.\n\nDialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\n\nTask Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.", "Automatic evaluation metrics (Perplexity (PPl), Response-Intent Prediction (RIP), Response-Slot Prediction(RSP), Extended Response-Intent Prediction(ERIP),  Extended Response-Slot Prediction (ERSP)) and Human Evaluation Metrics (Fluency, Coherence, Engagement, Lenhth, TaskSuc)", "Automatic metrics used: Perplexity, RIP, RSP, ERIP ERSP.\nHuman evaluation metrics used: Fluency, Coherence, Engagement, Dialog length and Task Success Score."], "best_model_answer": "Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP) , Extended Response-Slot Prediction (ERSP) , Fluency, Coherence , Engagement, Dialog length , Task Success Score (TaskSuc)", "oracle_answer": "Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP) , Extended Response-Slot Prediction (ERSP) , Fluency, Coherence , Engagement, Dialog length , Task Success Score (TaskSuc)", "oracle_answer_rank": 1}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "question": "What is the accuracy of this model compared to sota?", "references": ["Unanswerable", "The model improves the state of the art performance for the ISTEX dataset (F1 micro: 0.870, F1 macro: 0.858) and for the Microposts 2016 dataset (F1 micro:  0.087).", "The micro and macro f1-scores of this model are 0.482 and 0.399 on the AIDA-CoNLL dataset, 0.087 and 0.515 on the Microposts 2016 dataset, 0.870 and 0.858 on the ISTEX-1000 dataset, 0.335 and 0.310 on the RSS-500 dataset", "The accuracy ", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "question": "What previous methods do they compare against?", "references": ["two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented., Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter.", "Liu et. al (2015), Yang et. al (2012)", "They compare against two other methods that apply message-,user-, topic- and propagation-based features and rely on an SVM classifier. One perform early rumor detection and operates with a delay of 24 hrs, while the other requires a cluster of 5 repeated messages to judge them for rumors.", "Liu et. al (2015) , Yang et. al (2012)", "Liu et al. (2015) and Yang et al. (2012)"], "best_model_answer": "two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented., Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter.", "oracle_answer": "Liu et al. (2015) and Yang et al. (2012)", "oracle_answer_rank": 5}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "question": "What is their evaluation metric?", "references": ["accuracy to evaluate effectiveness, Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability, throughput per second", "The metrics are accuracy, detection error trade-off curves and computing efficiency", "accuracy , Detection Error Trade-off (DET) curves, efficiency of computing the proposed features, measured by the throughput per second", "accuracy to evaluate effectiveness, Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability, throughput per second", "Accuracy compared to two state-of-the-art baselines"], "best_model_answer": "accuracy to evaluate effectiveness, Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability, throughput per second", "oracle_answer": "accuracy to evaluate effectiveness, Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability, throughput per second", "oracle_answer_rank": 1}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "question": "Do they build a dataset of rumors?", "references": ["Yes", "Yes", "Yes", "Yes, consisting of trusted resources, rumours and non-rumours", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "question": "What languages do they evaluate their methods on?", "references": ["Chinese", "Mandarin Chinese", "Chinese", "Mandarin Chinese (see table 3)", "Chinese"], "best_model_answer": "Chinese", "oracle_answer": "Chinese", "oracle_answer_rank": 1}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "question": "How do they define rumors?", "references": ["the presence of information unconfirmed by the official media is construed as an indication of being a rumour. ", "information of doubtful or unconfirmed truth", "information that is not fact- and background-checked and thoroughly investigated for authenticity", "Information of doubtful or unconfirmed truth"], "best_model_answer": "the presence of information unconfirmed by the official media is construed as an indication of being a rumour. ", "oracle_answer": "Information of doubtful or unconfirmed truth", "oracle_answer_rank": 4}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "question": "What baselines did they compare with?", "references": ["LDA, Doc-NADE, HTMM, GMNTM", "LDA, Doc-NADE, HTMM, GMNTM", "LDA BIBREF2, Doc-NADE BIBREF24, HTMM BIBREF9, GMNTM BIBREF12", "LDA BIBREF2 , Doc-NADE BIBREF24, HTMM BIBREF9, GMNTM BIBREF12, LDA BIBREF2 , Doc-NADE BIBREF24, HTMM BIBREF9 , GMNTM BIBREF12"], "best_model_answer": "LDA, Doc-NADE, HTMM, GMNTM", "oracle_answer": "LDA, Doc-NADE, HTMM, GMNTM", "oracle_answer_rank": 1}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "question": "Which tasks are explored in this paper?", "references": ["generative model evaluation (i.e. test set perplexity) and document classification", "generative model evaluation, document classification", "generative model evaluation (i.e. test set perplexity), document classification", "generative document evaluation task, document classification task, topic2sentence task"], "best_model_answer": "generative model evaluation (i.e. test set perplexity) and document classification", "oracle_answer": "generative model evaluation (i.e. test set perplexity), document classification", "oracle_answer_rank": 3}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "question": "Which NER dataset do they use?", "references": ["CoNLL2003, OntoNotes 5.0, OntoNotes 4.0., Chinese NER dataset MSRA, Weibo NER, Resume NER", "CoNLL2003 , OntoNotes 5.0, OntoNotes 4.0, MSRA , Weibo, Resume ", "CoNLL2003, OntoNotes 5.0, OntoNotes 4.0, MSRA, Weibo NER, Resume NER", "CoNLL2003, OntoNotes 5.0, BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part, Chinese NER dataset MSRA, Weibo NER, Resume NER"], "best_model_answer": "CoNLL2003, OntoNotes 5.0, OntoNotes 4.0., Chinese NER dataset MSRA, Weibo NER, Resume NER", "oracle_answer": "CoNLL2003, OntoNotes 5.0, OntoNotes 4.0, MSRA, Weibo NER, Resume NER", "oracle_answer_rank": 3}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "question": "How do they incorporate direction and relative distance in attention?", "references": ["by using an relative sinusodial positional embedding and unscaled attention", "Yes", "calculate the attention scores  which can  distinguish different directions and distances", "Self-attention mechanism is changed to allow for direction-aware calculations"], "best_model_answer": "by using an relative sinusodial positional embedding and unscaled attention", "oracle_answer": "by using an relative sinusodial positional embedding and unscaled attention", "oracle_answer_rank": 1}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "question": "Do they outperform current NER state-of-the-art models?", "references": ["No", "Yes", "Yes", "we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features"], "best_model_answer": "No", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "question": "What was their accuracy score?", "references": ["95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset", "KALM achieves an accuracy of 95.6%, KALM-QA achieves 100% accuracy", "KALM-QA achieves an accuracy of 95% for parsing the queries, The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy", "KALM achieves an accuracy of 95.6%, KALM-QA achieves 95% accuracy on the manually constructured general questions dataset based on the 50 logical frames and achieves 100% accuracy on MetaQA dataset"], "best_model_answer": "95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset", "oracle_answer": "KALM achieves an accuracy of 95.6%, KALM-QA achieves 95% accuracy on the manually constructured general questions dataset based on the 50 logical frames and achieves 100% accuracy on MetaQA dataset", "oracle_answer_rank": 4}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "question": "What are the state-of-the-art systems?", "references": ["SEMAFOR, SLING, Stanford KBP ", "SEMAFOR, SLING, Stanford KBP system", "SEMAFOR, SLING, Stanford KBP system", "SEMAFOR, SLING, and Stanford KBP system, BIBREF14"], "best_model_answer": "SEMAFOR, SLING, Stanford KBP ", "oracle_answer": "SEMAFOR, SLING, Stanford KBP system", "oracle_answer_rank": 2}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "question": "What dataset did they evaluate on?", "references": ["dataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset", "first dataset is manually constructed general questions based on the 50 logical frames, second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions", "a manually created dataset of 50 logical frames mostly derived from FrameNet, a manually constructed general questions dataset based on the 50 logical frames and MetaQA dataset", " manually constructed general questions based on the 50 logical frames, MetaQA dataset"], "best_model_answer": "dataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset", "oracle_answer": "a manually created dataset of 50 logical frames mostly derived from FrameNet, a manually constructed general questions dataset based on the 50 logical frames and MetaQA dataset", "oracle_answer_rank": 3}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "question": "What are the contributions of this paper?", "references": ["adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach, investigation on the quality of existing Italian word embeddings for this task, a comparison against a state-of-the-art discrete classifier", "(1) Using seq2seq for event detection and classification in Italian (2) Investigating quality of Italian word embeddings for this task (3) Comparison to state-of-the-art discrete classifier", "the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach, an investigation on the quality of existing Italian word embeddings for this task, a comparison against a state-of-the-art discrete classifier, pre-trained models and scripts running the system", "Adapting a seq2seq neural system to event detection and classification for Italian, investigating the quality of existing embeddings for the task, and comparing against a state-of-the-art discrete classifier."], "best_model_answer": "adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach, investigation on the quality of existing Italian word embeddings for this task, a comparison against a state-of-the-art discrete classifier", "oracle_answer": "adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach, investigation on the quality of existing Italian word embeddings for this task, a comparison against a state-of-the-art discrete classifier", "oracle_answer_rank": 1}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "question": "What are the baselines this paper uses?", "references": [" cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features", "FBK-HLT - a cascade of two SVM classifiers (one for detection and one for classification)", "FBK-HLT BIBREF23", "Unanswerable"], "best_model_answer": " cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features", "oracle_answer": "FBK-HLT - a cascade of two SVM classifiers (one for detection and one for classification)", "oracle_answer_rank": 2}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "question": "Can the model be extended to other languages?", "references": ["Unanswerable", "Yes", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "question": "How do they decide what is the semantic concept label of particular cluster?", "references": ["Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.", "Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.", "They automatically  label the cluster using WordNet and context-sensitive strengths of domain-specific word embeddings", "Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering"], "best_model_answer": "Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.", "oracle_answer": "Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.", "oracle_answer_rank": 2}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "question": "How do they discover coherent word clusters?", "references": ["First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.", "First, we trained domain-specific word embeddings, Then, we used k-means clustering to cluster the embeddings of the gender-associated words", "First, they  trained domain-specific word embeddings using the Word2Vec  model, then used k-means clustering to cluster the embeddings of the gender-associated words.", "The authors first generated a set of words which are associated with each gender, then built domain-specific word embeddings and used k-means clustering to cluster the gendered word associations together. "], "best_model_answer": "First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.", "oracle_answer": "First, we trained domain-specific word embeddings, Then, we used k-means clustering to cluster the embeddings of the gender-associated words", "oracle_answer_rank": 2}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "question": "How big are two introduced datasets?", "references": ["300K sentences in each dataset", "each consisting of over 300K sentences", "Celeb dataset: 15917 texts and 342645 sentences\nProfessor dataset: 283973 texts and 976677 sentences", "Celebrity Dataset has 15,917 texts, 342,645 sentences, and the Female Male Proportions are  0.67/ 0.33. \nProfessor Dataset has 283,973 texts, 976, 667 sentences, and the Femal Male Proportions are 0.28./ 0,72"], "best_model_answer": "300K sentences in each dataset", "oracle_answer": "300K sentences in each dataset", "oracle_answer_rank": 1}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "question": "What are strong baselines authors used?", "references": ["The authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.", "Unanswerable", "the top 4 predicted labels and the centroid of the cluster", "the top 4 predicted labels and the centroid of the cluster as a strong baseline label"], "best_model_answer": "The authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.", "oracle_answer": "the top 4 predicted labels and the centroid of the cluster as a strong baseline label", "oracle_answer_rank": 4}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "question": "How do data-driven models usually respond to abuse?", "references": ["either by refusing politely, or, with flirtatious responses, or, by retaliating", "Data-driven systems rank low in general", "politely refuse, politely refuses, flirtatious responses", "flirt; retaliation"], "best_model_answer": "either by refusing politely, or, with flirtatious responses, or, by retaliating", "oracle_answer": "either by refusing politely, or, with flirtatious responses, or, by retaliating", "oracle_answer_rank": 1}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "question": "How much data did they gather from crowdsourcing?", "references": ["600K", "9960", "9960 HITs from 472 crowd workers", "9960 HITs"], "best_model_answer": "600K", "oracle_answer": "9960 HITs", "oracle_answer_rank": 4}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "question": "How many different strategies were evaluated?", "references": ["14", "12", "14", "Unanswerable"], "best_model_answer": "14", "oracle_answer": "14", "oracle_answer_rank": 1}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "question": "Was the automatic annotation evaluated?", "references": ["No", "No", "Yes", "Yes"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "question": "What morphological typologies are considered?", "references": ["agglutinative and fusional languages", "agglutinative and fusional", "Turkish, Finnish, Czech, German, Spanish, Catalan and English", "agglutinative and fusional languages"], "best_model_answer": "agglutinative and fusional languages", "oracle_answer": "agglutinative and fusional languages", "oracle_answer_rank": 1}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "question": "Does the model consider both derivational and inflectional morphology?", "references": ["Yes", "Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "question": "What type of morphological features are used?", "references": ["char3 slides a character window of width $n=3$ over the token, lemma of the token, additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units., characters, character sequences", "For all languages, morph outputs the lemma of the token followed by language specific morphological tags, additional information for some languages, such as parts-of-speech tags for Turkish", "language specific morphological tags", "morph outputs the lemma of the token followed by language specific morphological tags, semantic roles of verbal predicates"], "best_model_answer": "char3 slides a character window of width $n=3$ over the token, lemma of the token, additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units., characters, character sequences", "oracle_answer": "For all languages, morph outputs the lemma of the token followed by language specific morphological tags, additional information for some languages, such as parts-of-speech tags for Turkish", "oracle_answer_rank": 2}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "question": "What datasets are used in this paper?", "references": ["The Semantic Scholar corpus , Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K and COCO", "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, January 2018 English Wikipedia dataset, Flickr30K, COCO", "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K, COCO", "Semantic Scholar corpus BIBREF21 (SemScholar), Springer Nature's SciGraph, Textbook Question Answering corpus BIBREF23, Wikipedia, Flickr30K, COCO"], "best_model_answer": "The Semantic Scholar corpus , Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K and COCO", "oracle_answer": "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K, COCO", "oracle_answer_rank": 3}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "question": "What language are the captions in?", "references": ["English", "Unanswerable", "Unanswerable", "English"], "best_model_answer": "English", "oracle_answer": "English", "oracle_answer_rank": 1}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "question": "What ad-hoc approaches are explored?", "references": ["HolE, Vecsigrafo", "Embedding network, 2WayNet, VSE++, DSVE-loc)", "Unanswerable"], "best_model_answer": "HolE, Vecsigrafo", "oracle_answer": "HolE, Vecsigrafo", "oracle_answer_rank": 1}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "question": "What supervised baselines did they compare with?", "references": ["direct combination, supervised pre-training", "direct combination baseline, supervised pre-training baseline", "The direct combination baseline , The supervised pre-training baseline", "direct combination baseline, supervised pre-training baseline"], "best_model_answer": "direct combination, supervised pre-training", "oracle_answer": "direct combination baseline, supervised pre-training baseline", "oracle_answer_rank": 2}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "question": "Is the data specific to a domain?", "references": ["No", "No", "No", "Yes"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "question": "Where do their figure and captions come from?", "references": ["The Semantic Scholar corpus, Springer Nature's SciGraph", "scientific publications, middle school science curricula", "scientific literature", "SN SciGraph and AI2 Semantic Scholar"], "best_model_answer": "The Semantic Scholar corpus, Springer Nature's SciGraph", "oracle_answer": "The Semantic Scholar corpus, Springer Nature's SciGraph", "oracle_answer_rank": 1}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "question": "did the top teams experiment with lexicons?", "references": ["Unanswerable", "Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "question": "did they experiment with lexicons?", "references": ["Unanswerable", "No", "No"], "best_model_answer": "Unanswerable", "oracle_answer": "No", "oracle_answer_rank": 2}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "question": "what was the baseline?", "references": ["Weka baseline BIBREF5", "Weka baseline BIBREF5", "Weka", " Weka baseline BIBREF5"], "best_model_answer": "Weka baseline BIBREF5", "oracle_answer": "Weka baseline BIBREF5", "oracle_answer_rank": 1}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "question": "what was their result?", "references": ["Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.", "0.689 on development and 0.522 on test set", "For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100., In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25., On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively., on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25", "Unanswerable"], "best_model_answer": "Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.", "oracle_answer": "Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.", "oracle_answer_rank": 1}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "question": "what dataset was used?", "references": [" training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger", "datasets provided for the shared task BIBREF5", "Dataset of tweets provided for the shared task.", "Dataset from shared task BIBREF5"], "best_model_answer": " training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger", "oracle_answer": "datasets provided for the shared task BIBREF5", "oracle_answer_rank": 2}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "question": "What is their definition of hate speech?", "references": ["rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech", "Hate speech is a text that contains one or more of the following aspects: directness, offensiveness, targeting a group or individual based on specific attributes, overall negativity.", " in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis."], "best_model_answer": "rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech", "oracle_answer": "rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech", "oracle_answer_rank": 1}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "question": "What languages does the new dataset contain?", "references": ["English, French, Arabic", "English, French, Arabic", "English, French, Arabic", "English, French, and Arabic "], "best_model_answer": "English, French, Arabic", "oracle_answer": "English, French, Arabic", "oracle_answer_rank": 1}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "question": "What aspects are considered?", "references": [" (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments", "whether the text is direct or indirect, if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal, the attribute based on which it discriminates against an individual or a group of people, the name of this group,  how the annotators feel about its content within a range of negative to neutral sentiments", "(a) whether the text is direct or indirect, (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal, (c) the attribute based on which it discriminates against an individual or a group of people, (d) the name of this group, (e) how the annotators feel about its content within a range of negative to neutral sentiments", "Directness, Hostility, Target group, Target, Sentiment of the annotator"], "best_model_answer": " (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments", "oracle_answer": "(a) whether the text is direct or indirect, (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal, (c) the attribute based on which it discriminates against an individual or a group of people, (d) the name of this group, (e) how the annotators feel about its content within a range of negative to neutral sentiments", "oracle_answer_rank": 3}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "question": "How big is their dataset?", "references": ["13 000 tweets", "13014", "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets", "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets"], "best_model_answer": "13 000 tweets", "oracle_answer": "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets", "oracle_answer_rank": 3}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "question": "What are the opportunities presented by the use of Semantic Web technologies in Machine Translation?", "references": ["disambiguation, Named Entities, Non-standard speech, Translating KBs", "disambiguation, NERD,  non-standard language, translating KBs", "Disambiguation, Named Entities, Non-standard speech, Translating KBs", "SWT can be applied to support the semantic disambiguation in MT: to  recognize ambiguous words before translation and  as a post-editing technique applied to  the output language. SWT may be used for translating KBs."], "best_model_answer": "disambiguation, Named Entities, Non-standard speech, Translating KBs", "oracle_answer": "disambiguation, Named Entities, Non-standard speech, Translating KBs", "oracle_answer_rank": 1}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "question": "What are the challenges associated with the use of Semantic Web technologies in Machine Translation?", "references": ["syntactic disambiguation problem which as yet lacks good solutions, directly related to the ambiguity problem and therefore has to be resolved in that wider context, In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open", "reordering errors,  lexical and syntactic ambiguity", "SWT are hard to implement"], "best_model_answer": "syntactic disambiguation problem which as yet lacks good solutions, directly related to the ambiguity problem and therefore has to be resolved in that wider context, In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open", "oracle_answer": "syntactic disambiguation problem which as yet lacks good solutions, directly related to the ambiguity problem and therefore has to be resolved in that wider context, In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open", "oracle_answer_rank": 1}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "question": "What are the other obstacles to automatic translations which are not mentioned in the abstract?", "references": ["Excessive focus on English and European languages, limitations of SMT approaches for translating across domains, no-standard speech texts from users, morphologically rich languages, parallel data for training differs widely from real user speech", "reordering errors", "Unanswerable", "reordering errors"], "best_model_answer": "Excessive focus on English and European languages, limitations of SMT approaches for translating across domains, no-standard speech texts from users, morphologically rich languages, parallel data for training differs widely from real user speech", "oracle_answer": "reordering errors", "oracle_answer_rank": 2}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "question": "what eeg features were used?", "references": ["We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0,  So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel, We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. ", "root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy", "root mean square, zero crossing rate, moving window average, kurtosis, power spectral entropy", "root mean square, zero crossing rate, moving window average, kurtosis, power spectral entropy, extracted 31(channels) X 5 or 155 features"], "best_model_answer": "We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0,  So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel, We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. ", "oracle_answer": "root mean square, zero crossing rate, moving window average, kurtosis, power spectral entropy", "oracle_answer_rank": 3}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "question": "what were the baselines?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "question": "what dataset was used?", "references": [" two types of simultaneous speech EEG recording databases ", "The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment.", "Speech EEG recording collected from male and female subjects under different background noises", "For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment."], "best_model_answer": " two types of simultaneous speech EEG recording databases ", "oracle_answer": "The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment.", "oracle_answer_rank": 2}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "question": "Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "question": "Do they evaluate which compression method yields the most gains?", "references": ["Yes", "Yes", "No", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "question": "On which datasets does LadaBERT achieve state-of-the-art?", "references": ["MNLI-m, MNLI-mm, SST-2, QQP, QNLI", "LadaBERT -1, -2 achieves state of art on all datasets namely, MNLI-m MNLI-mm, SST-2, QQP, and QNLI. \nLadaBERT-3 achieves SOTA on the first four dataset. \nLadaBERT-4 achieves SOTA on MNLI-m, MNLI-mm, and QNLI ", "SST-2, MNLI-m, MNLI-mm, QNLI, QQP", "LadaBERT-1 and LadaBERT-2  on MNLI-m, MNLI-mm, SST-2, QQP and QNLI .\nLadaBERT-3  on MNLI-m, MNLI-mm, SST-2, and QQP . LadaBERT-4  on MNLI-m, MNLI-mm and QNLI ."], "best_model_answer": "MNLI-m, MNLI-mm, SST-2, QQP, QNLI", "oracle_answer": "MNLI-m, MNLI-mm, SST-2, QQP, QNLI", "oracle_answer_rank": 1}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "question": "What domain of text are they working with?", "references": ["news articles", "news", "news articles", "news"], "best_model_answer": "news articles", "oracle_answer": "news articles", "oracle_answer_rank": 1}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "question": "What dataset do they use?", "references": ["DUC 2002 document summarization corpus, our own DailyMail news highlights corpus", "DUC 2002, our own Dailymail news highlights corpus", "the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus", "DailyMail news articles"], "best_model_answer": "DUC 2002 document summarization corpus, our own DailyMail news highlights corpus", "oracle_answer": "DUC 2002 document summarization corpus, our own DailyMail news highlights corpus", "oracle_answer_rank": 1}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "question": "Do they compare to abstractive summarization methods?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "question": "What types of commonsense knowledge are they talking about?", "references": ["hypernym relations", "the collection of information that an ordinary person would have", "Hypernymy or is-a relations between words or phrases", "Knowledge than an ordinary person would have such as transitive entailment relation, complex ordering, compositionality, multi-word entities"], "best_model_answer": "hypernym relations", "oracle_answer": "the collection of information that an ordinary person would have", "oracle_answer_rank": 2}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "question": "What do they mean by intrinsic geometry of spaces of learned representations?", "references": ["In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.", "The intrinsic geometry is that the general concept embedding should be smaller than the specific concept embedding in every coordinate of the embeddings", "the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than local relation predictions"], "best_model_answer": "In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.", "oracle_answer": "In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.", "oracle_answer_rank": 1}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "question": "Did they pre-train on existing sentiment corpora?", "references": ["Yes", "Yes", "Yes", "No, they used someone else's pretrained model. "], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "question": "What were the most salient features extracted by the models?", "references": ["unigrams and bigrams, word2vec, manually constructed lexica, sentiment embeddings", "Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "unigrams and bigrams, word2vec, manually constructed lexica, sentiment embeddings", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "question": "How many languages are in the dataset?", "references": ["2", "2", "2 (Spanish and English)"], "best_model_answer": "2", "oracle_answer": "2", "oracle_answer_rank": 1}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "question": "Did the system perform well on low-resource languages?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "question": "What are the parts of the \"multimodal\" resources?", "references": ["spatial organisation , discourse structure", "node types that represent different diagram elements, The same features are used for both AI2D and AI2D-RST for nodes with layout information, discourse relations, information about semantic relations", "grouping, connectivity, and discourse structure "], "best_model_answer": "spatial organisation , discourse structure", "oracle_answer": "grouping, connectivity, and discourse structure ", "oracle_answer_rank": 3}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "question": "Are annotators familiar with the science topics annotated?", "references": ["The annotation for AI2D was\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts", "Unanswerable", "Unanswerable"], "best_model_answer": "The annotation for AI2D was\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "question": "How are the expert and crowd-sourced annotations compared to one another?", "references": ["by using them as features in classifying diagrams and\ntheir parts using various graph neural networks.", "Expert annotators incorporate domain knowledge from multimodality theory while non-expert cannot but they are less time-consuming and use less resources.", "results are not entirely comparable due to different node types, more reasonable to compare architectures"], "best_model_answer": "by using them as features in classifying diagrams and\ntheir parts using various graph neural networks.", "oracle_answer": "Expert annotators incorporate domain knowledge from multimodality theory while non-expert cannot but they are less time-consuming and use less resources.", "oracle_answer_rank": 2}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "question": "What platform do the crowd-sourced workers come from?", "references": ["Amazon Mechanical Turk", "Amazon Mechanical Turk", "Unanswerable", "Amazon Mechanical Turk"], "best_model_answer": "Amazon Mechanical Turk", "oracle_answer": "Amazon Mechanical Turk", "oracle_answer_rank": 1}
{"question_id": "fff1ed2435ba622d884ecde377ff2de127167638", "question": "Who are considered trained experts?", "references": ["Annotators trained on multimodality theory", "Unanswerable", "domain knowledge from multimodality theory", "Those who have domain knowledge on multimodal communication and annotation."], "best_model_answer": "Annotators trained on multimodality theory", "oracle_answer": "domain knowledge from multimodality theory", "oracle_answer_rank": 3}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "question": "Which model architecture do they opt for?", "references": ["Recurrent Neural Networks, Convolutional Neural Networks", "RNNs and CNNs", "HAN BIBREF10, CNN BIBREF11", "CNN, RNN"], "best_model_answer": "Recurrent Neural Networks, Convolutional Neural Networks", "oracle_answer": "HAN BIBREF10, CNN BIBREF11", "oracle_answer_rank": 3}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "question": "Which dataset do they use?", "references": ["Clueweb09", "Clueweb09 derived dataset, new dataset based on Wikipedia crawl data", "the Clueweb09 derived dataset , dataset based on Wikipedia crawl data", "Clueweb09 derived dataset, Wikipedia crawl data"], "best_model_answer": "Clueweb09", "oracle_answer": "the Clueweb09 derived dataset , dataset based on Wikipedia crawl data", "oracle_answer_rank": 3}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "question": "Which setup shows proves to be the hardest: cross-topic, cross-domain, cross-temporal, or across annotators?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "60e6296ca2a697892bd67558a21a83ef01a38177", "question": "Do they compare their semantic feature approach to lexical approaches?", "references": ["Yes", "Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "question": "what dataset was used for training?", "references": ["64M segments from YouTube videos", "YouCook2 , sth-sth", "64M segments from YouTube videos", "About 64M segments from YouTube videos comprising a total of 1.2B tokens."], "best_model_answer": "64M segments from YouTube videos", "oracle_answer": "64M segments from YouTube videos", "oracle_answer_rank": 1}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "question": "what is the size of the training data?", "references": ["64M video segments with 1.2B tokens", "64M", "64M segments from YouTube videos, INLINEFORM0 B tokens, vocabulary of 66K wordpieces"], "best_model_answer": "64M video segments with 1.2B tokens", "oracle_answer": "64M video segments with 1.2B tokens", "oracle_answer_rank": 1}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "question": "what features were derived from the videos?", "references": ["1500-dimensional vectors similar to those used for large scale image classification tasks.", "features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks", "1500-dimensional vectors, extracted from the video frames at 1-second intervals"], "best_model_answer": "1500-dimensional vectors similar to those used for large scale image classification tasks.", "oracle_answer": "features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks", "oracle_answer_rank": 2}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "question": "Do any of the models use attention?", "references": ["Yes", "Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "question": "What translation models are explored?", "references": ["NMT architecture BIBREF10", "architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism", "LSTM with attention"], "best_model_answer": "NMT architecture BIBREF10", "oracle_answer": "architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism", "oracle_answer_rank": 2}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "question": "What is symbolic rewriting?", "references": ["It is a process of translating a set of formal symbolic data to another set of formal symbolic data.", "Unanswerable", "Symbolic rewriting is the method to rewrite ground and nonground data from one to another form using rules."], "best_model_answer": "It is a process of translating a set of formal symbolic data to another set of formal symbolic data.", "oracle_answer": "It is a process of translating a set of formal symbolic data to another set of formal symbolic data.", "oracle_answer_rank": 1}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "question": "How do they incorporate expert knowledge into their topic model?", "references": ["The experts define anchors and the model learns correlations between the anchors and latent topics.", "anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors", "They use an anchored information theoretic topic modeling using Correlation Explanation and  information bottleneck."], "best_model_answer": "The experts define anchors and the model learns correlations between the anchors and latent topics.", "oracle_answer": "anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors", "oracle_answer_rank": 2}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "question": "On which corpora do they evaluate on?", "references": ["20 Newsgroups, i2b2 2008 Obesity Challenge BIBREF22 data set", "20 Newsgroups , i2b2 2008 Obesity Challenge", "20 Newsgroups, i2b2 2008 Obesity Challenge BIBREF22 data set", " i2b2 2008 Obesity Challenge BIBREF22, 20 Newsgroups"], "best_model_answer": "20 Newsgroups, i2b2 2008 Obesity Challenge BIBREF22 data set", "oracle_answer": "20 Newsgroups, i2b2 2008 Obesity Challenge BIBREF22 data set", "oracle_answer_rank": 1}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "question": "Do they compare against popular topic models, such as LDA?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "question": "What is F-score obtained?", "references": ["For Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32", "50.60 on Named Entity and 59.32 on Nominal Mention", "Best proposed model achieves F1 score of 50.60, 59.32, 54.82, 20.96 on Named Entity, Nominam Mention, Overall, Out of vocabulary respectively.", "Best F1 score obtained is 54.82% overall"], "best_model_answer": "For Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32", "oracle_answer": "Best proposed model achieves F1 score of 50.60, 59.32, 54.82, 20.96 on Named Entity, Nominam Mention, Overall, Out of vocabulary respectively.", "oracle_answer_rank": 3}
{"question_id": "2e4688205c8e344cded7a053b6014cce04ef1bd5", "question": "What is the state-of-the-art?", "references": ["Peng and Dredze peng-dredze:2016:P16-2", "Peng and Dredze peng-dredze:2016:P16-2", "Peng and Dredze peng-dredze:2016:P16-2"], "best_model_answer": "Peng and Dredze peng-dredze:2016:P16-2", "oracle_answer": "Peng and Dredze peng-dredze:2016:P16-2", "oracle_answer_rank": 1}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "question": "Which Chinese social media platform does the data come from?", "references": ["Unanswerable", "Sina Weibo service", "Sina Weibo"], "best_model_answer": "Unanswerable", "oracle_answer": "Sina Weibo service", "oracle_answer_rank": 2}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "question": "What dataset did they use?", "references": ["Peng and Dredze peng-dredze:2016:P16-2, Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service", "Peng and Dredze peng-dredze:2016:P16-2", "a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2"], "best_model_answer": "Peng and Dredze peng-dredze:2016:P16-2, Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service", "oracle_answer": "Peng and Dredze peng-dredze:2016:P16-2", "oracle_answer_rank": 2}
{"question_id": "b70e4c49300dc3eab18e907ab903afd2a0c6075a", "question": "What are the five downstream tasks?", "references": ["These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.", "NLI (XNLI dataset), document classification (MLDoc dataset), intent classification, POS tagging, NER", "NLI (XNLI dataset), document classification (MLDoc dataset),  intent classification, sequence tagging tasks: POS tagging, NER", "NLI, document classification, intent classification, POS tagging, NER"], "best_model_answer": "These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.", "oracle_answer": "NLI (XNLI dataset), document classification (MLDoc dataset), intent classification, POS tagging, NER", "oracle_answer_rank": 2}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "question": "Is this more effective for low-resource than high-resource languages?", "references": ["Yes", "Yes", "Yes", "we see that the gains are more pronounced in low resource languages"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "question": "How did they select the 50 languages they test?", "references": ["These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model", "For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.", "intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model"], "best_model_answer": "These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model", "oracle_answer": "These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model", "oracle_answer_rank": 1}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "question": "What kind of evaluations do use to evaluate dialogue?", "references": ["They use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.", "Unanswerable", "perplexity (ppl.) and BLEU, which of the two dialogues is better in terms of engagingness, interestingness, and humanness", "perplexity, BLEU, ACUTE-EVA"], "best_model_answer": "They use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.", "oracle_answer": "perplexity (ppl.) and BLEU, which of the two dialogues is better in terms of engagingness, interestingness, and humanness", "oracle_answer_rank": 3}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "question": "Which translation pipelines do they use to compare against?", "references": ["Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.", "M-Bert2Bert, M-CausalBert, Bert2Bert, CausalBert, Poly-encoder BIBREF75, XNLG", "Google Translate API"], "best_model_answer": "Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.", "oracle_answer": "Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.", "oracle_answer_rank": 1}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "question": "Which languages does their newly created dataset contain?", "references": ["Chinese, French, Indonesian, Italian, Korean, Japanese", "English, Chinese, French, Indonesian, Italian, Korean, Japanese", "Chinese, French, Indonesian, Italian, Korean, Japanese", "Chinese, French, Indonesian, Italian, Korean, and Japanese"], "best_model_answer": "Chinese, French, Indonesian, Italian, Korean, Japanese", "oracle_answer": "Chinese, French, Indonesian, Italian, Korean, Japanese", "oracle_answer_rank": 1}
{"question_id": "ae2142ee9e093ce485025168f4bcb3da4602739d", "question": "did they collect their own contrastive test set?", "references": ["No", "Yes", "It is automatically created from the OpenSubtitles corpus.", "Yes"], "best_model_answer": "No", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "question": "what are the baselines?", "references": ["bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21", " standard bidirectional RNN model with attention, A standard context-agnostic Transformer", "standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8"], "best_model_answer": "bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21", "oracle_answer": "standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8", "oracle_answer_rank": 3}
{"question_id": "cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "question": "what context aware models were experimented?", "references": ["standard bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, standard context-agnostic Transformer, concat22, concat21, BIBREF8", "bidirectional RNN, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21, BIBREF8", "a standard bidirectional RNN model with attention, concat22 , s-hier, s-t-hier, s-hier-to-2, concat21 , BIBREF8 "], "best_model_answer": "standard bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, standard context-agnostic Transformer, concat22, concat21, BIBREF8", "oracle_answer": "a standard bidirectional RNN model with attention, concat22 , s-hier, s-t-hier, s-hier-to-2, concat21 , BIBREF8 ", "oracle_answer_rank": 3}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "question": "what languages did they experiment on?", "references": ["English, German", "English, German ", "English , German "], "best_model_answer": "English, German", "oracle_answer": "English, German", "oracle_answer_rank": 1}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "question": "How do they obtain the entity linking results in their model?", "references": ["They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.", "The mention is linked to the entity with the greatest commonness score.", "we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string."], "best_model_answer": "They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.", "oracle_answer": "They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.", "oracle_answer_rank": 1}
{"question_id": "6295951fda0cfa2eb4259d544b00bc7dade7c01e", "question": "Which model architecture do they use?", "references": ["BiLSTMs , MLP ", "BiLSTM with a three-layer perceptron", "BiLSTM"], "best_model_answer": "BiLSTMs , MLP ", "oracle_answer": "BiLSTM with a three-layer perceptron", "oracle_answer_rank": 2}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "question": "Which datasets do they evaluate on?", "references": ["FIGER (GOLD) BIBREF0, BBN BIBREF5", "FIGER (GOLD) , BBN", "FIGER (GOLD), BBN"], "best_model_answer": "FIGER (GOLD) BIBREF0, BBN BIBREF5", "oracle_answer": "FIGER (GOLD) , BBN", "oracle_answer_rank": 2}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "question": "How many domain experts were involved into creation of dataset?", "references": ["Unanswerable", "1", "One domain expert."], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "question": "What metrics are used for evaluation?", "references": ["F1-score", "precision, recall, f1-score, and support", "Precision, recall, f1-score, and support."], "best_model_answer": "F1-score", "oracle_answer": "precision, recall, f1-score, and support", "oracle_answer_rank": 2}
{"question_id": "77c3416578b52994227bae7f2529600f02183e12", "question": "What is the performance of fine tuned model on this dataset?", "references": ["F1-score of $0.89$", "The model gives an F1-score of $0.89$ for the concept recognition task.", " F1-score of $0.89$"], "best_model_answer": "F1-score of $0.89$", "oracle_answer": "F1-score of $0.89$", "oracle_answer_rank": 1}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "question": "Are constructed datasets open sourced?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "question": "How does labeling scheme look like?", "references": ["Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.", "BIO Labelling Scheme\nabb: represents abbreviations such as TRL representing Technology Readiness Level.\n\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n\norg: represents an organization such as `NASA', `aerospace industry', etc.\n\nart: represents names of artifacts or instruments such as `AS1300'\n\ncardinal: represents numerical values such as `1', `100', 'one' etc.\n\nloc: represents location-like entities such as component facilities or centralized facility.\n\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.", "1. abb\n2. grp\n3. syscon\n4. opcon\n5. seterm\n6. event\n7. org\n8. art\n9. cardinal\n10. loc\n11. mea"], "best_model_answer": "Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.", "oracle_answer": "Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.", "oracle_answer_rank": 1}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "question": "What pretrained language model is used?", "references": ["BERT", "BERT", "BERT "], "best_model_answer": "BERT", "oracle_answer": "BERT", "oracle_answer_rank": 1}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "question": "How big is constructed dataset?", "references": ["3700 sentences", "3700 sentences ", "roughly 3700 sentences at the word-token level"], "best_model_answer": "3700 sentences", "oracle_answer": "3700 sentences", "oracle_answer_rank": 1}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "question": "What metric is considered?", "references": ["Precision, recall and F-measure.", "precision, recall, F-measure", "precision, recall and F-measure"], "best_model_answer": "Precision, recall and F-measure.", "oracle_answer": "Precision, recall and F-measure.", "oracle_answer_rank": 1}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "question": "What hand-crafted features are used?", "references": ["position of sentence, sentence length, tense, qualifying adjectives, meta-discourse features", " sentences with their rhetorical status ", "Unanswerable"], "best_model_answer": "position of sentence, sentence length, tense, qualifying adjectives, meta-discourse features", "oracle_answer": "position of sentence, sentence length, tense, qualifying adjectives, meta-discourse features", "oracle_answer_rank": 1}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "question": "What word embeddings are used?", "references": ["INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 )", "Sentiment-Specific Word Embedding, word2vec", "word2vec, Sentiment-Specific Word Embedding"], "best_model_answer": "INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 )", "oracle_answer": "Sentiment-Specific Word Embedding, word2vec", "oracle_answer_rank": 2}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "question": "Do they annotate their own dataset?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "question": "How are the sentence embeddings generated?", "references": ["sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors", "Averaging the vectors of the words in a sentence, directly learning paragraph vectors using PV-DM, taking average of the SSWE of the words in a sentence.", " average the vectors in word sequence, training paragraph vectors, Sentiment-Specific Word Embedding"], "best_model_answer": "sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors", "oracle_answer": "Averaging the vectors of the words in a sentence, directly learning paragraph vectors using PV-DM, taking average of the SSWE of the words in a sentence.", "oracle_answer_rank": 2}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "question": "What is argumentative zoning?", "references": [" Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences", "process of assigning rhetorical status to the extracted sentences", "a process of assigning rhetorical status to the extracted sentences"], "best_model_answer": " Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences", "oracle_answer": "process of assigning rhetorical status to the extracted sentences", "oracle_answer_rank": 2}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "question": "How did they obtain the tweets?", "references": ["crawled two blackmarket sites, used Twitter's REST API", "By crawling YouLikeHits and Like4Like sites and then using Twitter's REST API", "We used Twitter's REST API"], "best_model_answer": "crawled two blackmarket sites, used Twitter's REST API", "oracle_answer": "crawled two blackmarket sites, used Twitter's REST API", "oracle_answer_rank": 1}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "question": "What baseline do they compare to?", "references": [" spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.", "Wu et al. BIBREF4, Rajdev et. al. BIBREF11", "Word2Vec and Doc2Vec to encode the tweets, then MLP classifier; Random Forest classifier on a standard set of features"], "best_model_answer": " spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.", "oracle_answer": " spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.", "oracle_answer_rank": 1}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "question": "What language is explored in this paper?", "references": ["English", "English", "English"], "best_model_answer": "English", "oracle_answer": "English", "oracle_answer_rank": 1}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "question": "What blackmarket services do they look at?", "references": ["Credit-based Freemium services", "Credit-based Freemium services", "YouLikeHits and Like4Like"], "best_model_answer": "Credit-based Freemium services", "oracle_answer": "Credit-based Freemium services", "oracle_answer_rank": 1}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "question": "What languages do they use during pretraining?", "references": ["English, French, Chinese", "English, Chinese, French", "English/French/Chinese"], "best_model_answer": "English, French, Chinese", "oracle_answer": "English, French, Chinese", "oracle_answer_rank": 1}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "question": "What is the architecture of the decoder?", "references": ["pre-trained Xnlg, 6-layer decoder", "6 transformer layers, each layer containing 1024 hidden units, 8 attention heads, and GELU activations.", "denoising auto-encoding (DAE) objective BIBREF24"], "best_model_answer": "pre-trained Xnlg, 6-layer decoder", "oracle_answer": "pre-trained Xnlg, 6-layer decoder", "oracle_answer_rank": 1}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "question": "What is the architecture of the encoder?", "references": ["pre-trained Xnlg with a 10-layer encoder", "denoising auto-encoding (DAE) objective BIBREF24", "10 transformer layers, each layer containing 1024 hidden units, 8 attentions heads, and GELU activations."], "best_model_answer": "pre-trained Xnlg with a 10-layer encoder", "oracle_answer": "pre-trained Xnlg with a 10-layer encoder", "oracle_answer_rank": 1}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "question": "What is their baseline?", "references": ["CorefNqg BIBREF33, Mp-Gsn BIBREF31, Xlm BIBREF5, Xlm Fine-tuning, Pipeline (Xlm), Pipeline (Xlm) with Google Translator", "CorefNqg, Mp-Gsn, Xlm, Pipeline (Xlm), Pipeline (Xlm) with Google Translator", "CorefNqg BIBREF33 , Mp-Gsn BIBREF31, Xlm BIBREF5"], "best_model_answer": "CorefNqg BIBREF33, Mp-Gsn BIBREF31, Xlm BIBREF5, Xlm Fine-tuning, Pipeline (Xlm), Pipeline (Xlm) with Google Translator", "oracle_answer": "CorefNqg BIBREF33, Mp-Gsn BIBREF31, Xlm BIBREF5, Xlm Fine-tuning, Pipeline (Xlm), Pipeline (Xlm) with Google Translator", "oracle_answer_rank": 1}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "question": "What human evaluation metrics do they look at?", "references": ["human preference", "triple pairing task, hierarchical generation", "Accuracy at pairing stories with the prompts used to generate them; accuracy of prompt ranking"], "best_model_answer": "human preference", "oracle_answer": "triple pairing task, hierarchical generation", "oracle_answer_rank": 2}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "question": "Which automated evaluation metrics are used?", "references": ["perplexity, prompt ranking accuracy", "model perplexity on the test set , prompt ranking accuracy", "perplexity , prompt ranking accuracy"], "best_model_answer": "perplexity, prompt ranking accuracy", "oracle_answer": "perplexity, prompt ranking accuracy", "oracle_answer_rank": 1}
{"question_id": "d66550f65484696c1284903708b87809ea705786", "question": "What baselines do they compare against?", "references": ["gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism, LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention, an ensemble of two Conv seq2seq with self-attention models, KNN model", "Language Models, seq2seq, Ensemble, KNN", "Language Models, seq2seq: using LSTMs and convolutional seq2seq architectures, Conv seq2seq with decoder self-attention, an ensemble of two Conv seq2seq with self-attention models, KNN model"], "best_model_answer": "gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism, LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention, an ensemble of two Conv seq2seq with self-attention models, KNN model", "oracle_answer": "Language Models, seq2seq: using LSTMs and convolutional seq2seq architectures, Conv seq2seq with decoder self-attention, an ensemble of two Conv seq2seq with self-attention models, KNN model", "oracle_answer_rank": 3}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "question": "Do they use pre-trained embeddings like BERT?", "references": ["No", "No", "Unanswerable"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "question": "What model is used to generate the premise?", "references": ["convolutional language model from BIBREF4", " convolutional language model from BIBREF4", "convolutional language model"], "best_model_answer": "convolutional language model from BIBREF4", "oracle_answer": "convolutional language model from BIBREF4", "oracle_answer_rank": 1}
{"question_id": "f2dba5bf75967407cce5d0a9c2618269225081f5", "question": "Are the stories in the dataset fictional stories?", "references": ["No", "No", "Yes"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "b783ec5cb9ad595da7db2c0ddf871152ae382c5f", "question": "Where are the stories collected from?", "references": ["online forum", "Reddit's WritingPrompts forum", "Reddit's WritingPrompts forum"], "best_model_answer": "online forum", "oracle_answer": "Reddit's WritingPrompts forum", "oracle_answer_rank": 2}
{"question_id": "3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "question": "which pretrained embeddings were experimented with?", "references": ["word2vec , fastText , GloVe , Baroni , SL999 ", "word2vec, fastText, GloVe, Baroni, SL999", "word2vec, fastText, GloVe, Baroni, SL999"], "best_model_answer": "word2vec , fastText , GloVe , Baroni , SL999 ", "oracle_answer": "word2vec , fastText , GloVe , Baroni , SL999 ", "oracle_answer_rank": 1}
{"question_id": "47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "question": "what datasets where used?", "references": ["STSB , SICK, MRPC", "STSB, SICK, MRPC", "SICK, STSB, MRPC"], "best_model_answer": "STSB , SICK, MRPC", "oracle_answer": "STSB , SICK, MRPC", "oracle_answer_rank": 1}
{"question_id": "67cb001f8ca122ea859724804b41529fea5faeef", "question": "what are the state of the art methods they compare with?", "references": ["ECNU, HCTI", "HCTI BIBREF5, InferSent BIBREF23 ", "ECNU BIBREF6, HCTI BIBREF5"], "best_model_answer": "ECNU, HCTI", "oracle_answer": "ECNU BIBREF6, HCTI BIBREF5", "oracle_answer_rank": 3}
{"question_id": "42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "question": "What agreement measure is used?", "references": ["Fleiss's Kappa", "Fleiss's Kappa ", "Fleiss's Kappa"], "best_model_answer": "Fleiss's Kappa", "oracle_answer": "Fleiss's Kappa", "oracle_answer_rank": 1}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "question": "Do they report the annotation agreement?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "b857f3e3f1dad5df55f69d062978967fe023ac6f", "question": "How many annotators participated?", "references": ["170", "three ", "Unanswerable"], "best_model_answer": "170", "oracle_answer": "170", "oracle_answer_rank": 1}
{"question_id": "5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "question": "What social-network features are used?", "references": ["Relative positions of the author and target accounts in the directed following network by\ncomputing modified versions of Jaccard\u2019s similarity index, friends count, followers count, verified status, number of tweets posted within 6 months.", "Downward overlap, upward overlap, inward overlap, outward overlap, bidirectional overlap, count of friends of each user, count of followers of each user, users verified status, number of tweets posted within six-month snapshots", "Neighborhood Overlap,  count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines"], "best_model_answer": "Relative positions of the author and target accounts in the directed following network by\ncomputing modified versions of Jaccard\u2019s similarity index, friends count, followers count, verified status, number of tweets posted within 6 months.", "oracle_answer": "Neighborhood Overlap,  count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines", "oracle_answer_rank": 3}
{"question_id": "235c7c7ca719068136928b18e19f9661e0f72806", "question": "What are the five factors considered?", "references": ["Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance", "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance", "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance"], "best_model_answer": "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance", "oracle_answer": "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance", "oracle_answer_rank": 1}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "question": "How is cyberbullying defined?", "references": ["They define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance", "cyberbullying is a complex social phenomenon, different from the simpler notion of cyberaggression", "A public display of intention to \u201cinflict injury or discomfort\u201d upon a weaker victim through repeated acts of aggression."], "best_model_answer": "They define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance", "oracle_answer": "cyberbullying is a complex social phenomenon, different from the simpler notion of cyberaggression", "oracle_answer_rank": 2}
{"question_id": "c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "question": "What evaluation was performed on the output?", "references": ["similarity of the generated texts with training data objectively, humor content subjectively, syntactic correctness of the generated sentences", "For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria, To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment", "Phrase Overlap match and K-gram-Jaccard similarity"], "best_model_answer": "similarity of the generated texts with training data objectively, humor content subjectively, syntactic correctness of the generated sentences", "oracle_answer": "For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria, To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment", "oracle_answer_rank": 2}
{"question_id": "9f1d81b2a6fe6835042a5229690e1951b97ff671", "question": "Where did the joke data come from?", "references": ["CrowdTruth and Subreddits", "CrowdTruth , Subreddits", "CrowdTruth, Subreddits"], "best_model_answer": "CrowdTruth and Subreddits", "oracle_answer": "CrowdTruth , Subreddits", "oracle_answer_rank": 2}
{"question_id": "fae930129c2638ba6f9c9b3383e85aa130a73876", "question": "What type of quotes is this system trying to generate?", "references": ["Unanswerable", "Unanswerable", "inspirational"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "1acfbdc34669cf19a778aceca941543f11b9a861", "question": "What size filters do they use in the convolution layer?", "references": ["1x3 filter size is used in convolutional layers.", "Unanswerable", "1x3"], "best_model_answer": "1x3 filter size is used in convolutional layers.", "oracle_answer": "1x3 filter size is used in convolutional layers.", "oracle_answer_rank": 1}
{"question_id": "864295caceb1e15144c1746ab5671d085d7ff7a1", "question": "By how much do they outperform state-of-the-art models on knowledge graph completion?", "references": [" improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement), INLINEFORM1 % absolute improvement in Hits@10", "0.105 in MRR and 6.1 percent points in Hits@10 on FB15k-237", "On FB15k-237 dataset it outperforms 0.105 in MRR and  6.1% absolute improvement in Hits@10"], "best_model_answer": " improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement), INLINEFORM1 % absolute improvement in Hits@10", "oracle_answer": "On FB15k-237 dataset it outperforms 0.105 in MRR and  6.1% absolute improvement in Hits@10", "oracle_answer_rank": 3}
{"question_id": "79e61134a6e29141cd19252571ffc92a0b4bc97f", "question": "did they test with other pretrained models besides bert?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "question": "what models did they compare with?", "references": ["BERT, BERT adding a Bi-LSTM on top, DenseNet BIBREF33 and HighwayLSTM BIBREF34, BERT+ BIMPM, remove the first bi-LSTM of BIMPM, Sim-Transformer", "BERT, BERT+ Bi-LSTM ,  BERT+ DenseNet, BERT+HighwayLSTM,   Ensembled model, BERT+ BIMPM, BERT+ BIMPM(first bi-LSTM removed),  BERT + Sim-Transformer .", "BERT, BERT + Bi-LSTM, BERT + HighwayLSTM, BERT + DenseNet, Ensembled Model, BERT + BIMPM, BERT + Sim-Transformer"], "best_model_answer": "BERT, BERT adding a Bi-LSTM on top, DenseNet BIBREF33 and HighwayLSTM BIBREF34, BERT+ BIMPM, remove the first bi-LSTM of BIMPM, Sim-Transformer", "oracle_answer": "BERT, BERT + Bi-LSTM, BERT + HighwayLSTM, BERT + DenseNet, Ensembled Model, BERT + BIMPM, BERT + Sim-Transformer", "oracle_answer_rank": 3}
{"question_id": "5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "question": "what datasets were used for testing?", "references": ["CoNLL03 , Yahoo Answer Classification Dataset, \u201cQuora-Question-Pair\u201d dataset 1", "CoNLL03,  Yahoo Answer Classification Dataset, \u201cQuora-Question-Pair\u201d dataset 1", "CoNLL03 dataset BIBREF5, Yahoo Answer Classification Dataset,  \u201cQuora-Question-Pair\u201d dataset"], "best_model_answer": "CoNLL03 , Yahoo Answer Classification Dataset, \u201cQuora-Question-Pair\u201d dataset 1", "oracle_answer": "CoNLL03 , Yahoo Answer Classification Dataset, \u201cQuora-Question-Pair\u201d dataset 1", "oracle_answer_rank": 1}
{"question_id": "7d539258b948cd5b5ad1230a15e4b739f29ed947", "question": "What inter-annotator agreement did they obtain?", "references": [" two inter-annotator agreement , aw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons", "Raw agreement is around .90 for this dataset.", "The average agreement on scene, function and construal is 0.915"], "best_model_answer": " two inter-annotator agreement , aw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons", "oracle_answer": "The average agreement on scene, function and construal is 0.915", "oracle_answer_rank": 3}
{"question_id": "9c1f70affc87024b4280f0876839309b8dddd579", "question": "How did they annotate the corpus?", "references": ["The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.", "Tokenization, Adposition Targets, Data Format, Reliability of Annotation", "The corpus is jointly annotated by three native Mandarin Chinese speakers, Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication, Annotation was conducted in two phases"], "best_model_answer": "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.", "oracle_answer": "The corpus is jointly annotated by three native Mandarin Chinese speakers, Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication, Annotation was conducted in two phases", "oracle_answer_rank": 3}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "question": "What is the size of the corpus?", "references": ["933 manually identified adpositions", "20287", "933 manually identified adpositions"], "best_model_answer": "933 manually identified adpositions", "oracle_answer": "933 manually identified adpositions", "oracle_answer_rank": 1}
{"question_id": "65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "question": "Which datasets do they use?", "references": ["Unanswerable", "https://github.com/Sairamvinay/Fake-News-Dataset\n\n", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "question": "What models are explored in this paper?", "references": ["SVM, Logistic Regression, ANN, LSTM, and Random Forest", "Artificial Neural Network (ANN), Long Short Term Memory networks (LSTMs),  Random Forest, Logistic Regression,  Support Vector Machine (SVM)", "SVM, Logistic Regression, ANN, LSTM, Random Forest, TFIDF, CV, W2V"], "best_model_answer": "SVM, Logistic Regression, ANN, LSTM, and Random Forest", "oracle_answer": "SVM, Logistic Regression, ANN, LSTM, and Random Forest", "oracle_answer_rank": 1}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "question": "what features of the essays are extracted?", "references": ["Following groups of features are extracted:\n- Numerical Features\n- Language Models\n- Clusters\n- Latent Dirichlet Allocation\n- Part-Of-Speech\n- Bag-of-words", "Numerical features, language models features, clusters, latent Dirichlet allocation, Part-of-Speech tags, Bag-of-words.", "Numerical features, Language Models, Clusters, Latent Dirichlet Allocation, Part-Of-Speech tags, Bag-of-words"], "best_model_answer": "Following groups of features are extracted:\n- Numerical Features\n- Language Models\n- Clusters\n- Latent Dirichlet Allocation\n- Part-Of-Speech\n- Bag-of-words", "oracle_answer": "Numerical features, language models features, clusters, latent Dirichlet allocation, Part-of-Speech tags, Bag-of-words.", "oracle_answer_rank": 2}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "question": "what were the evaluation metrics?", "references": ["Accuracy metric", "accuracy", "Accuracy"], "best_model_answer": "Accuracy metric", "oracle_answer": "accuracy", "oracle_answer_rank": 2}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "question": "what model is used?", "references": ["gradient boosted trees", "Light Gradient Boosting Machine", "gradient boosted trees"], "best_model_answer": "gradient boosted trees", "oracle_answer": "gradient boosted trees", "oracle_answer_rank": 1}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "question": "what future work is described?", "references": ["the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not", "Investigate the effectiveness of LDA to capture the subject of the essay.", "investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used"], "best_model_answer": "the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not", "oracle_answer": "investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used", "oracle_answer_rank": 3}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "question": "what was the baseline?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "question": "How is the sentence alignment quality evaluated?", "references": ["Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text", "The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:\n\nWrong alignment\n\nPartial alignment with slightly compositional translational equivalence\n\nPartial alignment with compositional translation and additional or missing information\n\nCorrect alignment with compositional translation and few additional or missing information\n\nCorrect alignment and fully compositional translation\n\nThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.", "5-point scale used in KocabiyikogluETAL:18"], "best_model_answer": "Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text", "oracle_answer": "Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text", "oracle_answer_rank": 1}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "question": "How is the speech alignment quality evaluated?", "references": ["Through a 3-point scale by annotators.", "Wrong alignment, Partial alignment, some words or sentences may be missing, Correct alignment, allowing non-spoken syllables at start or end.", "The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability."], "best_model_answer": "Through a 3-point scale by annotators.", "oracle_answer": "The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.", "oracle_answer_rank": 3}
{"question_id": "b569827ecd04ae8757dc3c9523ab97e3f47a6e00", "question": "Is their gating mechanism specially designed to handle one sentence bags?", "references": ["Yes", "No", "Unanswerable"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "question": "Do they show examples where only one sentence appears in a bag and their method works, as opposed to using selective attention?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "question": "By how much do they outperform previous state-of-the-art in terms of top-n precision?", "references": ["Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%", "5.3 percent points", "Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3%"], "best_model_answer": "Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%", "oracle_answer": "Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%", "oracle_answer_rank": 1}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "question": "By how much do they outperform existing methods?", "references": ["In Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt", "Their best implementation for semantic relatedness task comparison outperforms standard MaxEnt by 0,052 Pearson  Correlation.\nTheir best implementation for Textual Entailment task comparison (84,2 accuracy) DOES NOT outperform standard SVM (84,6 accuracy).\n", "Best proposed result had 0.851 and  0.842 compared to best previous result of 0.828 and 0.846 on person correlation and accuracy respectively."], "best_model_answer": "In Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt", "oracle_answer": "Their best implementation for semantic relatedness task comparison outperforms standard MaxEnt by 0,052 Pearson  Correlation.\nTheir best implementation for Textual Entailment task comparison (84,2 accuracy) DOES NOT outperform standard SVM (84,6 accuracy).\n", "oracle_answer_rank": 2}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "question": "Which datasets do they evaluate on?", "references": ["SICK (Sentences Involving Compositional Knowledge) dataset ", "SICK (Sentences Involving Compositional Knowledge) dataset", "SICK (Sentences Involving Compositional Knowledge) dataset"], "best_model_answer": "SICK (Sentences Involving Compositional Knowledge) dataset ", "oracle_answer": "SICK (Sentences Involving Compositional Knowledge) dataset ", "oracle_answer_rank": 1}
{"question_id": "ad8411edf11d3429c9bdd08b3e07ee671464d73c", "question": "Do they separately evaluate performance of their learned representations (before forwarding them to the CNN layer)?", "references": ["Yes", "No", "No"], "best_model_answer": "Yes", "oracle_answer": "No", "oracle_answer_rank": 2}
{"question_id": "11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "question": "What was the baseline?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "question": "What dataset was used in this challenge?", "references": ["SRE18 development and SRE18 evaluation datasets", "SRE19", "SRE04/05/06/08/10/MIXER6\nLDC98S75/LDC99S79/LDC2002S06/LDC2001S13/LDC2004S07\nVoxceleb 1/2\nFisher + Switchboard I\nCallhome+Callfriend"], "best_model_answer": "SRE18 development and SRE18 evaluation datasets", "oracle_answer": "SRE18 development and SRE18 evaluation datasets", "oracle_answer_rank": 1}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "question": "Which subsystem outperformed the others?", "references": ["primary system is the linear fusion of all the above six subsystems", "eftdnn ", "eftdnn"], "best_model_answer": "primary system is the linear fusion of all the above six subsystems", "oracle_answer": "eftdnn ", "oracle_answer_rank": 2}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "question": "Do they reduce language variation of text by enhancing frequencies?", "references": ["Unanswerable", "Unanswerable", "Variation decreases when frequencies of synonyms is enhanced; variation increases when frequencies of synonyms, hyponyms, hypernyms are enhanced"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "question": "Which domains do they explore?", "references": ["economic, political", " news articles related to Islam and articles discussing Islam basics", "economic, political"], "best_model_answer": "economic, political", "oracle_answer": "economic, political", "oracle_answer_rank": 1}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "question": "Which thesauri did they use?", "references": ["WordNet, European Union EuroVoc, RuThes", "WordNet, EuroVoc,  RuThes", "WordNet , EuroVoc , RuThes "], "best_model_answer": "WordNet, European Union EuroVoc, RuThes", "oracle_answer": "WordNet, EuroVoc,  RuThes", "oracle_answer_rank": 2}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "question": "What is their definition of hate speech?", "references": ["language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group"], "best_model_answer": "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "oracle_answer": "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "oracle_answer_rank": 1}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "question": "What type of model do they train?", "references": ["logistic regression, na\u00efve Bayes, decision trees, random forests, linear SVMs", "logistic regression, na\u00efve Bayes, decision trees, random forests, linear SVM", "logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs"], "best_model_answer": "logistic regression, na\u00efve Bayes, decision trees, random forests, linear SVMs", "oracle_answer": "logistic regression, na\u00efve Bayes, decision trees, random forests, linear SVMs", "oracle_answer_rank": 1}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "question": "How many users does their dataset have?", "references": ["33,458", "33,458 Twitter users are orginally used, but than random sample of tweets is extracted resulting in smaller number or users in final dataset.", "33458"], "best_model_answer": "33,458", "oracle_answer": "33,458", "oracle_answer_rank": 1}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "question": "How long is their dataset?", "references": ["85400000", "24,802 ", "24,802 labeled tweets"], "best_model_answer": "85400000", "oracle_answer": "24,802 ", "oracle_answer_rank": 2}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "question": "In what tasks does fine-tuning all layers hurt performance?", "references": ["SST-2", "Unanswerable", "SST-2"], "best_model_answer": "SST-2", "oracle_answer": "SST-2", "oracle_answer_rank": 1}
{"question_id": "441be93e2830cc0fc65afad6959db92754c9f5a8", "question": "Do they test against the large version of RoBERTa?", "references": ["For GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.", "Yes", "Yes"], "best_model_answer": "For GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "7f11f128fd39b8060f5810fa84102f000d94ea33", "question": "What is the performance improvement of their method over state-of-the-art models on the used datasets? ", "references": ["Average improvement in accuracy is 2.26 points", "Unanswerable", "Unanswerable"], "best_model_answer": "Average improvement in accuracy is 2.26 points", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "2a55076a66795793d79a3edfae1041098404fbc3", "question": "Could the proposed training framework be applied to other NLP problems?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a", "question": "How does the proposed training framework mitigate the bias pattern?", "references": ["Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading", "Artifacts in biased datasets are balanced by assigning specific weights for every sample", "by balancing or, smoothing the artifacts across different classes by assigning specific weights for every sample"], "best_model_answer": "Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading", "oracle_answer": "Artifacts in biased datasets are balanced by assigning specific weights for every sample", "oracle_answer_rank": 2}
{"question_id": "8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "question": "Which datasets do they use in the cross-dataset evaluation?", "references": ["SNLI, MultiNLI, JOCI, SICK", "SNLI, MultiNLI, JOCI, SICK", "SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13, SICK BIBREF14"], "best_model_answer": "SNLI, MultiNLI, JOCI, SICK", "oracle_answer": "SNLI, MultiNLI, JOCI, SICK", "oracle_answer_rank": 1}
{"question_id": "0aca0a208a1e28857fab44e397dc7880e010dbca", "question": "Which was the most helpful strategy?", "references": ["Vote entropy and KL divergence,  all the active learning strategies we tested do not work well with deep learning model", "Entropy algorithm is the best way to build machine learning models. Vote entropy and KL divergence are helpful for the training of machine learning ensemble classifiers.", "entropy"], "best_model_answer": "Vote entropy and KL divergence,  all the active learning strategies we tested do not work well with deep learning model", "oracle_answer": "Vote entropy and KL divergence,  all the active learning strategies we tested do not work well with deep learning model", "oracle_answer_rank": 1}
{"question_id": "471683ba6251b631f38a24d42b6dba6f52dee429", "question": "How large is their tweets dataset?", "references": ["3,685,984 unique tweets", "3,685,984 unique tweets", "3,685,984 unique tweets"], "best_model_answer": "3,685,984 unique tweets", "oracle_answer": "3,685,984 unique tweets", "oracle_answer_rank": 1}
{"question_id": "5dfd58f91e7740899c23ebfe04b7176edce9ead2", "question": "what is the size of the idn tagged corpus?", "references": ["10K", "10K sentences, 250K tokens", "10K sentences and 250K tokens"], "best_model_answer": "10K", "oracle_answer": "10K sentences, 250K tokens", "oracle_answer_rank": 2}
{"question_id": "c09bceea67273c10a0621da1a83b409f53342fd9", "question": "what neural network models were explored?", "references": ["Feedforward, biLSTM", "feedforward, bidirectional LSTM (biLSTM)", "feedforward network , bidirectional LSTM"], "best_model_answer": "Feedforward, biLSTM", "oracle_answer": "feedforward, bidirectional LSTM (biLSTM)", "oracle_answer_rank": 2}
{"question_id": "732bd97ae34541f215c436e2a1b98db1649cba27", "question": "what rule based models were evaluated?", "references": ["Rashel et al. BIBREF14", "rule-based tagger designed by Rashel et al. BIBREF14", "rule-based tagger designed by Rashel et al. BIBREF14"], "best_model_answer": "Rashel et al. BIBREF14", "oracle_answer": "rule-based tagger designed by Rashel et al. BIBREF14", "oracle_answer_rank": 2}
{"question_id": "183b385fb59ff1e3f658d4555a08b67c005a8734", "question": "what datasets have been used for this task?", "references": ["IDN Tagged Corpus ", "IDN Tagged Corpus", " IDN Tagged Corpus"], "best_model_answer": "IDN Tagged Corpus ", "oracle_answer": "IDN Tagged Corpus ", "oracle_answer_rank": 1}
{"question_id": "5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "question": "How much data do they use to train the embeddings?", "references": ["11,529,432 segmented words and 20,402 characters", "11,529,432 segmented words", "11,529,432 segmented words"], "best_model_answer": "11,529,432 segmented words and 20,402 characters", "oracle_answer": "11,529,432 segmented words", "oracle_answer_rank": 2}
{"question_id": "a79a23573d74ec62cbed5d5457a51419a66f6296", "question": "Do they evaluate their embeddings in any downstream task appart from word similarity and word analogy?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "d427e9d181434078c78b7ee33a26b269f160f6d2", "question": "What dialects of Chinese are explored?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "question": "What are the issues identified for out-of-vocabulary words?", "references": ["model did not have a flexibility in OOV words, One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, It was noticed that the model performed better when using the vectors from different FastText models", "for unknown words the model assigned a zero vector", "Also, the model with the dataset vectors did not have the flexibility to classify unknown words., the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results"], "best_model_answer": "model did not have a flexibility in OOV words, One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, It was noticed that the model performed better when using the vectors from different FastText models", "oracle_answer": "Also, the model with the dataset vectors did not have the flexibility to classify unknown words., the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results", "oracle_answer_rank": 3}
{"question_id": "5d03a82a70f7b1ab9829891403ec31607828cbd5", "question": "Is the morphology detection task evaluated?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "6cad6f074b0486210ffa4982c8d1632f5aa91d91", "question": "How does the model proposed extend ENAMEX?", "references": ["Extended with facility (FAC) type.", "The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC)", "SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based models"], "best_model_answer": "Extended with facility (FAC) type.", "oracle_answer": "The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC)", "oracle_answer_rank": 2}
{"question_id": "d38b3e0896b105d171e69ce34c689e4a7e934522", "question": "Which morphological features are extracted?", "references": ["like the gender, the number, and the case", "Information about the dataset includes the tokens of a set of articles and their position in a sentence, the lemma and the part of speech of every token, The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case", "The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers"], "best_model_answer": "like the gender, the number, and the case", "oracle_answer": "Information about the dataset includes the tokens of a set of articles and their position in a sentence, the lemma and the part of speech of every token, The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case", "oracle_answer_rank": 2}
{"question_id": "4379a3ece3fdb93b71db43f62833f5f724c49842", "question": "Do the authors report results on only English datasets?", "references": ["Yes", "Unanswerable", "Unanswerable"], "best_model_answer": "Yes", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "0abc2499195185c94837e0340d00cd3b83ee795e", "question": "What are the characteristics of the dataset of Twitter users?", "references": ["413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\"", "Tweet Diversity, URL score, Mean Daily Posts, Topics, Mean Post Length, Profile Picture", "a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter, Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites, Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information,  Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation"], "best_model_answer": "413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\"", "oracle_answer": "413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\"", "oracle_answer_rank": 1}
{"question_id": "138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "question": "How can an existing bot detection system by customized for health-related research?", "references": ["An existing bot detection score for each user  can be used as a feature in training", "Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. ", "simple derived features, we were able to significantly improve bot detection performance in health-related data"], "best_model_answer": "An existing bot detection score for each user  can be used as a feature in training", "oracle_answer": "simple derived features, we were able to significantly improve bot detection performance in health-related data", "oracle_answer_rank": 3}
{"question_id": "7e906dc00e92088a25df3719104d1750e5a27485", "question": "What type of health-related research takes place in social media?", "references": ["Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.,  Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. ", " drug reaction detection, syndromic surveillance, subject recruitment for cancer trials, characterizing drug abuse", "almost exclusively on population-level studies, very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women"], "best_model_answer": "Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.,  Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. ", "oracle_answer": "Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.,  Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. ", "oracle_answer_rank": 1}
{"question_id": "0d9241e904bd2bbf5b9a6ed7b5fc929651d3e28e", "question": "Do the QA tuples fall under a specific domain?", "references": ["conversations, which consist of at least one question and one free-form answer", "No", "No"], "best_model_answer": "conversations, which consist of at least one question and one free-form answer", "oracle_answer": "No", "oracle_answer_rank": 2}
{"question_id": "95646d0ac798dcfc15b43fa97a1908df9f7b9681", "question": "What is the baseline model?", "references": ["pre-trained version of BERT without special emoji tokens", "pre-trained version of BERT without special emoji tokens", "pre-trained version of BERT without special emoji tokens"], "best_model_answer": "pre-trained version of BERT without special emoji tokens", "oracle_answer": "pre-trained version of BERT without special emoji tokens", "oracle_answer_rank": 1}
{"question_id": "12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "question": "How large is the corpus of QA tuples?", "references": ["2000 tuples", "2000 tuples", "2000 tuples"], "best_model_answer": "2000 tuples", "oracle_answer": "2000 tuples", "oracle_answer_rank": 1}
{"question_id": "647f6e6b168ec38fcdb737d3b276f78402282f9d", "question": "What corpus did they use?", "references": ["a customer support dataset", "2000 tuples collected by BIBREF24 that are sourced from Twitter", " customer support dataset with a relatively high usage of emoji"], "best_model_answer": "a customer support dataset", "oracle_answer": "a customer support dataset", "oracle_answer_rank": 1}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "question": "what boosting techniques were used?", "references": ["Light Gradient Boosting Machine (LGBM)", "Light Gradient Boosting Machine", "Light Gradient Boosting Machine"], "best_model_answer": "Light Gradient Boosting Machine (LGBM)", "oracle_answer": "Light Gradient Boosting Machine", "oracle_answer_rank": 2}
{"question_id": "ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "question": "did they experiment with other text embeddings?", "references": ["No", "Yes", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "afd1c482c311e25fc42b9dd59cdc32ac542f5752", "question": "what is the size of this improved dataset?", "references": ["363,078 structured abstracts", "363,078", "Unanswerable"], "best_model_answer": "363,078 structured abstracts", "oracle_answer": "363,078 structured abstracts", "oracle_answer_rank": 1}
{"question_id": "ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "question": "how was the new dataset collected?", "references": ["The new dataset was collected from structured abstracts from PubMed and filtering abstract headings representative of the desired categories.", "collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories", "By searching for structured abstracts on PubMed using specific filters."], "best_model_answer": "The new dataset was collected from structured abstracts from PubMed and filtering abstract headings representative of the desired categories.", "oracle_answer": "collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories", "oracle_answer_rank": 2}
{"question_id": "7066f33c373115b1ead905fe70a1e966f77ebeee", "question": "who annotated the new dataset?", "references": ["The P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.", "automatic labeling, lemmatization of the abstract section labels in order to cluster similar categories, manually looked at a small number of samples for each label to determine if text was representative", "Unanswerable"], "best_model_answer": "The P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.", "oracle_answer": "The P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.", "oracle_answer_rank": 1}
{"question_id": "018b81f810a39b3f437a85573d24531efccd835f", "question": "what shortcomings of previous datasets are mentioned?", "references": ["using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label., Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "In the previous dataset a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "Information about the intervention and study design is mistakenly marked by a P label; a P-labeled section that contained more than one sentence would be split into multiple P-labeled sentences."], "best_model_answer": "using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label., Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "oracle_answer": "In the previous dataset a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "oracle_answer_rank": 2}
{"question_id": "e2c8d7f3ef5913582503e50244ca7158d0a62c42", "question": "Do single-language BERT outperforms multilingual BERT?", "references": ["Unanswerable", "For some language yes, but not for another.", "No"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "654fe0109502f2ed2dc8dad359dbbce4393e03dc", "question": "What types of agreement relations do they explore?", "references": ["subject-verb, noun-determiner, noun-attributive adjective, subject-predicate adjective", "The subject and verb in (UNKREF3) agree for number, while the noun and determiner in (UNKREF4), the noun and attributive adjective in (UNKREF5) and the subject and predicated adjective in (UNKREF6) agree for both number and gender.", "Subject-verb agreement,  noun-determiner agreement,  noun -attributive adjective agreement and noun-predicate adjective agreement."], "best_model_answer": "subject-verb, noun-determiner, noun-attributive adjective, subject-predicate adjective", "oracle_answer": "Subject-verb agreement,  noun-determiner agreement,  noun -attributive adjective agreement and noun-predicate adjective agreement.", "oracle_answer_rank": 3}
{"question_id": "da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "question": "what text classification datasets do they evaluate on?", "references": ["Amazon, Yelp, IMDB, MR, MPQA, Subj, TREC", "Amazon, Yelp, IMDB , MR , MPQA , Subj, TREC", "Amazon, Yelp, IMDB, MR BIBREF16, MPQA BIBREF17, Subj BIBREF18, TREC BIBREF19"], "best_model_answer": "Amazon, Yelp, IMDB, MR, MPQA, Subj, TREC", "oracle_answer": "Amazon, Yelp, IMDB, MR, MPQA, Subj, TREC", "oracle_answer_rank": 1}
{"question_id": "363a24ecb8ab45215134935e7e8165fff72ff90f", "question": "which models is their approach compared to?", "references": ["TextFooler", "word-LSTM BIBREF20, word-CNN BIBREF21,  fine-tuned BERT BIBREF12 base-uncased ", "word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier"], "best_model_answer": "TextFooler", "oracle_answer": "word-LSTM BIBREF20, word-CNN BIBREF21,  fine-tuned BERT BIBREF12 base-uncased ", "oracle_answer_rank": 2}
{"question_id": "74396ead9f88a9efc7626240ce128582ab69ef2b", "question": "by how much did their approach outperform previous work?", "references": ["By 0,008 F1,  0, 02 Recall and 0,02 Precision.", "New best result is F1 score of 0.752 compared to 0.744 of the best previous work.", "by 0.008 in terms of F1 score,  and 0.02 in terms of recall and precision "], "best_model_answer": "By 0,008 F1,  0, 02 Recall and 0,02 Precision.", "oracle_answer": "by 0.008 in terms of F1 score,  and 0.02 in terms of recall and precision ", "oracle_answer_rank": 3}
{"question_id": "8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "question": "what was the previous best results model?", "references": [" F1 (0.744)", " BIBREF12 buschmeier-cimiano-klinger:2014:W14-26", "logistic regression classifier"], "best_model_answer": " F1 (0.744)", "oracle_answer": " F1 (0.744)", "oracle_answer_rank": 1}
{"question_id": "eaed0b721cc3137b964f5265c7ecf76f565053e9", "question": "what are the baseline models?", "references": ["the All Sarcasm case, the Random case", "All Sarcasm case assumes that every instance is sarcastic,  Random case randomly assigns each instance as sarcastic or non-sarcastic", "All Sarcasm, Random case"], "best_model_answer": "the All Sarcasm case, the Random case", "oracle_answer": "the All Sarcasm case, the Random case", "oracle_answer_rank": 1}
{"question_id": "ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "question": "what domains are explored?", "references": ["Twitter, and Amazon product reviews", "Data was taken from two domains: Twitter, and Amazon product reviews. ", "Twitter, Amazon "], "best_model_answer": "Twitter, and Amazon product reviews", "oracle_answer": "Twitter, and Amazon product reviews", "oracle_answer_rank": 1}
{"question_id": "38af3f25c36c3725a31304ab96e2c200c55792b4", "question": "what training data was used?", "references": ["Twitter dataset,  Amazon product reviews", "Twitter product reviews containing exactly one of the trailing hashtags \u201c#sarcasm,\u201d \u201c#happiness,\u201d \u201c#sadness,\u201d \u201c#anger,\u201d \u201c#surprise,\u201d \u201c#fear,\u201d and \u201c#disgust\u201d, and Amazon product reviews from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661.", "Twitter, and Amazon product reviews"], "best_model_answer": "Twitter dataset,  Amazon product reviews", "oracle_answer": "Twitter, and Amazon product reviews", "oracle_answer_rank": 3}
{"question_id": "9465d96a1368299fd3662d91aa94ba85347b4ccd", "question": "What is the performance of the best model?", "references": ["F1 Macro of 0.89", "LSTMs and GRU with attention which achieved 0.89 F1 score", "0.89 F1 score"], "best_model_answer": "F1 Macro of 0.89", "oracle_answer": "0.89 F1 score", "oracle_answer_rank": 3}
{"question_id": "e8c3f59313df20db0cdd49b84a37c44da849fe17", "question": "What are the models tested on the dataset?", "references": ["linear SVM, RBF SVM, linear classifier with SGDC, multinomial naive bayes, bernoulli naive bayes, pooled GRU, stacked LSTM with attention, LSTM and GRU with attention, 2d convolution with pooling, GRU with Capsule, LSTM with Capsule and attention, and BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning, The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features, Pooled GRU,  Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU , Stacked LSTM with Attention , LSTM and GRU with Attention, 2D Convolution with Pooling , GRU with Capsule,  LSTM with Capsule and Attention, BERT"], "best_model_answer": "linear SVM, RBF SVM, linear classifier with SGDC, multinomial naive bayes, bernoulli naive bayes, pooled GRU, stacked LSTM with attention, LSTM and GRU with attention, 2d convolution with pooling, GRU with Capsule, LSTM with Capsule and attention, and BERT", "oracle_answer": "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU , Stacked LSTM with Attention , LSTM and GRU with Attention, 2D Convolution with Pooling , GRU with Capsule,  LSTM with Capsule and Attention, BERT", "oracle_answer_rank": 3}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "question": "Which method best performs on the offensive language identification task?", "references": ["LSTM and GRU with Attention can be considered as the best model trained for OGTD", "LSTMs and GRU with attention", " a system using LSTMs and GRU with attention"], "best_model_answer": "LSTM and GRU with Attention can be considered as the best model trained for OGTD", "oracle_answer": "LSTMs and GRU with attention", "oracle_answer_rank": 2}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "question": "Did they use crowdsourcing for the annotations?", "references": ["No", "Yes", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "question": "How many annotators did they have?", "references": ["Three, plus 2 in case of disagreement below 66%.", "three", "three volunteers "], "best_model_answer": "Three, plus 2 in case of disagreement below 66%.", "oracle_answer": "three", "oracle_answer_rank": 2}
{"question_id": "5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "question": "Is the dataset balanced?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "question": "What models do they experiment on?", "references": [" Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features, Pooled GRU, Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU , Stacked LSTM with Attention , LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26, BERT BIBREF24"], "best_model_answer": " Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features, Pooled GRU, Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "oracle_answer": "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU , Stacked LSTM with Attention , LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "oracle_answer_rank": 2}
{"question_id": "211c242c028b35bb9cbd5e303bb6c750f859fd34", "question": "Do any of their reviews contain translations for both Catalan and Basque?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "9b05d5f723a8a452522907778a084b52e27fd924", "question": "What is the size of their published dataset?", "references": ["911", "The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343.", "910"], "best_model_answer": "911", "oracle_answer": "911", "oracle_answer_rank": 1}
{"question_id": "21175d8853fd906266f884bced85c598c35b1cbc", "question": "How many annotators do they have for their dataset?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "87c00edc497274ae6a972c3097818de85b1b384f", "question": "How does sentence construction component works?", "references": ["Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph."], "best_model_answer": "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "oracle_answer": "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "oracle_answer_rank": 1}
{"question_id": "de4e949c6917ff6933f5fa2a3062ba703aba014c", "question": "What are two use cases that demonstrate capability of created system?", "references": ["The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project, The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference", "natural language description for workflow created by the system built in the Phylotastic project, about people and includes descriptions for certain class", "The first application is to build a natural language description of the ontologies built in an evolutionary biology project called Phylotastic, so that biologists can understand the output, without knowledge of ontologies. The second aims to create an abstract or intermediate representation of the Wikipedia pages from the BlueSky session in 2018."], "best_model_answer": "The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project, The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference", "oracle_answer": "The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project, The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference", "oracle_answer_rank": 1}
{"question_id": "4cf05da602669a4c09c91ff5a1baae6e30adefdf", "question": "Do they explore how their word representations vary across languages?", "references": ["Yes", "Yes", "No"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "7380e62edcb11f728f6d617ee332dc8b5752b185", "question": "Which neural language model architecture do they use?", "references": ["character-level RNN", "standard stacked character-based LSTM BIBREF4", "LSTM"], "best_model_answer": "character-level RNN", "oracle_answer": "standard stacked character-based LSTM BIBREF4", "oracle_answer_rank": 2}
{"question_id": "f37b01e0c366507308fca44c20d3f69621b94a6e", "question": "How do they show genetic relationships between languages?", "references": ["hierarchical clustering", "By doing hierarchical clustering of word vectors", "By applying hierarchical clustering on language vectors found during training"], "best_model_answer": "hierarchical clustering", "oracle_answer": "By doing hierarchical clustering of word vectors", "oracle_answer_rank": 2}
{"question_id": "95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "question": "Did they test the idea that the system reduces the time needed to encode ADR reports on real pharmacologists? ", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "ab37ae82e38f64d3fa95782f2c791488f26cd43f", "question": "Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "question": "What are the steps in the MagiCoder algorithm?", "references": ["Preprocessing of the original text, Word-by-word linear scan of the description and \u201cvoting task\u201d, Weights calculation, Sorting of voted terms and winning terms release", "Procedure INLINEFORM0 takes the narrative description, performs tokenization and stop-word removal and puts it into an array of words., Procedures INLINEFORM0 and INLINEFORM1 get LLTs and create a dictionary of words and of their stemmed versions, respectively, By the functional notation INLINEFORM0 (resp., INLINEFORM1 ), we refer to the set of LLTs containing the word INLINEFORM2 (resp., the stem of INLINEFORM3 )., Function INLINEFORM0 returns the stemmed version of word INLINEFORM1 .\n\n, Function INLINEFORM0 returns the position of word INLINEFORM1 in term INLINEFORM2 .\n\n, INLINEFORM0 is a flag, initially set to 0, which holds 1 if at least a stemmed matching with the MedDRA term INLINEFORM1 is found., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are arrays and INLINEFORM3 appends INLINEFORM4 to array INLINEFORM5 , where INLINEFORM6 may be an element or a sequence of elements., INLINEFORM0 ( INLINEFORM1 ) are the weights related to the criteria defined in Section UID23 .\n\n, Procedure INLINEFORM0 performs the multi-value sorting of the array INLINEFORM1 based on the values of the properties INLINEFORM2 of its elements., Procedure INLINEFORM0 , where INLINEFORM1 is a set of terms and INLINEFORM2 is a term, tests whether INLINEFORM3 (considered as a string) is prefix of a term in INLINEFORM4 ., Dually, procedure INLINEFORM5 tests if in INLINEFORM6 there are one or more prefixes of INLINEFORM7 , and eventually remove them from INLINEFORM8 ., Function INLINEFORM0 specifies whether a word INLINEFORM1 has been already covered (i.e., a term voted by INLINEFORM2 has been selected) in the (partial) solution during the term release: INLINEFORM3 holds 1 if INLINEFORM4 has been covered (with or without stemming) and it holds 0 otherwise., Procedures INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 is a set of terms, implement ordered-phrases and maximal-set-of-voters criteria (defined in Section UID28 ), respectively., Function INLINEFORM0 , returns the first INLINEFORM1 elements of an ordered set INLINEFORM2 . If INLINEFORM3 , the function returns the complete list of ordered terms and INLINEFORM4 nil values.", "Definition of ad hoc data structures, Preprocessing of the original text, Word-by-word linear scan of the description and \u201cvoting task\u201d, Weights calculation, Sorting of voted terms and winning terms release"], "best_model_answer": "Preprocessing of the original text, Word-by-word linear scan of the description and \u201cvoting task\u201d, Weights calculation, Sorting of voted terms and winning terms release", "oracle_answer": "Definition of ad hoc data structures, Preprocessing of the original text, Word-by-word linear scan of the description and \u201cvoting task\u201d, Weights calculation, Sorting of voted terms and winning terms release", "oracle_answer_rank": 3}
{"question_id": "71413505d7d6579e2a453a1f09f4efd20197ab4b", "question": "How is the system constructed to be linear in the size of the narrative input and the terminology?", "references": ["The system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.", "main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms", "The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms., INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description."], "best_model_answer": "The system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.", "oracle_answer": "The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms., INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description.", "oracle_answer_rank": 3}
{"question_id": "3e6b6820e7843209495b4f9a72177573afaa4bc3", "question": "What conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder?", "references": ["Unanswerable", "Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects", "a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals, expect the proposed investigations of views on recovery by previously under-researched ethnic, language, and cultural groups to yield valuable insights on the appropriateness of the recovery approach for a wider population, The datasets collected in this project can serve as useful resources for future research"], "best_model_answer": "Unanswerable", "oracle_answer": "Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects", "oracle_answer_rank": 2}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "question": "What languages were included in this multilingual population?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "question": "What computational linguistic methods were used for the analysis?", "references": ["Unanswerable", "Unanswerable", "language identification"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "question": "Was permission sought from the bipolar patients to use this data?", "references": ["For Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.", "Unanswerable", "No"], "best_model_answer": "For Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.", "oracle_answer": "For Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.", "oracle_answer_rank": 1}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "question": "How are the individuals with bipolar disorder identified?", "references": ["characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12", " Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'.", "Twitter and Reddit users  are identified automatically  via self-reported diagnosis statements. Blog users are identified manually."], "best_model_answer": "characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12", "oracle_answer": "Twitter and Reddit users  are identified automatically  via self-reported diagnosis statements. Blog users are identified manually.", "oracle_answer_rank": 3}
{"question_id": "c2ce25878a17760c79031a426b6f38931cd854b2", "question": "What is the source of the training/testing data?", "references": ["CCPC1.0", "Two major forms(Jueju and Lvshi) of SHI and 121 major forms of CI from Chinese Classical Poerty Corpus (CCPC1.0)", "Chinese poem corpus with 250,000 Jueju and Lvshi, 20,000 CIs, 700,000 pairs of couplets"], "best_model_answer": "CCPC1.0", "oracle_answer": "Two major forms(Jueju and Lvshi) of SHI and 121 major forms of CI from Chinese Classical Poerty Corpus (CCPC1.0)", "oracle_answer_rank": 2}
{"question_id": "1d263356692ed8cdee2a13f103a82d98f43d66eb", "question": "What are the types of chinese poetry that are generated?", "references": ["SHI , CI ", "two major forms of SHI, Jueju, and Lvshi,, 121 major forms (Cipai) of CI ", "two primary categories, SHI and CI, SHI and CI can be further divided into many different types"], "best_model_answer": "SHI , CI ", "oracle_answer": "two major forms of SHI, Jueju, and Lvshi,, 121 major forms (Cipai) of CI ", "oracle_answer_rank": 2}
{"question_id": "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "question": "what is the previous work they are comparing to?", "references": ["RNN and Transformer baseline systems utilize BPE BIBREF3, fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work", "Subword based NMT, Character-based NMT", "RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides"], "best_model_answer": "RNN and Transformer baseline systems utilize BPE BIBREF3, fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work", "oracle_answer": "RNN and Transformer baseline systems utilize BPE BIBREF3, fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work", "oracle_answer_rank": 1}
{"question_id": "c7f43c95db3d0c870407cd0e7becdd802463683b", "question": "Do they use skip-gram word2vec?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "question": "How is quality of the word vectors measured?", "references": ["correlation between the word similarity scores by human judgment and the word distances in vector space, select the semantically closest word, from the candidate answers", "They evaluate it on the word analogy, word similarity and synonym selection tasks using Pearson correlation coefficient as the metric.", "Unanswerable"], "best_model_answer": "correlation between the word similarity scores by human judgment and the word distances in vector space, select the semantically closest word, from the candidate answers", "oracle_answer": "correlation between the word similarity scores by human judgment and the word distances in vector space, select the semantically closest word, from the candidate answers", "oracle_answer_rank": 1}
{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "question": "Do they report results only on English data?", "references": ["Unanswerable", "No", "Yes"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "question": "Where do the news texts come from?", "references": ["online news websites, New York Times, Japan Times", "the New York Times", "New York Times, Japan Times"], "best_model_answer": "online news websites, New York Times, Japan Times", "oracle_answer": "New York Times, Japan Times", "oracle_answer_rank": 3}
{"question_id": "07edc082eb86aecef3db5cad2534459c1310d6e8", "question": "What baseline is used for this task?", "references": ["FirstPhrases baseline, MultipartiteRank BIBREF17", " FirstPhrases baseline, MultipartiteRank", "FirstPhrase, MultipartiteRank"], "best_model_answer": "FirstPhrases baseline, MultipartiteRank BIBREF17", "oracle_answer": " FirstPhrases baseline, MultipartiteRank", "oracle_answer_rank": 2}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "question": "What type of nerual keyphrase generation models are trained?", "references": ["CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes)", "encoder-decoder model", "CopyRNN BIBREF2"], "best_model_answer": "CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes)", "oracle_answer": "CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes)", "oracle_answer_rank": 1}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "question": "How do the editors' annotations differ from those in existing datasets?", "references": ["Existing datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors", " news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm, provide additional tags which will be used by a taxonomy team to improve the algorithm", "Exper annotators use a smaller, more controlled indexing vocabulary."], "best_model_answer": "Existing datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors", "oracle_answer": "Existing datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors", "oracle_answer_rank": 1}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "question": "How long is their dataset?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "question": "Do they use pretrained word embeddings?", "references": ["No", "Yes", "Yes"], "best_model_answer": "No", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "question": "How many layers does their model have?", "references": ["6", "6", "6 layers"], "best_model_answer": "6", "oracle_answer": "6", "oracle_answer_rank": 1}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "question": "What metrics do they use?", "references": ["F-measure", "F-measure", "F-measure"], "best_model_answer": "F-measure", "oracle_answer": "F-measure", "oracle_answer_rank": 1}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "question": "what dataset did they use?", "references": ["DUC-2001 dataset BIBREF6, Inspec dataset, NUS Keyphrase Corpus BIBREF10, ICSI Meeting Corpus", "DUC-2001, Inspec ,  NUS Keyphrase Corpus,  ICSI Meeting Corpus ", "DUC-2001 dataset, Inspec dataset, NUS Keyphrase Corpus, ICSI Meeting Corpus"], "best_model_answer": "DUC-2001 dataset BIBREF6, Inspec dataset, NUS Keyphrase Corpus BIBREF10, ICSI Meeting Corpus", "oracle_answer": "DUC-2001 dataset, Inspec dataset, NUS Keyphrase Corpus, ICSI Meeting Corpus", "oracle_answer_rank": 3}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "question": "what was their model's f1 score?", "references": ["On DUC 27.53, on Inspec 27.01, on ICSI 4.30, and on Nus 9.10", "27.53, 27.01, 4.30 and 9.10 for DUC, Inspec, ICSI and Nus datasets respectively.", "F1 score their system achieved is 27.53, 27.01, 4.30 and 9.10 on DUC, Inspec, ICSI and NUS dataset respectively."], "best_model_answer": "On DUC 27.53, on Inspec 27.01, on ICSI 4.30, and on Nus 9.10", "oracle_answer": "27.53, 27.01, 4.30 and 9.10 for DUC, Inspec, ICSI and Nus datasets respectively.", "oracle_answer_rank": 2}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "question": "what are the state of the art models?", "references": [" SingleRank and Topical PageRank", "SingleRank and Topical PageRank", "SingleRank, Topical PageRank"], "best_model_answer": " SingleRank and Topical PageRank", "oracle_answer": " SingleRank and Topical PageRank", "oracle_answer_rank": 1}
{"question_id": "2711ae6dd532d136295c95253dbf202e37ecd3e7", "question": "How do you know the word alignments are correct?", "references": ["we use the word alignment information from the ART model", "In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model,", "we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48)."], "best_model_answer": "we use the word alignment information from the ART model", "oracle_answer": "In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model,", "oracle_answer_rank": 2}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "question": "How slow is the unparallelizable ART model in the first place?  ", "references": ["784 miliseconds", "Unanswerable", "While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory"], "best_model_answer": "784 miliseconds", "oracle_answer": "784 miliseconds", "oracle_answer_rank": 1}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "question": "What metric is used to measure translation accuracy?", "references": ["BLEU ", "BLEU score", "BLUE and the percentage of repetitive words"], "best_model_answer": "BLEU ", "oracle_answer": "BLEU ", "oracle_answer_rank": 1}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "question": "Were any datasets other than WMT used to test the model?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "question": "Are the results applicable to other language pairs than German-English?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "question": "What dicrimating features are discovered?", "references": ["Highest gain is obtained by average, maximum, minimum, and standard deviation. Probability and proportionality features have low information gain", "average, maximum and minimum, standard deviation", "a document's terms' minimum, maximum, average (relative to all terms and to in-vocabulary terms), and standard deviation of weights; and proportion of terms that are in-vocabulary"], "best_model_answer": "Highest gain is obtained by average, maximum, minimum, and standard deviation. Probability and proportionality features have low information gain", "oracle_answer": "average, maximum and minimum, standard deviation", "oracle_answer_rank": 2}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "question": "What results are obtained on the alternate datasets?", "references": ["Accuracy results range from 74.4 to 100 ", " three representations obtained comparative results and support the robustness of the low dimensionality representation", "Comparable to state-of-the-art"], "best_model_answer": "Accuracy results range from 74.4 to 100 ", "oracle_answer": "Accuracy results range from 74.4 to 100 ", "oracle_answer_rank": 1}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "question": "Are answers in this dataset guaranteed to be substrings of the text? If not, what is the coverage of answers being substrings?", "references": ["Yes", "No, the answers can also be summaries or yes/no.", "Unanswerable"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "question": "How much is the gap between pretraining on SQuAD and not pretraining on SQuAD?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "question": "What is the machine learning method used to make the predictions?", "references": ["SGNN", "SGNN, Word, BIBREF23, Event, BIBREF24, NTN, BIBREF4, KGEB, BIBREF18 ", "Compositional Neural Network, Element-wise Multiplicative Composition, Neural Tensor Network"], "best_model_answer": "SGNN", "oracle_answer": "SGNN", "oracle_answer_rank": 1}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "question": "How is the event prediction task evaluated?", "references": ["accuracy", "replacing the event embeddings on SGNN and running it on the MCNC dataset", "we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings"], "best_model_answer": "accuracy", "oracle_answer": "replacing the event embeddings on SGNN and running it on the MCNC dataset", "oracle_answer_rank": 2}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "question": "What are the datasets used in the paper?", "references": ["ATOMIC, hard similarity small and big dataset, the transitive sentence similarity dataset, the standard multiple choice narrative cloze (MCNC) dataset", "ATOMIC , MCNC", "ATOMIC, New York Times Gigaword, an unreleased extension of the dataset by BIBREF5, MCNC"], "best_model_answer": "ATOMIC, hard similarity small and big dataset, the transitive sentence similarity dataset, the standard multiple choice narrative cloze (MCNC) dataset", "oracle_answer": "ATOMIC, New York Times Gigaword, an unreleased extension of the dataset by BIBREF5, MCNC", "oracle_answer_rank": 3}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "question": "Is there a difference between the model's performance for morphologically impoverished and morphologically complex languages?", "references": ["They did not report results for English but expect that morphologically complex languages will perform better.", "Unanswerable", "Yes"], "best_model_answer": "They did not report results for English but expect that morphologically complex languages will perform better.", "oracle_answer": "They did not report results for English but expect that morphologically complex languages will perform better.", "oracle_answer_rank": 1}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "question": "What languages do they apply the model to?", "references": ["English", "English", "English", "English"], "best_model_answer": "English", "oracle_answer": "English", "oracle_answer_rank": 1}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "question": "How are the embeddings evaluated in the human judgement comparison?", "references": ["human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments", "human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments", "Using cosine similarity between the embeddings which is then correlated with human judgement"], "best_model_answer": "human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments", "oracle_answer": "human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments", "oracle_answer_rank": 1}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "question": "what was the margin their system outperformed previous ones?", "references": ["15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500", "15.6 and 16.5 for accuracy and NDCG on MCTest-150, 7.3 and 4.6 on MCTest-500.", "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500"], "best_model_answer": "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500", "oracle_answer": "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500", "oracle_answer_rank": 1}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "question": "what prior approaches did they compare to?", "references": ["Addition, Addition-proj, Neural Reasoner, Attentive Reader", "Neural Reasoner, Attentive Reader", "The Neural Reasoner, The Attentive Reader"], "best_model_answer": "Addition, Addition-proj, Neural Reasoner, Attentive Reader", "oracle_answer": "Neural Reasoner, Attentive Reader", "oracle_answer_rank": 2}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "question": "what are the baselines?", "references": ["one-stage RNN system containing 2-layer BLSTM", "one-stage RNN system", "a one-stage RNN system"], "best_model_answer": "one-stage RNN system containing 2-layer BLSTM", "oracle_answer": "one-stage RNN system", "oracle_answer_rank": 2}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "question": "what results do they achieve?", "references": [" relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline, accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi", "state-of-the-art in the Chinese dialect recognition task", " The relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task (88.88 and 87.24)  relative to the baseline  78.85."], "best_model_answer": " relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline, accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi", "oracle_answer": " relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline, accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi", "oracle_answer_rank": 1}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "question": "what chinese dialects are explored?", "references": ["Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian", "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka, Fujian", "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian"], "best_model_answer": "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian", "oracle_answer": "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian", "oracle_answer_rank": 1}
{"question_id": "94d794df4a3109522c2ea09dad5d40e55d35df51", "question": "Which neural machine translation model was used?", "references": ["SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8", "Unanswerable", "SGNMT"], "best_model_answer": "SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8", "oracle_answer": "SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8", "oracle_answer_rank": 1}
{"question_id": "044c66c6b7ff7378682f24887b05e1af79dcd04f", "question": "What position did this entry finish in, in the overall shared task?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "question": "What are the restrictions of the restricted track?", "references": ["explore the potential of purely neural models for grammatical error correction", "The organizers provided a dataset allowed to use for training", "goal on the restricted track was to explore the potential of purely neural models for grammatical error correction"], "best_model_answer": "explore the potential of purely neural models for grammatical error correction", "oracle_answer": "goal on the restricted track was to explore the potential of purely neural models for grammatical error correction", "oracle_answer_rank": 3}
{"question_id": "ab95ca983240ad5289c123a2774f8e0db424f4a1", "question": "What does BEA stand for?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "question": "Which works better according to human evaluation, the concurrent or the modular system?", "references": ["Modular", "Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text.", "They are equal"], "best_model_answer": "Modular", "oracle_answer": "Modular", "oracle_answer_rank": 1}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "question": "Were the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it?", "references": ["Wiki community effort", "Wikipedia editors", " Wikipedia edits"], "best_model_answer": "Wiki community effort", "oracle_answer": "Wikipedia editors", "oracle_answer_rank": 2}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "question": "How is subjective text automatically neutralized?", "references": [" Identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy and suggest edits that would make it more neutral.", "The text is modified to remove the subjective bias while preserve the meaning as much as possible", "algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed"], "best_model_answer": " Identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy and suggest edits that would make it more neutral.", "oracle_answer": "algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed", "oracle_answer_rank": 3}
{"question_id": "1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "question": "What is the sign language recognition task investigated?", "references": ["We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. ", " American Sign Language recognition ", "Unanswerable"], "best_model_answer": "We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. ", "oracle_answer": "We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. ", "oracle_answer_rank": 1}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "question": "What is the performance of the best model in the sign language recognition task?", "references": ["Spatial AI-LSTM", "Accuracy 81%", "Best performing model is Spatial AI-LSTM with accuracy 81% and Std. Deviation 6%"], "best_model_answer": "Spatial AI-LSTM", "oracle_answer": "Best performing model is Spatial AI-LSTM with accuracy 81% and Std. Deviation 6%", "oracle_answer_rank": 3}
{"question_id": "f85520bbc594918968d7d9f33d11639055458344", "question": "What are the deep learning architectures used?", "references": ["Axis Independent Architecture (AI-LSTM), Spatial AI-LSTM,  Max CNN-LSTM, 3D CNN", "Recurrent Neural Networks (RNN), 3D Convolutional Neural Network, Axis Independent LSTM, Spatial AI-LSTM, Max CNN-LSTM network", "3D CNN, Axis independent LSTM,  spatial axis independent LSTM, and combined network "], "best_model_answer": "Axis Independent Architecture (AI-LSTM), Spatial AI-LSTM,  Max CNN-LSTM, 3D CNN", "oracle_answer": "Axis Independent Architecture (AI-LSTM), Spatial AI-LSTM,  Max CNN-LSTM, 3D CNN", "oracle_answer_rank": 1}
{"question_id": "e4f2d59030b17867449cf5456118ab722296bebd", "question": "Who made the stated claim (that \"this is because character-level models learn morphology\")?", "references": ["Unanswerable", "Chung et al. (2016)", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "e664b58ea034a638e7142f8a393a88aadd1e215e", "question": "Which languages do they use?", "references": ["Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew", "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew", "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, Hebrew"], "best_model_answer": "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew", "oracle_answer": "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew", "oracle_answer_rank": 1}
{"question_id": "c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "question": "Do the character-level models perform better than models with access to morphological analyses only?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "question": "What is case syncretism?", "references": ["A situation in which a noun's syntactic function is ambiguous without context.", "The phenomena where words that have the same form express different morphological cases", "when noun case is ambiguous"], "best_model_answer": "A situation in which a noun's syntactic function is ambiguous without context.", "oracle_answer": "A situation in which a noun's syntactic function is ambiguous without context.", "oracle_answer_rank": 1}
{"question_id": "2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "question": "Do humans assess the quality of the generated responses?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "question": "What models are used to generate responses?", "references": ["Seq2Seq, Variational Auto-Encoder (VAE), Reinforcement Learning (RL)", "Seq2Seq BIBREF25, BIBREF24, Variational Auto-Encoder (VAE) BIBREF26, Reinforcement Learning (RL)", "Seq2Seq BIBREF25, BIBREF24, Variational Auto-Encoder (VAE) BIBREF26, Reinforcement Learning (RL)"], "best_model_answer": "Seq2Seq, Variational Auto-Encoder (VAE), Reinforcement Learning (RL)", "oracle_answer": "Seq2Seq BIBREF25, BIBREF24, Variational Auto-Encoder (VAE) BIBREF26, Reinforcement Learning (RL)", "oracle_answer_rank": 2}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "question": "What types of hate speech are considered?", "references": ["Unanswerable", " Potentially hateful comments are identified using hate keywords.", "race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability."], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "a4cf0cf372f62b2dbc7f31c600c6c66246263328", "question": "Which baselines to they compare to?", "references": ["AllText, F8W", "AllText, F8W", "AllText, F8W"], "best_model_answer": "AllText, F8W", "oracle_answer": "AllText, F8W", "oracle_answer_rank": 1}
{"question_id": "f7b91b99279833f9f489635eb8f77c6d13136098", "question": "Which sentence compression technique works best?", "references": ["ESC model", "semi-supervised ESC model ", "ESC model "], "best_model_answer": "ESC model", "oracle_answer": "ESC model", "oracle_answer_rank": 1}
{"question_id": "99e514acc0109b7efa4e3860ce1e8c455f5bb790", "question": "Do they compare performance against state of the art systems?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "question": "What is the performance of large state-of-the-art models on these datasets?", "references": ["Average  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5", "Akbik et al. (2019) -  89.3 on Ontonotes 5\nBaevski et al. (2019) 93.5 on CoNLL-03", "93.5"], "best_model_answer": "Average  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5", "oracle_answer": "Akbik et al. (2019) -  89.3 on Ontonotes 5\nBaevski et al. (2019) 93.5 on CoNLL-03", "oracle_answer_rank": 2}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "question": "What is used as a baseline model?", "references": ["Neural CRF model with and without ELMo embeddings", "Neural CRF model with and without ELMo embeddings", "Neural CRF model with and without ELMo embeddings"], "best_model_answer": "Neural CRF model with and without ELMo embeddings", "oracle_answer": "Neural CRF model with and without ELMo embeddings", "oracle_answer_rank": 1}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "question": "How do they build gazetter resources from Wikipedia knowlege base?", "references": ["process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long, we use the sitelink count to keep the six most popular types, To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure", "Extract entity type tuples at appropriate level of granularity depending on the NER task.", "To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State."], "best_model_answer": "process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long, we use the sitelink count to keep the six most popular types, To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure", "oracle_answer": "To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State.", "oracle_answer_rank": 3}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "question": "What is the dataset that is used to train the embeddings?", "references": [" LibriSpeech BIBREF46", "LibriSpeech", "LibriSpeech"], "best_model_answer": " LibriSpeech BIBREF46", "oracle_answer": "LibriSpeech", "oracle_answer_rank": 2}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "question": "What speaker characteristics are used?", "references": ["speaker characteristics, microphone characteristics, background noise", "Unanswerable", "Acoustic factors such as speaker characteristics, microphone characteristics, background noise."], "best_model_answer": "speaker characteristics, microphone characteristics, background noise", "oracle_answer": "speaker characteristics, microphone characteristics, background noise", "oracle_answer_rank": 1}
{"question_id": "fef5b65263c81299acc350a101dabaf5a8cb9c6e", "question": "What language is used for the experiments?", "references": ["English", "English", "English"], "best_model_answer": "English", "oracle_answer": "English", "oracle_answer_rank": 1}
{"question_id": "f40e23adc8245562c8677f0f86fa5175179b5422", "question": "Is the embedding model test in any downstream task?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "question": "what is the baseline model", "references": ["two baseline models TF-IDF and LSI which only use Cosine similarity", "two baseline models TF-IDF and LSI", "The baseline models used for this paper are based on the TF-IDF and LSI features and cosine similarity as a retrieval method.", "For the first task they have two baseline models, TF-IDF and LSI which both use cosine similarity. For the QA task, they baseline models were the original CNN and CNN with separate TF-IDF, LSI features."], "best_model_answer": "two baseline models TF-IDF and LSI which only use Cosine similarity", "oracle_answer": "two baseline models TF-IDF and LSI which only use Cosine similarity", "oracle_answer_rank": 1}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "question": "What sizes were their datasets?", "references": ["ast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours", "150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data", "20 hours of training data, dev and test sets comprise 4.5 hours of speech"], "best_model_answer": "ast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours", "oracle_answer": "20 hours of training data, dev and test sets comprise 4.5 hours of speech", "oracle_answer_rank": 3}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "question": "How many layers does their model have?", "references": ["10 ", "two ", "two CNN layers, three-layer bi-directional long short-term memory network (LSTM), followed by a three-layer LSTM"], "best_model_answer": "10 ", "oracle_answer": "two ", "oracle_answer_rank": 2}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "question": "What is their model's architecture?", "references": [" the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2", "encoder-decoder model, end-to-end system architecture", "two CNN layers, three-layer bi-directional long short-term memory network (LSTM),  followed by a three-layer LSTM"], "best_model_answer": " the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2", "oracle_answer": " the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2", "oracle_answer_rank": 1}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "question": "What languages did they use?", "references": ["Spanish, English , Chinese , Mandarin Chinese , Croatian , Czech , French , Polish , Portuguese , Swedish ", "Spanish, English, Mandarin Chinese, Croatian, Czech, French, Polish, Portuguese, Swedish", "Spanish-English"], "best_model_answer": "Spanish, English , Chinese , Mandarin Chinese , Croatian , Czech , French , Polish , Portuguese , Swedish ", "oracle_answer": "Spanish, English , Chinese , Mandarin Chinese , Croatian , Czech , French , Polish , Portuguese , Swedish ", "oracle_answer_rank": 1}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "question": "Are there experiments with real data?", "references": ["Yes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech.", "No", "Yes"], "best_model_answer": "Yes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech.", "oracle_answer": "Yes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech.", "oracle_answer_rank": 1}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "question": "What supervised machine learning models do they use?", "references": ["ZeroR, Na\u00efve Bayes, J48, and random forest classifiers", "ZeroR, Na\u00efve Bayes, J48, and random forest ", "They use four classifiers: ZeroR, Naive Bayes, J48, and random forest."], "best_model_answer": "ZeroR, Na\u00efve Bayes, J48, and random forest classifiers", "oracle_answer": "ZeroR, Na\u00efve Bayes, J48, and random forest classifiers", "oracle_answer_rank": 1}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "question": "Does the supervised machine learning approach outperform previous work?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "question": "How large is the released data set?", "references": ["1470 sentences", "316 sentences in Hypertension corpus, 877 sentences in Rhinosinusitis corpus", "Unanswerable"], "best_model_answer": "1470 sentences", "oracle_answer": "1470 sentences", "oracle_answer_rank": 1}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "question": "What is an example of a condition-action pair?", "references": ["Unanswerable", "Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation", "If patients have asthma, then beta-blockers, including eye drops, are contraindicated"], "best_model_answer": "Unanswerable", "oracle_answer": "Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation", "oracle_answer_rank": 2}
{"question_id": "97708d93bccc832ea671dc31a76dad6a121fcd60", "question": "Which metrics were considered?", "references": ["ter, bleu , rouge , nist, lepor, cider, meteor, Semantic Text Similarity,  Flesch Reading Ease , characters per utterance (len) and per word (cpw), words per sentence, syllables per sentence (sps) and per word (spw), polysyllabic words per utterance (pol) and per word (ppw), the number of misspellings (msp) and the parsing score as returned by the Stanford parser (prs)", "ter, bleu, rouge, nist, lepor, cider, meteor, Semantic Similarity (sim), readability and grammaticality", "ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23, Semantic Similarity (sim), readability and grammaticality"], "best_model_answer": "ter, bleu , rouge , nist, lepor, cider, meteor, Semantic Text Similarity,  Flesch Reading Ease , characters per utterance (len) and per word (cpw), words per sentence, syllables per sentence (sps) and per word (spw), polysyllabic words per utterance (pol) and per word (ppw), the number of misspellings (msp) and the parsing score as returned by the Stanford parser (prs)", "oracle_answer": "ter, bleu, rouge, nist, lepor, cider, meteor, Semantic Similarity (sim), readability and grammaticality", "oracle_answer_rank": 2}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "question": "What NLG tasks were considered?", "references": ["provide information about hotels and restaurants in San Francisco, provides information about restaurants in Cambridge", "utterance generation for spoken dialogue systems, provide information about hotels and restaurants in San Francisco, provides information about restaurants in Cambridge", "rnnlg, TGen,  lols"], "best_model_answer": "provide information about hotels and restaurants in San Francisco, provides information about restaurants in Cambridge", "oracle_answer": "provide information about hotels and restaurants in San Francisco, provides information about restaurants in Cambridge", "oracle_answer_rank": 1}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "question": "what state of the art methods are compared to?", "references": ["CLASSY04, ICSI, Submodular, DPP, RegSum", "CLASSY04, ICSI, Submodular, DPP and RegSum.", "CLASSY04, ICSI, Submodular, DPP, RegSum"], "best_model_answer": "CLASSY04, ICSI, Submodular, DPP, RegSum", "oracle_answer": "CLASSY04, ICSI, Submodular, DPP, RegSum", "oracle_answer_rank": 1}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "question": "what are the performance metrics?", "references": ["Rouge-1, Rouge-2 and Rouge-4 recall", "Rouge-1 recall, Rouge-2 recall, Rouge-4 recall", "Rouge-1, Rouge-2 and Rouge-4 recall"], "best_model_answer": "Rouge-1, Rouge-2 and Rouge-4 recall", "oracle_answer": "Rouge-1, Rouge-2 and Rouge-4 recall", "oracle_answer_rank": 1}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "question": "what is the original model they refer to?", "references": ["BIBREF0 , BIBREF6", "Original centroid-based model by BIBREF5", "it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection"], "best_model_answer": "BIBREF0 , BIBREF6", "oracle_answer": "BIBREF0 , BIBREF6", "oracle_answer_rank": 1}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "question": "how are sentences selected prior to making the summary?", "references": ["Using three algorithms: N-first, N-best and New-TF-IDF.", "Sentences are selected using 3 different greedy selection algorithms.", "All words in the vocabulary are ranked by their value in the centroid vector. Then the ranked list of sentences is de-queued in decreasing order."], "best_model_answer": "Using three algorithms: N-first, N-best and New-TF-IDF.", "oracle_answer": "Sentences are selected using 3 different greedy selection algorithms.", "oracle_answer_rank": 2}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "question": "Do they evaluate only on English datasets?", "references": ["The twitter dataset is English-only; no information for the reddit dataset is given", "Yes", "Yes"], "best_model_answer": "The twitter dataset is English-only; no information for the reddit dataset is given", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "question": "What type of frequency analysis was used?", "references": ["mean of occurrence per utterance and the standard deviation (SD) of each marker", "based on their occurrence", "Mean of occurrence per utterance and the standard deviation is calculated for every marker type; the means between each pair of types is compared via independent t-tests"], "best_model_answer": "mean of occurrence per utterance and the standard deviation (SD) of each marker", "oracle_answer": "mean of occurrence per utterance and the standard deviation (SD) of each marker", "oracle_answer_rank": 1}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "question": "What type of classifiers were used?", "references": ["Support Vector Machines (SVM) classifier with linear kernel BIBREF16", "Support Vector Machines (SVM) classifier with linear kernel BIBREF16 ", "Support Vector Machines (SVM) classifier with linear kernel"], "best_model_answer": "Support Vector Machines (SVM) classifier with linear kernel BIBREF16", "oracle_answer": "Support Vector Machines (SVM) classifier with linear kernel BIBREF16", "oracle_answer_rank": 1}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "question": "Who annotated the Twitter and Reddit data for irony?", "references": ["collected using hashtags, such as #irony, #sarcasm, and #sarcastic", "Authors of the tweets and reddit posts", "Twitter and Reddit users of the original data "], "best_model_answer": "collected using hashtags, such as #irony, #sarcasm, and #sarcastic", "oracle_answer": "Authors of the tweets and reddit posts", "oracle_answer_rank": 2}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "question": "what resources are combined to build the labeler?", "references": ["multilingual word vectors, training data across languages", "a sequence of pretrained embeddings for the surface forms of the sentence tokens, annotations for a single predicate, CoNLL 2009 dataset", "multilingual word vectors, concatenate a language ID vector to each multilingual word embedding"], "best_model_answer": "multilingual word vectors, training data across languages", "oracle_answer": "multilingual word vectors, training data across languages", "oracle_answer_rank": 1}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "question": "what datasets were used?", "references": ["semantic role labeling portion of the CoNLL-2009 shared task BIBREF0", "CoNLL 2009 dataset", "semantic role labeling portion of the CoNLL-2009 shared task"], "best_model_answer": "semantic role labeling portion of the CoNLL-2009 shared task BIBREF0", "oracle_answer": "semantic role labeling portion of the CoNLL-2009 shared task BIBREF0", "oracle_answer_rank": 1}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "question": "what is the monolingual baseline?", "references": ["For each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.", " basic model adapts the span-based dependency SRL model of He2017-deepsrl", "biLSTM with pre-trained GloVe embeddings."], "best_model_answer": "For each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.", "oracle_answer": "For each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.", "oracle_answer_rank": 1}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "question": "what languages are explored in this paper?", "references": ["Catalan, Chinese, Czech, English, German, Japanese, Spanish", "Catalan, Chinese, Czech, English, German, Japanese, Spanish", " Catalan, Chinese, Czech, English, German, Japanese and Spanish"], "best_model_answer": "Catalan, Chinese, Czech, English, German, Japanese, Spanish", "oracle_answer": "Catalan, Chinese, Czech, English, German, Japanese, Spanish", "oracle_answer_rank": 1}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "question": "Does their model use MFCC?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "question": "What is the problem of session segmentation?", "references": ["ot all sentences in the current conversation session are equally important,  irrelevant to the current context, and should not be considered when the computer synthesizes the reply", "To retain near and context relevant dialog session utterances and to discard far, irrelevant ones.", "Retaining relevant contextual information from previous utterances. "], "best_model_answer": "ot all sentences in the current conversation session are equally important,  irrelevant to the current context, and should not be considered when the computer synthesizes the reply", "oracle_answer": "To retain near and context relevant dialog session utterances and to discard far, irrelevant ones.", "oracle_answer_rank": 2}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "question": "What dataset do they use?", "references": ["real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances", "chatting corpus from DuMi and conversation data from Douban forum", "chatting corpus from DuMi"], "best_model_answer": "real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances", "oracle_answer": "chatting corpus from DuMi", "oracle_answer_rank": 3}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "question": "Was the filtering based on fluency and domain relevance done automatically?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "question": "How was domain relevance estimated?", "references": ["For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.", "the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document", "we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$"], "best_model_answer": "For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.", "oracle_answer": "For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.", "oracle_answer_rank": 1}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "question": "How many hand-crafted templates did they have to make?", "references": ["269.", "269", "106, 163"], "best_model_answer": "269.", "oracle_answer": "269.", "oracle_answer_rank": 1}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "question": "How was the fluency measured?", "references": ["For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.", "$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count", "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count"], "best_model_answer": "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.", "oracle_answer": "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.", "oracle_answer_rank": 1}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "question": "What data is used in this work?", "references": ["Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data", "post-game interviews from ASAP Sport's website", "tennis post-match interview transcripts, live text play-by-play commentaries"], "best_model_answer": "Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data", "oracle_answer": "Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data", "oracle_answer_rank": 1}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "question": "What dataset is used?", "references": ["the XKCD color dataset, the Caltech\u2013UCSD Birds dataset", "XKCD color dataset, Caltech\u2013UCSD Birds dataset, actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game", "XKCD color dataset; Caltech-UCSD Birds dataset; game data from Amazon Mechanical Turk workers "], "best_model_answer": "the XKCD color dataset, the Caltech\u2013UCSD Birds dataset", "oracle_answer": "XKCD color dataset; Caltech-UCSD Birds dataset; game data from Amazon Mechanical Turk workers ", "oracle_answer_rank": 3}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "question": "How was the dataset collected?", "references": ["data was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice", "crowdsourcing", "using crowdsourcing"], "best_model_answer": "data was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice", "oracle_answer": "using crowdsourcing", "oracle_answer_rank": 3}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "question": "what evaluation metrics were used?", "references": ["Accuracy,  MAE: Mean Absolute Error ", "MAE: Mean Absolute Error, Accuracy$\\pm k$", "MAE: Mean Absolute Error, Accuracy$\\pm k$"], "best_model_answer": "Accuracy,  MAE: Mean Absolute Error ", "oracle_answer": "MAE: Mean Absolute Error, Accuracy$\\pm k$", "oracle_answer_rank": 2}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "question": "What datasets are used?", "references": ["Craigslist Bargaining dataset (CB)", "Craigslist Bargaining dataset (CB)", "Craigslist Bargaining dataset (CB) "], "best_model_answer": "Craigslist Bargaining dataset (CB)", "oracle_answer": "Craigslist Bargaining dataset (CB)", "oracle_answer_rank": 1}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "question": "how did they measure grammatical correctness?", "references": ["Identify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.", "They measure grammatical correctness by checking whether a sentence has the same sequence of POS tags.", "identify for each sentence of the speech its POS tags, Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct., points in a certain direction, evaluate those sentences manually"], "best_model_answer": "Identify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.", "oracle_answer": "Identify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.", "oracle_answer_rank": 1}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "question": "how was quality of sentence transition measured?", "references": ["Manually, using the criterion score between 0 and 3.", "The quality of sentence transition was measured manually by checking how well do consecutive sentences connect", "Manually evaluated on scale 0 to 3."], "best_model_answer": "Manually, using the criterion score between 0 and 3.", "oracle_answer": "Manually evaluated on scale 0 to 3.", "oracle_answer_rank": 3}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "question": "what is the size of the dataset?", "references": ["3857 speech segments", "2771 speeches containing 50871 sentences", "3857 speech segments from 53 US Congressional floor debates"], "best_model_answer": "3857 speech segments", "oracle_answer": "3857 speech segments", "oracle_answer_rank": 1}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "question": "what manual evaluation is presented?", "references": ["Manual evaluation of four evaluation criteria:   grammatical correctness,  sentence transitions, speech structure, and speech content. ", "generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it", "The manual evaluation contains 4 criteria to check grammatical correctness, sentence transitions, speech structure, and speech content of the generated speech and assigning a score between 0 to 3 for each criterion"], "best_model_answer": "Manual evaluation of four evaluation criteria:   grammatical correctness,  sentence transitions, speech structure, and speech content. ", "oracle_answer": "The manual evaluation contains 4 criteria to check grammatical correctness, sentence transitions, speech structure, and speech content of the generated speech and assigning a score between 0 to 3 for each criterion", "oracle_answer_rank": 3}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "question": "What downstream tasks are analyzed?", "references": ["sentiment classification, question answering", "General Language Understanding, question answering task (SQuAD v1.1 - BIBREF14), classification task (IMDb sentiment classification - BIBREF13)", "a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14)."], "best_model_answer": "sentiment classification, question answering", "oracle_answer": "a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).", "oracle_answer_rank": 3}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "question": "How much time takes the training of DistilBERT?", "references": ["on 8 16GB V100 GPUs for approximately 90 hours", "90 hours", "Unanswerable"], "best_model_answer": "on 8 16GB V100 GPUs for approximately 90 hours", "oracle_answer": "on 8 16GB V100 GPUs for approximately 90 hours", "oracle_answer_rank": 1}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "question": "Which datasets do they use?", "references": ["LibriSpeech corpus BIBREF46, GlobalPhone corpus BIBREF47", "LibriSpeech corpus, GlobalPhone corpus", "LibriSpeech, GlobalPhone"], "best_model_answer": "LibriSpeech corpus BIBREF46, GlobalPhone corpus BIBREF47", "oracle_answer": "LibriSpeech corpus, GlobalPhone corpus", "oracle_answer_rank": 2}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "question": "How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?", "references": ["They compare retrieval performance in MAP.", "They compare MAP performance of query-by-example STD using representations obtained from naive encoder and their method", "MAP, MAP results on large testing database (250K segments)"], "best_model_answer": "They compare retrieval performance in MAP.", "oracle_answer": "They compare retrieval performance in MAP.", "oracle_answer_rank": 1}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "question": "Which pairs of languages do they consider similar enough to capture phonetic structure?", "references": ["German and French", "English paired with any of the following: French, German, Czech, Spanish.", "English, German and French"], "best_model_answer": "German and French", "oracle_answer": "English, German and French", "oracle_answer_rank": 3}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "question": "Did they try Roberta?", "references": ["Unanswerable", "No", "No"], "best_model_answer": "Unanswerable", "oracle_answer": "No", "oracle_answer_rank": 2}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "question": "What are their results on this task?", "references": ["98.11% accuracy with a 0.4% improvement upon the benchmark model", " BiLSTM based framework and the linear classifier reaches a 97.97% accuracy, SVM classifier reaches a remarkable 98.11% accuracy", "F1 score of 97.97 for a linear classifier and 98.11 for a SVM classifier"], "best_model_answer": "98.11% accuracy with a 0.4% improvement upon the benchmark model", "oracle_answer": " BiLSTM based framework and the linear classifier reaches a 97.97% accuracy, SVM classifier reaches a remarkable 98.11% accuracy", "oracle_answer_rank": 2}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "question": "How is the text segmented?", "references": ["dividing documents into chunks before processing", "Unanswerable", "They simply split document in chunks, get embedding for each chunk and train BiLSTM models with embeddings."], "best_model_answer": "dividing documents into chunks before processing", "oracle_answer": "dividing documents into chunks before processing", "oracle_answer_rank": 1}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "question": "what are the state of the art models?", "references": ["The character-aware neural language model BIBREF1, Hierarchical attention networks BIBREF10, FastText BIBREF11 ", "character-aware neural language model BIBREF1, Hierarchical attention networks BIBREF10, FastText BIBREF11", "The character-aware neural language model, Hierarchical attention networks, FastText"], "best_model_answer": "The character-aware neural language model BIBREF1, Hierarchical attention networks BIBREF10, FastText BIBREF11 ", "oracle_answer": "The character-aware neural language model BIBREF1, Hierarchical attention networks BIBREF10, FastText BIBREF11 ", "oracle_answer_rank": 1}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "question": "How many parameters does their noisy channel model have?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "question": "Which language pairs do they evaluate on?", "references": ["English-German, Chinese-English", "English-German; Chinese-English; German-English", "En-De, De-En, Zh-En, Englsh-Russian and Russian-English"], "best_model_answer": "English-German, Chinese-English", "oracle_answer": "English-German, Chinese-English", "oracle_answer_rank": 1}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "question": "How large the improvement margin is?", "references": ["+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500", "Average F1 improvement of 5.07", "+7.24, +11.03, +14.67, +5.07 for 2000, 1000, 500 and zero training instances respectively"], "best_model_answer": "+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500", "oracle_answer": "+7.24, +11.03, +14.67, +5.07 for 2000, 1000, 500 and zero training instances respectively", "oracle_answer_rank": 3}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "question": "Which languages do they explore?", "references": ["Irish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia", "Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil", "Irish, Gujarati, Hindi, Arabic, English, Spanish, French, German, Tamil, Bengali, Odia, Marathi, Telugu, Hinglish"], "best_model_answer": "Irish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia", "oracle_answer": "Irish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia", "oracle_answer_rank": 1}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "question": "What are two baseline methods?", "references": ["Joint Neural Embedding (JNE)\nAdaMine", "Answer with content missing: (Table1 merged with Figure 3) Joint Neural\nEmbedding (JNE) and AdaMine", "JNE and AdaMine"], "best_model_answer": "Joint Neural Embedding (JNE)\nAdaMine", "oracle_answer": "Joint Neural Embedding (JNE)\nAdaMine", "oracle_answer_rank": 1}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "question": "How does model compare to the baselines?", "references": ["The model outperforms the two baseline models,  since it has higher recall values. ", "Answer with content missing: (Table1 part of Figure 3):\nProposed vs Best baseline result\n- Median Rank: 2.9 vs 3.0 (lower better)\n- Rank 1 recall: 34.6 vs 33.1 (higher better)", "The model improved over the baseline with scores of 34.6, 66.0 and 76.6 for Recall at 1, 5 and 10 respectively"], "best_model_answer": "The model outperforms the two baseline models,  since it has higher recall values. ", "oracle_answer": "The model improved over the baseline with scores of 34.6, 66.0 and 76.6 for Recall at 1, 5 and 10 respectively", "oracle_answer_rank": 3}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "question": "Are the two paragraphs encoded independently?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "question": "What is their baseline?", "references": ["synthetic emails generated by Dada engine", "Dada engine BIBREF6", "Dada engine"], "best_model_answer": "synthetic emails generated by Dada engine", "oracle_answer": "Dada engine", "oracle_answer_rank": 3}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "question": "Is human evaluation of the malicious content performed?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "question": "Do they compare to previous work?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "question": "by how much did their model outperform the other models?", "references": ["In terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.", "Unanswerable", "Their model outperforms other models by 0.01 micro F1 and 0.07 macro F1"], "best_model_answer": "In terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.", "oracle_answer": "In terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.", "oracle_answer_rank": 1}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "question": "What is reordering in the context of the paper?", "references": ["changing the order of the word-by-word translation so it matches the target language", "Changing the word order of the translation so it is in the right order of the target language.", "Re-arranging translated words so that they are in the correct order in the target language"], "best_model_answer": "changing the order of the word-by-word translation so it matches the target language", "oracle_answer": "Changing the word order of the translation so it is in the right order of the target language.", "oracle_answer_rank": 2}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "question": "How does the paper use language model for context aware search?", "references": ["the language model is combined with cross-lingual word embedding to obtain context information in the word-by-word translation", "combining a language model (LM) with cross-lingual word embedding, Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:\n\n$ L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h) $\n\nHere, $q(f,e)$ is a lexical score defined as:\n\n$ q(f,e) = \\frac{d(f,e) + 1}{2} $\n\nwhere $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ ., Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses.", "It is used to calculate the probability of a possible target word given the history of target words that come before it."], "best_model_answer": "the language model is combined with cross-lingual word embedding to obtain context information in the word-by-word translation", "oracle_answer": "combining a language model (LM) with cross-lingual word embedding, Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:\n\n$ L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h) $\n\nHere, $q(f,e)$ is a lexical score defined as:\n\n$ q(f,e) = \\frac{d(f,e) + 1}{2} $\n\nwhere $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ ., Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses.", "oracle_answer_rank": 2}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "question": "What datasets are used?", "references": ["PDTB 2.0", "PDTB 2.0 ", "PDTB 2.0 "], "best_model_answer": "PDTB 2.0", "oracle_answer": "PDTB 2.0", "oracle_answer_rank": 1}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "question": "what evaluation metrics were used?", "references": ["F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4", "ROUGE", "ROUGE-1, ROUGE-2 , ROUGE-SU4"], "best_model_answer": "F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4", "oracle_answer": "F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4", "oracle_answer_rank": 1}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "question": "what is the source of their dataset?", "references": ["Google News, follow the related tags on Twitter, scan the list of event archives on the Web, such as earthquakes happened in 2017", "Topics were taken from category names in Google News, tags on Twitter, event archives on the Web. News articles were taken from news websites.", " Google News, Twitter"], "best_model_answer": "Google News, follow the related tags on Twitter, scan the list of event archives on the Web, such as earthquakes happened in 2017", "oracle_answer": "Google News, follow the related tags on Twitter, scan the list of event archives on the Web, such as earthquakes happened in 2017", "oracle_answer_rank": 1}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "question": "by how much did the performance improve?", "references": ["The proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.", "Unanswerable", "They improved by 0.007 on average across R-1, R-2, R-SU4 over the best baseline."], "best_model_answer": "The proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.", "oracle_answer": "The proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.", "oracle_answer_rank": 1}
{"question_id": "86d1c990c1639490c239c3dbf5492ecc44ab6652", "question": "how many experts were there?", "references": ["Each topic is assigned to 4 experts", "5", "5"], "best_model_answer": "Each topic is assigned to 4 experts", "oracle_answer": "5", "oracle_answer_rank": 2}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "question": "what is the size of the data collected?", "references": [" The dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words", "19000", "45 topics from those 6 predefined categories, On average, each topic contains 215 pieces of comments and 940 comment sentences., 19k annotated aspect facets"], "best_model_answer": " The dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words", "oracle_answer": " The dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words", "oracle_answer_rank": 1}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "question": "did they use a crowdsourcing platform?", "references": ["No", "No", "Unanswerable"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "question": "how was annotation conducted?", "references": ["Experts identified aspect facets and wrote summaries.", "employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing", "Each topic is assigned to 4 experts to conduct the summary writing in two phases: facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets."], "best_model_answer": "Experts identified aspect facets and wrote summaries.", "oracle_answer": "employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing", "oracle_answer_rank": 2}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "question": "Do they report results only on English data?", "references": ["Unanswerable", "Yes", "Yes"], "best_model_answer": "Unanswerable", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "question": "Which 8 tasks has LGI learned?", "references": ["move left, move right, this is \u2026, the size is big/small, give me a \u2026, enlarge/shrink, rotate \u2026", "move left, move right, this is \u2026, the size is big/small, the size is not small/big, give me a \u2026, enlarge/shrink, rotate \u2026", "move left, move right, this is \u2026, the size is big/small\u2019, the size is not small/big, give me a \u2026, enlarge/shrink, rotate \u2026\u2019"], "best_model_answer": "move left, move right, this is \u2026, the size is big/small, give me a \u2026, enlarge/shrink, rotate \u2026", "oracle_answer": "move left, move right, this is \u2026, the size is big/small, the size is not small/big, give me a \u2026, enlarge/shrink, rotate \u2026", "oracle_answer_rank": 2}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "question": "In what was does an LSTM mimic the prefrontal cortex?", "references": ["the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "Unanswerable", "It combines language and vision streams similar to the human prefrontal cortex."], "best_model_answer": "the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "oracle_answer": "the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "oracle_answer_rank": 1}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "question": "In what way does an LSTM mimic the intra parietal sulcus?", "references": [" mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "textizer to produce text symbols output, extract the quantity information from language text ", "It mimics the number processing functionality of human Intra-Parietal Sulcus."], "best_model_answer": " mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "oracle_answer": " mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "oracle_answer_rank": 1}
{"question_id": "bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "question": "How do the authors define imagination, or imagined scenarios?", "references": ["Ability to change the answering contents by considering the consequence of the next few output sentences.", " transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image", "Continual thinking requires the capacity to generate mental imagination guided by language, and extract language representations from a real or imagined scenario"], "best_model_answer": "Ability to change the answering contents by considering the consequence of the next few output sentences.", "oracle_answer": " transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image", "oracle_answer_rank": 2}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "question": "Which classifiers did they experiment with?", "references": ["logistic regression classifier", "Long Short Term Memory (LSTM) language model, logistic regression model", "logistic regression classifier, trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier"], "best_model_answer": "logistic regression classifier", "oracle_answer": "logistic regression classifier", "oracle_answer_rank": 1}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "question": "How did they identify what language the text was?", "references": ["used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages", " We used NanigoNet, a language detector based on GCNNs", "NanigoNet"], "best_model_answer": "used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages", "oracle_answer": " We used NanigoNet, a language detector based on GCNNs", "oracle_answer_rank": 2}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "question": "Which repositories did they collect from?", "references": ["Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.", "GitHub repositories", "Has at least one pull request or pull request review comment event between November 2017 and September 2019,, 50 or more starts, size between 1MB and 1GB, permissive license"], "best_model_answer": "Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.", "oracle_answer": "Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.", "oracle_answer_rank": 1}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "question": "Which three features do they use?", "references": ["mechanical, spell, and grammatical edits", "Ratio of the target perplexity over the source calculated by a language model, Normalized edit distance between the source and the target, Binary variable indicating whether the edit purely consists of changes in numbers", "Ratio of the target perplexity over the source calculated by a language model, Normalized edit distance between the source and the target, Binary variable indicating whether the edit purely consists of changes in numbers"], "best_model_answer": "mechanical, spell, and grammatical edits", "oracle_answer": "Ratio of the target perplexity over the source calculated by a language model, Normalized edit distance between the source and the target, Binary variable indicating whether the edit purely consists of changes in numbers", "oracle_answer_rank": 2}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "question": "Which languages are covered in the corpus?", "references": ["the top 10 languages are English, simplified Chinese, Japanese, Russian, French, German, Portuguese, Spanish, Korean and Hindi", "English, Chinese, Japanese, Russian, French, German, Portugese, Spanish, Korean, Hindi and Others", "English,  Chinese (smpl.),  Japanese,  Russian,  French,  German,  Portuguese,  Spanish,  Korean , Hindi"], "best_model_answer": "the top 10 languages are English, simplified Chinese, Japanese, Russian, French, German, Portuguese, Spanish, Korean and Hindi", "oracle_answer": "English,  Chinese (smpl.),  Japanese,  Russian,  French,  German,  Portuguese,  Spanish,  Korean , Hindi", "oracle_answer_rank": 3}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "question": "Do they report results only on English data?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "question": "What is the BM25 baseline?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "question": "Which BERT layers were combined to boost performance?", "references": ["Top and bottom layers", " the top and bottom layer of the BERT fine-tuned on SNLI dataset", "combining the top and bottom layer embeddings"], "best_model_answer": "Top and bottom layers", "oracle_answer": "combining the top and bottom layer embeddings", "oracle_answer_rank": 3}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "question": "Which NLI data was used to improve the quality of the embeddings?", "references": ["MNLI BIBREF11, SNLI", "MNLI, SNLI", "Two natural language inference datasets, MNLI BIBREF11 and SNLI"], "best_model_answer": "MNLI BIBREF11, SNLI", "oracle_answer": "MNLI BIBREF11, SNLI", "oracle_answer_rank": 1}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "question": "Which four QA datasets are examined?", "references": ["(1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, (4) SearchQA BIBREF16", "WikiPassageQA, InsuranceQA, Quasar-t, SearchQA", "WikiPassageQA, InsuranceQA , Quasar-t , SearchQA"], "best_model_answer": "(1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, (4) SearchQA BIBREF16", "oracle_answer": "WikiPassageQA, InsuranceQA, Quasar-t, SearchQA", "oracle_answer_rank": 2}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "question": "what classifiers did they train?", "references": ["a multinomial logistic regression model with ridge estimator, a multilayer perceptron, a support vector machine learner, Sequential Minimal Optimization (SMO), a decision tree (J48)", "(1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), (4) a decision tree (J48)", "multinomial logistic regression model with ridge estimator, multilayer perceptron, support vector machine learner, Sequential Minimal Optimization, decision tree"], "best_model_answer": "a multinomial logistic regression model with ridge estimator, a multilayer perceptron, a support vector machine learner, Sequential Minimal Optimization (SMO), a decision tree (J48)", "oracle_answer": "a multinomial logistic regression model with ridge estimator, a multilayer perceptron, a support vector machine learner, Sequential Minimal Optimization (SMO), a decision tree (J48)", "oracle_answer_rank": 1}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "question": "what dataset did they use?", "references": ["subset of COCTAILL", "a subset of COCTAILL", "a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1)"], "best_model_answer": "subset of COCTAILL", "oracle_answer": "subset of COCTAILL", "oracle_answer_rank": 1}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "question": "what combination of features helped improve the classification?", "references": ["Using all the 61 features helped them improve the classification", "a combination of all features for the document level", "length-based, lexical, morphological, syntactic and semantic features"], "best_model_answer": "Using all the 61 features helped them improve the classification", "oracle_answer": "a combination of all features for the document level", "oracle_answer_rank": 2}
{"question_id": "a70656fc61bf526dd21db7d2ec697b29a5a9c24e", "question": "what linguistics features did they apply?", "references": ["length-based, lexical, morphological, syntactic, semantic", "Sentence length\nModal verbs to verbs\nAverage token length\nParticle IncSc\nExtra-long words\nSG pronoun IncSc\nNumber of characters\nPunctuation IncSc\nLIX\nSubjunction IncSc\nS-verb IncSc\nA1 lemma IncSc\nS-verbs to verbs\nA2 lemma IncSc\nAdjective IncSc\nB1 lemma IncSc\nAdjective variation\nB2 lemma IncSc\nAdverb IncSc\nC1 lemma IncSc\nAdverb variation\nC2 lemma IncSc\nNoun IncSc\nDifficult word IncSc\nNoun variation\nDifficult noun and verb IncSc\nVerb IncSc\nOut-of-Kelly IncSc\nVerb variation\nMissing lemma form IncSc\nNominal ratio\nAvg. Kelly log frequency\nNouns to verbs\nFunction word IncSc\nAverage dependency length\nLexical words to non-lexical words\nDependency arcs longer than\nLexical words to all tokens\nLongest dependency from root node\nNeuter gender noun IncSc\nRatio of right dependency arcs\nCon- and subjunction IncSc\nRatio of left dependency arcs\nPast participles to verbs\nModifier variation\nPresent participles to verbs\nPre-modifier IncSc\nPast verbs to verbs\nPost-modifier IncSc\nPresent verbs to verbs\nSubordinate IncSc\nSupine verbs to verbs\nRelative clause IncSc\nRelative structure IncSc\nPrepositional complement IncSc\nBilog type-token ratio\nSquare root type-token ratio\nAvg. nr. of senses per token\nPronouns to nouns\nNoun senses per noun\nPronouns to prepositions", "lexical, morphological, syntactic and semantic features"], "best_model_answer": "length-based, lexical, morphological, syntactic, semantic", "oracle_answer": "lexical, morphological, syntactic and semantic features", "oracle_answer_rank": 3}
{"question_id": "c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "question": "Are results reported only on English data?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "e0b54906184a4ad87d127bed22194e62de38222b", "question": "What type of model were the features used in?", "references": ["character-based Bidirectional Gated Recurrent neural network", "Random Selection, Majority Class, bag-of-words, Tweet2vec BIBREF32", "Logistic Regression classifier"], "best_model_answer": "character-based Bidirectional Gated Recurrent neural network", "oracle_answer": "character-based Bidirectional Gated Recurrent neural network", "oracle_answer_rank": 1}
{"question_id": "1f8044487af39244d723582b8a68f94750eed2cc", "question": "What unsupervised approach was used to deduce the thematic information?", "references": [" Latent Dirichlet Allocation (LDA)", "Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8", "Latent Dirichlet Allocation (LDA) topic modeling"], "best_model_answer": " Latent Dirichlet Allocation (LDA)", "oracle_answer": "Latent Dirichlet Allocation (LDA) topic modeling", "oracle_answer_rank": 3}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "question": "What profile features are used?", "references": ["Part-of-speech tags (POS), syntactic dependency relations, count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, tweet length", "bag of stopwords, Part-of-speech tags, syntactic dependency relations, count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, tweet length", "bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL), count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio and the tweet length"], "best_model_answer": "Part-of-speech tags (POS), syntactic dependency relations, count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, tweet length", "oracle_answer": "bag of stopwords, Part-of-speech tags, syntactic dependency relations, count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, tweet length", "oracle_answer_rank": 2}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "question": "what other representations do they compare with?", "references": ["word2vec averaging, Paragraph Vector", "Paragraph Vector, word2vec averagings", "Word2vec averaging (public release 300d), word2vec averaging (academic corpus), Paragraph Vector"], "best_model_answer": "word2vec averaging, Paragraph Vector", "oracle_answer": "word2vec averaging, Paragraph Vector", "oracle_answer_rank": 1}
{"question_id": "5d4190403eb800bb17eec71e979788e11cf74e67", "question": "what empirical evaluations performed?", "references": ["document retrieval, document clustering", "document retrieval, document clustering", " we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering."], "best_model_answer": "document retrieval, document clustering", "oracle_answer": "document retrieval, document clustering", "oracle_answer_rank": 1}
{"question_id": "56d41e0fcc288c1e65806ae77097d685c83e22db", "question": "which document understanding tasks did they evaluate on?", "references": ["document retrieval, document clustering", " document retrieval and document clustering", " document retrieval, document clustering"], "best_model_answer": "document retrieval, document clustering", "oracle_answer": "document retrieval, document clustering", "oracle_answer_rank": 1}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "question": "what dataset was used?", "references": ["669 academic papers published by IEEE, 850 academic papers", "669 academic papers published by IEEE", "For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation."], "best_model_answer": "669 academic papers published by IEEE, 850 academic papers", "oracle_answer": "669 academic papers published by IEEE, 850 academic papers", "oracle_answer_rank": 1}
{"question_id": "31cba86bc45970337ba035ecf36d8954a9a5206a", "question": "What private companies are members of consortium?", "references": ["Creditinfo, Grammatek, Mideind and Tiro", "The \u00c1rni Magn\u00fasson Instit. for Icelandic Studies, Reykjavik University (RU),  University of Iceland (UI),  R\u00daV, Creditinfo, The Association of the Visually Impaired, Grammatek, Mi\u00f0eind. Tiro", "Crediyinfo, Grammatek, \nMideind,\nTiro"], "best_model_answer": "Creditinfo, Grammatek, Mideind and Tiro", "oracle_answer": "Creditinfo, Grammatek, Mideind and Tiro", "oracle_answer_rank": 1}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "question": "Does programme plans gathering and open sourcing some large dataset for Icelandic language?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "b4b7333805cb6fdde44907747887a971422dc298", "question": "What other national language technology programs are described in the paper?", "references": ["STEVIN programme in the Netherlands, Plan for the Advancement of Language Technology in Spain, Estonian LT programmes", "STEVIN programme in the Netherlands,  Plan for the Advancement of Language Technology in Spain, Estonian LT programmes", "Netherlands, Spain, Estonian"], "best_model_answer": "STEVIN programme in the Netherlands, Plan for the Advancement of Language Technology in Spain, Estonian LT programmes", "oracle_answer": "STEVIN programme in the Netherlands, Plan for the Advancement of Language Technology in Spain, Estonian LT programmes", "oracle_answer_rank": 1}
{"question_id": "871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "question": "When did language technology start in Iceland?", "references": ["Around year 2000", "in the year 2000", "in the year 2000, couple of LT resources and products were developed in the years leading up to that"], "best_model_answer": "Around year 2000", "oracle_answer": "in the year 2000", "oracle_answer_rank": 2}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "question": "what domain do the opinions fall under?", "references": ["computers, wireless routers, speakers", "computers, wireless routers, speakers ", "computers, wireless routers, and speakers"], "best_model_answer": "computers, wireless routers, speakers", "oracle_answer": "computers, wireless routers, speakers", "oracle_answer_rank": 1}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "question": "what was the baseline?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "question": "what dataset was used?", "references": ["SNAP Amazon Dataset , Bing Liu's dataset", "Bing Liu's dataset", "SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20"], "best_model_answer": "SNAP Amazon Dataset , Bing Liu's dataset", "oracle_answer": "SNAP Amazon Dataset , Bing Liu's dataset", "oracle_answer_rank": 1}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "question": "is this the first dataset with a grading scaling rather than binary?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "question": "what are the existing datasets for this task?", "references": ["480 concepts previously analyzed in BIBREF1, BIBREF4", "Dataset I created and analyzed in BIBREF1, BIBREF4", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives, Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017)., Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. "], "best_model_answer": "480 concepts previously analyzed in BIBREF1, BIBREF4", "oracle_answer": "480 concepts previously analyzed in BIBREF1, BIBREF4", "oracle_answer_rank": 1}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "question": "what is the size of the introduced dataset?", "references": ["608 controversial Wikipedia concepts, 3561 concepts", "About 1216 in dataset II, 3561 in dataset III.", "Dataset I  -  480 concepts, 240  controversial examples, and  240 not-controversial examples.\nDataset II -  608 controversial concepts\nDataset III -  3561 controversial concepts"], "best_model_answer": "608 controversial Wikipedia concepts, 3561 concepts", "oracle_answer": "Dataset I  -  480 concepts, 240  controversial examples, and  240 not-controversial examples.\nDataset II -  608 controversial concepts\nDataset III -  3561 controversial concepts", "oracle_answer_rank": 3}
{"question_id": "376e8ed6e039e07c892c77b7525778178d56acb7", "question": "did they crowdsource annotations?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "4de6bcddd46726bf58326304b0490fdb9e7e86ec", "question": "how was labeling done?", "references": ["The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10", "10 or more annotators marked whether a topic was controversial or not. The score was then normalized on an integer scale of 0-10.", "As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia., For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random, The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10."], "best_model_answer": "The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10", "oracle_answer": "The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10", "oracle_answer_rank": 1}
{"question_id": "e831ce6c406bf5d1c493162732e1b320abb71b6f", "question": "where does their dataset come from?", "references": ["Wikipedia list of controversial issues, concepts whose Wikipedia pages are under edit protection", "Wikipedia ", "The topics from Wikipedia list of controversial issues that appear more than 50 times in Wikipedia, topics with their Wikipedia pages under edit protection."], "best_model_answer": "Wikipedia list of controversial issues, concepts whose Wikipedia pages are under edit protection", "oracle_answer": "Wikipedia list of controversial issues, concepts whose Wikipedia pages are under edit protection", "oracle_answer_rank": 1}
{"question_id": "267d70d9f3339c56831ea150d2213643fbc5129b", "question": "What is the performance of NJM?", "references": ["NJM vas selected as the funniest caption among the three options 22.59% of the times, and NJM captions posted to Bokete averaged 3.23 stars", "It obtained a score of 22.59%", "Captions generated by NJM were ranked \"funniest\" 22.59% of the time."], "best_model_answer": "NJM vas selected as the funniest caption among the three options 22.59% of the times, and NJM captions posted to Bokete averaged 3.23 stars", "oracle_answer": "Captions generated by NJM were ranked \"funniest\" 22.59% of the time.", "oracle_answer_rank": 3}
{"question_id": "477da8d997ff87400c6aad19dcc74f8998bc89c3", "question": "How are the results evaluated?", "references": ["The captions are ranked by humans in order of \"funniness\".", "a questionnaire", "With a questionnaire asking subjects to rank methods according to its \"funniness\". Also, by posting the captions to Bokete to evaluate them by received stars"], "best_model_answer": "The captions are ranked by humans in order of \"funniness\".", "oracle_answer": "With a questionnaire asking subjects to rank methods according to its \"funniness\". Also, by posting the captions to Bokete to evaluate them by received stars", "oracle_answer_rank": 3}
{"question_id": "4485e32052741972877375667901f61e602ec4de", "question": "How big is the self-collected corpus?", "references": ["999,571 funny captions for 70,981 images", " 999,571 funny captions for 70,981 images", "999571 captions for 70981 images."], "best_model_answer": "999,571 funny captions for 70,981 images", "oracle_answer": "999,571 funny captions for 70,981 images", "oracle_answer_rank": 1}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "question": "Which dataset do they use?", "references": ["Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", " survey data and hand crafted a total of 293 textual questions BIBREF13", "U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12"], "best_model_answer": "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", "oracle_answer": "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", "oracle_answer_rank": 1}
{"question_id": "54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "question": "Do they compare their proposed domain adaptation methods to some existing methods?", "references": ["No", "No", "Yes"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "question": "With how many languages do they experiment in the multilingual setup?", "references": ["ten languages", "16", "6"], "best_model_answer": "ten languages", "oracle_answer": "ten languages", "oracle_answer_rank": 1}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "question": "Which dataset do they use?", "references": ["GlobalPhone corpus", "GlobalPhone\nCroatian\nHausa\nMandarin\nSpanish\nSwedish\nTurkish\nZRSC\nBuckeye\nXitsonga", "GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus"], "best_model_answer": "GlobalPhone corpus", "oracle_answer": "GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus", "oracle_answer_rank": 3}
{"question_id": "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86", "question": "Which intrisic measures do they use do evaluate obtained representations?", "references": ["same-different, ABX evaluation measures", "same-different, ABX ", "Precision and recall at a given threshold"], "best_model_answer": "same-different, ABX evaluation measures", "oracle_answer": "same-different, ABX evaluation measures", "oracle_answer_rank": 1}
{"question_id": "19578949108ef72603afe538059ee55b4dee0751", "question": "Do they use pretrained embeddings in their model?", "references": ["Unanswerable", "No", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "86656aae3c27c6ea108f5600dd09ab7e421d876a", "question": "What sources do the news come from?", "references": ["1500 news sites", "From a diverse set of news sources on site newser.com", "newser.com"], "best_model_answer": "1500 news sites", "oracle_answer": "From a diverse set of news sources on site newser.com", "oracle_answer_rank": 2}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "question": "What is the size of Multi-news dataset?", "references": ["56216", "56,216", "56216 "], "best_model_answer": "56216", "oracle_answer": "56216", "oracle_answer_rank": 1}
{"question_id": "23fe8431058f2a7b7588745766fc715f271aad07", "question": "Which languages are explored?", "references": ["German (De) and English (En)", "German, English", "German (De) and English (En) languages"], "best_model_answer": "German (De) and English (En)", "oracle_answer": "German (De) and English (En)", "oracle_answer_rank": 1}
{"question_id": "e5b2eb6a49c163872054333f8670dd3f9563046a", "question": "What datasets are used in the paper?", "references": ["Europarl v9 parallel data set, NewsTest2013, NewsTest2014", "Europarl v9 parallel data set, NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.", "Europarl v9, NewsTest2013, NewsTest2014"], "best_model_answer": "Europarl v9 parallel data set, NewsTest2013, NewsTest2014", "oracle_answer": "Europarl v9 parallel data set, NewsTest2013, NewsTest2014", "oracle_answer_rank": 1}
{"question_id": "73760a45b23b2ec0cab181f82953fb296bb6cd19", "question": "What vocabulary sizes are explored?", "references": ["Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "Word 64k, Word 32k, BPE 64k,  BPE 32k,   BPE 16k,   BPE 8k,   BPE 4k,   BPE 2k,   BPE 1k.", "Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176."], "best_model_answer": "Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "oracle_answer": "Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "oracle_answer_rank": 1}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "question": "What datasets do they look at?", "references": ["Europarl v9, NewsTest2013 , NewsTest2014", "Europarl v9, NewsTest2013, NewsTest2014", "Europarl v9 parallel data set, NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track"], "best_model_answer": "Europarl v9, NewsTest2013 , NewsTest2014", "oracle_answer": "Europarl v9, NewsTest2013 , NewsTest2014", "oracle_answer_rank": 1}
{"question_id": "9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "question": "Which vocab sizes did they analyze?", "references": ["Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176.", "Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "Word 64k, Word 32k, BPE 64k,  BPE 32k,   BPE 16k,   BPE 8k,   BPE 4k,   BPE 2k,   BPE 1k."], "best_model_answer": "Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176.", "oracle_answer": "Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "oracle_answer_rank": 2}
{"question_id": "2929e92f9b4939297b4d0f799d464d46e8d52063", "question": "Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?", "references": ["suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities", "The WNUT 2017 dataset had entities already seen in the training set filtered out while the OntoNotes dataset did not. Cross-context patterns thus provided more significant information for NER in WNUT 2017 because the possibility of memorizing entity forms was removed.", "Ontonotes is less noisy than Wnut 2017"], "best_model_answer": "suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities", "oracle_answer": "The WNUT 2017 dataset had entities already seen in the training set filtered out while the OntoNotes dataset did not. Cross-context patterns thus provided more significant information for NER in WNUT 2017 because the possibility of memorizing entity forms was removed.", "oracle_answer_rank": 2}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "question": "What are the baseline models?", "references": ["BiLSTM-CNN", "BiLSTM-CNN proposed by BIBREF1", "Baseline-BiLSTM-CNN"], "best_model_answer": "BiLSTM-CNN", "oracle_answer": "BiLSTM-CNN", "oracle_answer_rank": 1}
{"question_id": "b537832bba2eb6d34702a9d71138e661c05a7c3a", "question": "What text classification tasks are considered?", "references": ["news classification, sentiment analysis, Wikipedia article classification, questions and answers categorization ", " AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes)", "news classification, sentiment analysis, Wikipedia article classification"], "best_model_answer": "news classification, sentiment analysis, Wikipedia article classification, questions and answers categorization ", "oracle_answer": "news classification, sentiment analysis, Wikipedia article classification, questions and answers categorization ", "oracle_answer_rank": 1}
{"question_id": "1002bd01372eba0f3078fb4a951505278ed45f2e", "question": "Do they compare against other models?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "question": "Are LSA-reduced n-gram features considered hand-crafted features?", "references": ["Yes", "No", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "28e50459da60ceda49fe1578c12f3f805b288bd0", "question": "What is the performance of the model on English, Spanish and Arabic?", "references": ["on PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic", "Accuracy: English 82.31, Spanish 80.22, Arabic 80.50", "In terms of accuracy, 81.789% for English, 78.227% for Spanish and 78.5% for Arabic"], "best_model_answer": "on PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic", "oracle_answer": "on PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic", "oracle_answer_rank": 1}
{"question_id": "c0cebef0e29b9d13c165b6f19f6ca8393348c671", "question": "What languages is the model tested on?", "references": ["Unanswerable", "Unanswerable", "English"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "5695908a8c6beb0e3863a1458a1b93aab508fd34", "question": "What is a personalized language model?", "references": ["A model that contains the expected user-specific entities.", "language model which contains user-specific entities", " contains the expected user-specific entities"], "best_model_answer": "A model that contains the expected user-specific entities.", "oracle_answer": "A model that contains the expected user-specific entities.", "oracle_answer_rank": 1}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "question": "Is the dataset used in other work?", "references": ["Yes, in Waseem and Hovy (2016)", "Yes", "Yes"], "best_model_answer": "Yes, in Waseem and Hovy (2016)", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "11679d1feba747c64bbbc62939a20fbb69ada0f3", "question": "What community-based profiling features are used?", "references": ["The features are the outputs from node2vec when run on a community graph where nodes are users and edges are connections if one user follows the other on Twitter.", "The features are the output of running node2vec on a community graph where the nodes are users, and they are connected if one of them follows the other on Twitter.", "The features are the output of running node2vec on a community graph where the nodes are users, and they are connected if one of them follows the other on Twitter."], "best_model_answer": "The features are the outputs from node2vec when run on a community graph where nodes are users and edges are connections if one user follows the other on Twitter.", "oracle_answer": "The features are the output of running node2vec on a community graph where the nodes are users, and they are connected if one of them follows the other on Twitter.", "oracle_answer_rank": 2}
{"question_id": "e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "question": "what user traits are taken into account?", "references": ["The hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.", "personal values", "Family, Nature, Work-Ethic, Religion"], "best_model_answer": "The hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.", "oracle_answer": "The hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.", "oracle_answer_rank": 1}
{"question_id": "7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "question": "does incorporating user traits help the task?", "references": ["No", "No", "only in the 806-class task predicting <= 25 clusters"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "cab082973e1648b0f0cc651ab4e0298a5ca012b5", "question": "how many activities are in the dataset?", "references": ["29,494", "29537", "30,000"], "best_model_answer": "29,494", "oracle_answer": "29,494", "oracle_answer_rank": 1}
{"question_id": "cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "question": "what social media platform was the data collected from?", "references": ["Twitter", "Twitter ", " Twitter"], "best_model_answer": "Twitter", "oracle_answer": "Twitter", "oracle_answer_rank": 1}
{"question_id": "101d7a355e8bf6d1860917876ee0b9971eae7a2f", "question": "Do they report results only for English data?", "references": ["Unanswerable", "Yes", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "4288621e960ffbfce59ef1c740d30baac1588b9b", "question": "What conclusions do the authors draw from their experiments?", "references": ["among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models, Tweet length affects the task prediction accuracies, LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive", "CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models, Tweet length affects the task prediction accuracies,, CNN, LSTM and BLSTM are extremely sensitive to word order", "Supervised models CNN, LSTM and BLSTM and unsupervised models BOW, DSSM, STV and T2V can  encapsulate most of the syntactic and social properties. Tweet length affects the task prediction accuracies for all models. LDA is insensitive to input word order, but, CNN, LSTM\nand BLSTM are not."], "best_model_answer": "among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models, Tweet length affects the task prediction accuracies, LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive", "oracle_answer": "among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models, Tweet length affects the task prediction accuracies, LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive", "oracle_answer_rank": 1}
{"question_id": "dce27c49b9bf1919ca545e04663507d83bb42dbe", "question": "What methods are used to correct the brevity problem?", "references": [" tuned word reward ", "Length normalization; Google\u2019s NMT correction; constant word reward", "Length normalization, Google's NMT, constant word reward"], "best_model_answer": " tuned word reward ", "oracle_answer": "Length normalization, Google's NMT, constant word reward", "oracle_answer_rank": 3}
{"question_id": "991ea04072b3412928be5e6e903cfa54eeac3951", "question": "Why does wider beam search hurt NMT?", "references": ["Using a wider beam increases the probability of a shorter translation to remain in the top k variants and eventually score higher than any longer and more accurate translation variant", "brevity problem", "if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem."], "best_model_answer": "Using a wider beam increases the probability of a shorter translation to remain in the top k variants and eventually score higher than any longer and more accurate translation variant", "oracle_answer": "if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem.", "oracle_answer_rank": 3}
{"question_id": "a82a12a22a45d9507bc359635ffe9574f15e0810", "question": "What linguistic model does the conventional method use?", "references": ["Random Forest to perform humor recognition by using the following two groups of features:  latent semantic structural features and semantic distance features.", "Random Forest BIBREF12", "Random Forest classifier using latent semantic structural features, semantic distance features and sentences' averaged Word2Vec representations"], "best_model_answer": "Random Forest to perform humor recognition by using the following two groups of features:  latent semantic structural features and semantic distance features.", "oracle_answer": "Random Forest classifier using latent semantic structural features, semantic distance features and sentences' averaged Word2Vec representations", "oracle_answer_rank": 3}
{"question_id": "2f9a31f5a2b668acf3bce8958f5daa67ab8b2c83", "question": "Do they evaluate only on English data?", "references": ["Unanswerable", "Unanswerable", "Yes"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "4830459e3d1d204e431025ce7e596ef3f8d757d2", "question": "How many speakers are included in the dataset?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "question": "How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?", "references": ["Laughter from the audience.", "by laughter", "By laughter from the audience"], "best_model_answer": "Laughter from the audience.", "oracle_answer": "By laughter from the audience", "oracle_answer_rank": 3}
{"question_id": "3a01dc85ac983002fd631f1c28fc1cbe16094c24", "question": "How you incorporate commonsense into an LSTM?", "references": ["by employing an external memory module containing commonsense knowledge", "using another LSTM for encoding all assertions $a$ in $A_x$ , as illustrated in Figure 3 . Each $a$ , originally in the form of $$ , is transformed into a sequence of tokens by chunking $c_1$ , $c_2$ , concepts which are potentially multi-word phrases, into $[c_{11},c_{12},c_{13}...]$ and $[c_{21},c_{22},c_{23}...]$ . Thus, $a=[c_{11},c_{12},c_{13}...,r,c_{21},c_{22},c_{23}...]$ .", "using another LSTM for encoding commonsense assertions"], "best_model_answer": "by employing an external memory module containing commonsense knowledge", "oracle_answer": "using another LSTM for encoding commonsense assertions", "oracle_answer_rank": 3}
{"question_id": "042800c3336ed5f4826203616a39747c61382ba6", "question": "Which commonsense knowledge base are they using?", "references": ["ConceptNet", "ConceptNet", "ConceptNet"], "best_model_answer": "ConceptNet", "oracle_answer": "ConceptNet", "oracle_answer_rank": 1}
{"question_id": "52868394eb2b3b37eb5f47f51c06ad53061f4495", "question": "How did they obtain the dataset?", "references": ["The authors crawled all areas listed an TripAdvisor's SiteIndex and gathered all links related to hotels. Using Selenium, they put a time gap between opening each page, to mimic human behaviour and avoid having their scraper being detected. They discarded pages without a review and for pages with a review, they collected the review's profile, the overall rating, the summary, the written text and subratings, where given. ", "hotel reviews from TripAdvisor", "TripAdvisor hotel reviews"], "best_model_answer": "The authors crawled all areas listed an TripAdvisor's SiteIndex and gathered all links related to hotels. Using Selenium, they put a time gap between opening each page, to mimic human behaviour and avoid having their scraper being detected. They discarded pages without a review and for pages with a review, they collected the review's profile, the overall rating, the summary, the written text and subratings, where given. ", "oracle_answer": "TripAdvisor hotel reviews", "oracle_answer_rank": 3}
{"question_id": "713e1c7b0ab17759ba85d7cd2041e387831661df", "question": "Did they experiment on this dataset?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "00db191facf903cef18fb1727d1cab638c277e0a", "question": "What sized character n-grams do they use?", "references": ["cahr3-MS-vec, char4-MS-vec, char2-MS-vec", "2, 3 and 4", "char3"], "best_model_answer": "cahr3-MS-vec, char4-MS-vec, char2-MS-vec", "oracle_answer": "cahr3-MS-vec, char4-MS-vec, char2-MS-vec", "oracle_answer_rank": 1}
{"question_id": "1edfe390828f02a2db9a88454421c7f3d4cdd611", "question": "Do they experiment with fine-tuning their embeddings?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "question": "Which dataset do they evaluate on for headline generation?", "references": ["English Gigaword corpus", "English Gigaword corpus BIBREF35", " the annotated English Gigaword corpus"], "best_model_answer": "English Gigaword corpus", "oracle_answer": "English Gigaword corpus", "oracle_answer_rank": 1}
{"question_id": "5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "question": "What results do their embeddings obtain on machine translation?", "references": ["BLEU score of 35.48 on En-Fr, 23.27 on En-De, 34.43 on Fr-En, 28.86 on De-En", "BLEU scores are: En-Fr(35.84), En-De(23.27), Fr-En(34.43) and De-En(28.86).", "Bleu on IWSLT16: En-FR 35.48, En-De 23.27, Fr-En 34.43, De-En 28.86"], "best_model_answer": "BLEU score of 35.48 on En-Fr, 23.27 on En-De, 34.43 on Fr-En, 28.86 on De-En", "oracle_answer": "Bleu on IWSLT16: En-FR 35.48, En-De 23.27, Fr-En 34.43, De-En 28.86", "oracle_answer_rank": 3}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "question": "Which dataset do they use?", "references": ["A crowdsourced twitter dataset containing 19358 tweets", "BIBREF4", "19538 tweets  from BIBREF4"], "best_model_answer": "A crowdsourced twitter dataset containing 19358 tweets", "oracle_answer": "19538 tweets  from BIBREF4", "oracle_answer_rank": 3}
{"question_id": "dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "question": "By how much do they outperform previous state-of-the-art approaches?", "references": ["BiLSTM for 0.02 F1,  Feature Engineering SotA  for 0.08 F1, and Concatenated NN Architecture for  0.24 F1.", "Proposed model had 0.63 F1 score and 83.49% accuracy compared to the 0.61 F1 and 83.28% accuracy of best compared method.", "By more than 0.02 with F1 score and 0.21% with accuracy"], "best_model_answer": "BiLSTM for 0.02 F1,  Feature Engineering SotA  for 0.08 F1, and Concatenated NN Architecture for  0.24 F1.", "oracle_answer": "By more than 0.02 with F1 score and 0.21% with accuracy", "oracle_answer_rank": 3}
{"question_id": "76e17e648a4d1f386eb6bf61b0c24f134af872be", "question": "What other scenarios can the bias mitigation methods be applied to?", "references": ["sentiment analysis , other identity problems like racial", "other identity problems like racial, sentiment analysis", "developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time"], "best_model_answer": "sentiment analysis , other identity problems like racial", "oracle_answer": "sentiment analysis , other identity problems like racial", "oracle_answer_rank": 1}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "question": "What model architectures are used?", "references": ["Convolutional Neural Network, Gated Recurrent Unit, Bidirectional GRU with self-attention", "Convolutional Neural Network, Gated Recurrent Unit, Bidirectional GRU with self-attention", "Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU), Bidirectional GRU with self-attention ( INLINEFORM0 -GRU)"], "best_model_answer": "Convolutional Neural Network, Gated Recurrent Unit, Bidirectional GRU with self-attention", "oracle_answer": "Convolutional Neural Network, Gated Recurrent Unit, Bidirectional GRU with self-attention", "oracle_answer_rank": 1}
{"question_id": "5d93245832d90b31aee42ea2bf1e7704c22ebeca", "question": "What pre-trained word embeddings are used?", "references": ["word2vec, FastText, randomly initialized embeddings (random)", "word2vec train on Google News corpus; FastText train on Wikipedia corpus; randomly initialized embeddings", "word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus,"], "best_model_answer": "word2vec, FastText, randomly initialized embeddings (random)", "oracle_answer": "word2vec train on Google News corpus; FastText train on Wikipedia corpus; randomly initialized embeddings", "oracle_answer_rank": 2}
{"question_id": "c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "question": "What metrics are used to measure gender biases?", "references": ["False Positive Equality Difference, False Negative Equality Difference", "AUC scores on the original test set , AUC scores on the unbiased generated test set, the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate", "AUC scores on the original test set (Orig. AUC),  AUC scores on the unbiased generated test set (Gen. AUC), false positive/negative equality differences"], "best_model_answer": "False Positive Equality Difference, False Negative Equality Difference", "oracle_answer": "AUC scores on the original test set , AUC scores on the unbiased generated test set, the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate", "oracle_answer_rank": 2}
{"question_id": "26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "question": "Which dataset(s) do they train on?", "references": ["Quora Question Pairs", "Quora Question Pairs", "the public benchmark dataset of \u201cQuora Question Pairs\u201d"], "best_model_answer": "Quora Question Pairs", "oracle_answer": "Quora Question Pairs", "oracle_answer_rank": 1}
{"question_id": "ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "question": "By how much does their model outperform state-of-the-art baselines?", "references": ["0.78% over the best state-of-the-art baseline", "The average improvement in accuracy of their model over baselines is 3.026 points.", "by more than  0.18"], "best_model_answer": "0.78% over the best state-of-the-art baseline", "oracle_answer": "0.78% over the best state-of-the-art baseline", "oracle_answer_rank": 1}
{"question_id": "ee19fd54997f2eec7c87c7d4a2169026fe208285", "question": "Do they compare to previous work?", "references": ["Yes", "Unanswerable", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "question": "How large is the dataset used?", "references": ["3731 movies containing around 372353 reviews", "3731", "3,731 movies; for each movie we are given a large set of reviews (99.8 on average)"], "best_model_answer": "3731 movies containing around 372353 reviews", "oracle_answer": "3731 movies containing around 372353 reviews", "oracle_answer_rank": 1}
{"question_id": "6a20a3220c4edad758b912e2d3e5b99b0b295d96", "question": "How exactly do they weigh between different statistical models?", "references": ["They define cWeight as weight obtained for each sentence using all the models where the sentences is in the summary of predicted by each model.", "by training on field-specific corpora", "after training on corpus, we assign weights among the different techniques"], "best_model_answer": "They define cWeight as weight obtained for each sentence using all the models where the sentences is in the summary of predicted by each model.", "oracle_answer": "by training on field-specific corpora", "oracle_answer_rank": 2}
{"question_id": "d5dcc89a08924bed9772bc431090cbb52fb7836f", "question": "What showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score?", "references": ["Combination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model", "Jaccard/Cosine Similarity Matrix+TextRank\n+InferSent Based Model", "Best result was obtained by using combination of: Jaccard/Cosine Similarity Matrix, TextRank and InferSent Based Model"], "best_model_answer": "Combination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model", "oracle_answer": "Combination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model", "oracle_answer_rank": 1}
{"question_id": "6d6b0628d8a942c57d7af1447a563021be79bc64", "question": "Is the re-ranking approach described in this paper a transductive learning technique?", "references": ["Yes", "Unanswerable", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "b21245212244ad7adf7d321420f2239a0f0fe56b", "question": "How big is the test set used for evaluating the proposed re-ranking approach?", "references": ["3084 real user requests  assigned to suitable answers from the training corpus.", "3084 real user requests from a chat-log of T-Mobile Austria", "3084"], "best_model_answer": "3084 real user requests  assigned to suitable answers from the training corpus.", "oracle_answer": "3084 real user requests from a chat-log of T-Mobile Austria", "oracle_answer_rank": 2}
{"question_id": "4a201b8b9cc566b56aedb5ab45335f202bc41845", "question": "What is the new metric?", "references": ["They propose two new metrics. One, which they call the Neighbour Similarity Test, calculates how many shared characteristics there are between entities whose representations are neighbors in the embedding space. The second, which they call the Type and Category Test, is the same as the  Neighbour Similarity Test, except it uses entity types and categories in the place of individual entity characteristics.", "Neighbour Similarity Test; Type and Category Test", "Neighbour Similarity Test (NST) and Type and Category Test (TCT)"], "best_model_answer": "They propose two new metrics. One, which they call the Neighbour Similarity Test, calculates how many shared characteristics there are between entities whose representations are neighbors in the embedding space. The second, which they call the Type and Category Test, is the same as the  Neighbour Similarity Test, except it uses entity types and categories in the place of individual entity characteristics.", "oracle_answer": "Neighbour Similarity Test (NST) and Type and Category Test (TCT)", "oracle_answer_rank": 3}
{"question_id": "6a90135bd001be69a888076aff1b149b78adf443", "question": "How long do other state-of-the-art models take to process the same amount of data?", "references": ["RDF2Vec takes 123 minutes to generate random walks and an estimated 96 hours to train word2vec. KGloVe takes an estimated 12 hours to train GloVe. fastText takes an estimated 72 hours to train", "RDF2Vec: 123 minutes runtime with >96 hours training, FastText: 5 minutes with >72 hours training", "between 12 hours and 96 hours"], "best_model_answer": "RDF2Vec takes 123 minutes to generate random walks and an estimated 96 hours to train word2vec. KGloVe takes an estimated 12 hours to train GloVe. fastText takes an estimated 72 hours to train", "oracle_answer": "RDF2Vec: 123 minutes runtime with >96 hours training, FastText: 5 minutes with >72 hours training", "oracle_answer_rank": 2}
{"question_id": "f92c344e9b1a986754277fd0f08a47dc3e5f9feb", "question": "What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?", "references": ["perplexity and BLEU score are not good enough and correlate very weakly with human judgments, word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses, metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality", "The metrics correlate very weakly with human judgements, word-overlap metrics require too many ground-truth reposnses and embedding-based metrics are insufficiently complex for modeling sentence-level compositionality in dialogue", "As there can be multiple correct output utterances for a given input utterance there is no quantitative way to evaluate how well a model is performing., The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments. , According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses."], "best_model_answer": "perplexity and BLEU score are not good enough and correlate very weakly with human judgments, word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses, metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality", "oracle_answer": "perplexity and BLEU score are not good enough and correlate very weakly with human judgments, word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses, metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality", "oracle_answer_rank": 1}
{"question_id": "b10388e343868ca8e5c7c601ebb903f52e756e61", "question": "What metrics are typically used to compare models?", "references": ["BLeU, perplexity", " perplexity and BLEU score", "BLeU , perplexity "], "best_model_answer": "BLeU, perplexity", "oracle_answer": "BLeU, perplexity", "oracle_answer_rank": 1}
{"question_id": "e8cdeb3a081d51cc143c7090a54c82d393f1a2ca", "question": "Is there a benchmark to compare the different approaches?", "references": ["No", "Unanswerable", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "833d3ae7613500f2867ed8b33d233d71781014e7", "question": "What GAN and RL approaches are used?", "references": ["adversarial training for open-domain dialogue generation , trust region actor-critic with experience replay , episodic natural actor-critic with experience replay, multi-turn dialogue agent, on-policy Monte Carlo method ", "the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances., The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones, The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines.", "authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated, task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones"], "best_model_answer": "adversarial training for open-domain dialogue generation , trust region actor-critic with experience replay , episodic natural actor-critic with experience replay, multi-turn dialogue agent, on-policy Monte Carlo method ", "oracle_answer": "authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated, task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones", "oracle_answer_rank": 3}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "question": "What type of neural models are used?", "references": ["Sequence to Sequence approaches for dialogue modelling, Language Model based approaches for dialogue modelling", "Sequence to Sequence approaches, Language Model based approaches", "Sequence to Sequence approaches, Language Model "], "best_model_answer": "Sequence to Sequence approaches for dialogue modelling, Language Model based approaches for dialogue modelling", "oracle_answer": "Sequence to Sequence approaches, Language Model based approaches", "oracle_answer_rank": 2}
{"question_id": "89414ef7fcb2709c47827f30a556f543b9a9e6e0", "question": "How does this research compare to research going on in the US and USSR at this time?", "references": ["lagging only a couple of years behind the research of the superpowers", "Author of this research noted the USA prototype effort from 1954 and research papers in 1955as well as USSR effort from 1955. ", "It is worthwhile to note that both the USA and the USSR had access to state-of-the-art computers, and the political support for the production of such systems meant that computers were made available to researchers in machine translation. However, the results were poor in the late 1950s, and a working system was yet to be shown. All work was therefore theoretical work implemented on a computer, which proved to be sub-optimal."], "best_model_answer": "lagging only a couple of years behind the research of the superpowers", "oracle_answer": "Author of this research noted the USA prototype effort from 1954 and research papers in 1955as well as USSR effort from 1955. ", "oracle_answer_rank": 2}
{"question_id": "faffcc6ef27c1441e6528f924e320368430d8da3", "question": "What is the reason this research was not adopted in the 1960s?", "references": ["the lack of funding", " poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers", "the lack of federal funding, Laszlo\u2019s group had to manage without an actual computer"], "best_model_answer": "the lack of funding", "oracle_answer": "the lack of funding", "oracle_answer_rank": 1}
{"question_id": "baaa6ad7148b785429a20f38786cd03ab9a2646e", "question": "What were the usual logical approaches of the time period?", "references": ["They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype., Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian., Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards.,  Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7.", "to have a logical intermediate language, under the working name \u201cInterlingua\u201d, which was the connector of both natural languages", "The idea was to have a logical intermediate language"], "best_model_answer": "They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype., Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian., Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards.,  Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7.", "oracle_answer": "to have a logical intermediate language, under the working name \u201cInterlingua\u201d, which was the connector of both natural languages", "oracle_answer_rank": 2}
{"question_id": "0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "question": "what language was the data in?", "references": ["English", "English ", "English"], "best_model_answer": "English", "oracle_answer": "English", "oracle_answer_rank": 1}
{"question_id": "051034cc94f2c02d3041575c53f969b3311c9ea1", "question": "which automatic metrics were used in evaluation?", "references": ["BLEU, SARI", "BLEU , SARI ", "BLEU, SARI"], "best_model_answer": "BLEU, SARI", "oracle_answer": "BLEU, SARI", "oracle_answer_rank": 1}
{"question_id": "511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "question": "how do humans judge the simplified sentences?", "references": ["Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.", "We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.", "By fluency, adequacy, and simplicity using a five point Likert scale."], "best_model_answer": "Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.", "oracle_answer": "Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.", "oracle_answer_rank": 1}
{"question_id": "a3705b53c6710b41154c65327b7bbec175bdfae7", "question": "What kind of data is used to train the model?", "references": ["Modern and historical versions of literature like the Bible and a Spanish novel.", "Dutch Bible BIBREF1, El Quijote BIBREF2,  El Conde Lucanor BIBREF2", "Dutch Bible, El Quijote"], "best_model_answer": "Modern and historical versions of literature like the Bible and a Spanish novel.", "oracle_answer": "Dutch Bible, El Quijote", "oracle_answer_rank": 3}
{"question_id": "b62b7ec5128219f04be41854247d5af992797937", "question": "Does proposed approach use neural networks?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "question": "What machine learning techniques are used in the model architecture?", "references": ["Classical IMT approaches, Prefix-based IMT , Neural Machine Translation, Prefix-based Interactive Neural Machine Translation", "NMT systems using NMT-Keras, SMT systems were trained with Moses, Statistical IMT systems", "classification for SMT and neural methods for NMT"], "best_model_answer": "Classical IMT approaches, Prefix-based IMT , Neural Machine Translation, Prefix-based Interactive Neural Machine Translation", "oracle_answer": "classification for SMT and neural methods for NMT", "oracle_answer_rank": 3}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "question": "By how much did their model outperform baselines?", "references": ["Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.", "Their average F1 score is higher than that of baseline by 0.0234 ", "on event expression tasks average by 2.3% with respect to F1; on phase 2 subtask by 11.3% with respect to recall"], "best_model_answer": "Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.", "oracle_answer": "on event expression tasks average by 2.3% with respect to F1; on phase 2 subtask by 11.3% with respect to recall", "oracle_answer_rank": 3}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "question": "Which baselines did they compare against?", "references": ["memorization, median report, max report", "memorization baseline", "memorization"], "best_model_answer": "memorization, median report, max report", "oracle_answer": "memorization", "oracle_answer_rank": 3}
{"question_id": "11f9c207476af75a9272105e646df02594059c3f", "question": "What dataset did they use to evaluate?", "references": ["Clinical TempEval corpus", "Clinical TempEval corpus", "Clinical TempEval corpus"], "best_model_answer": "Clinical TempEval corpus", "oracle_answer": "Clinical TempEval corpus", "oracle_answer_rank": 1}
{"question_id": "9ea3669528c2b295f21770cb7f70d0c4b4389223", "question": "what was their system's f1 score?", "references": ["0.6955", "0.6955", "69.55"], "best_model_answer": "0.6955", "oracle_answer": "0.6955", "oracle_answer_rank": 1}
{"question_id": "9863f5765ba70f7ff336a580346ef70205abbbd8", "question": "what were the baselines?", "references": ["RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM, Word2vec, Multi-kernel, CNN, Memnet", "RB (Rule based method), CB (Common-sense based method), RB+CB+ML, SVM, Word2vec, Multi-kernel, CNN", "RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM classifier using the unigram, bigram and trigram features, SVM classifier using word representations learned by Word2vec, multi-kernel method BIBREF31,  convolutional neural network for sentence classification BIBREF5"], "best_model_answer": "RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM, Word2vec, Multi-kernel, CNN, Memnet", "oracle_answer": "RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM, Word2vec, Multi-kernel, CNN, Memnet", "oracle_answer_rank": 1}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "question": "what emotion cause dataset was used?", "references": ["simplified Chinese emotion cause corpus BIBREF31", "a simplified Chinese emotion cause corpus BIBREF31", "Chinese emotion cause corpus"], "best_model_answer": "simplified Chinese emotion cause corpus BIBREF31", "oracle_answer": "simplified Chinese emotion cause corpus BIBREF31", "oracle_answer_rank": 1}
{"question_id": "58a3cfbbf209174fcffe44ce99840c758b448364", "question": "what are the recent models they compare with?", "references": ["Recurrent Highway Networks, NAS, BIBREF5", "BIBREF1, Neural Cache BIBREF6, BIBREF0", "Recurrent Highway Networks, NAS "], "best_model_answer": "Recurrent Highway Networks, NAS, BIBREF5", "oracle_answer": "Recurrent Highway Networks, NAS, BIBREF5", "oracle_answer_rank": 1}
{"question_id": "6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "question": "what were their results on the hutter prize dataset?", "references": ["slightly off the state of the art", "1.30 and 1.31", "1.30 BPC is their best result"], "best_model_answer": "slightly off the state of the art", "oracle_answer": "1.30 and 1.31", "oracle_answer_rank": 2}
{"question_id": "54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "question": "what architectures were reevaluated?", "references": ["LSTMs, Recurrent Highway Networks, NAS", "Answer with content missing: (Architecture section missing) The Long Short-Term Memory, Recurrent Highway Network and NAS", "LSTM, RHN and NAS."], "best_model_answer": "LSTMs, Recurrent Highway Networks, NAS", "oracle_answer": "LSTMs, Recurrent Highway Networks, NAS", "oracle_answer_rank": 1}
{"question_id": "5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "question": "what dataset was used?", "references": ["WMT 2014,  UGC (User Generated Content)", "11.5k French reviews from Foursquare", "WMT 2014, UGC (User Generated Content)"], "best_model_answer": "WMT 2014,  UGC (User Generated Content)", "oracle_answer": "WMT 2014,  UGC (User Generated Content)", "oracle_answer_rank": 1}
{"question_id": "eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "question": "what are the human evaluation metrics?", "references": [" translation quality.", "The outputs are ranked by human evaluators, the wins, ties and losses are counted, then the Wilcoxon signed-rank test is applied.", "number of wins, ties and losses, and apply the Wilcoxon signed-rank test"], "best_model_answer": " translation quality.", "oracle_answer": "The outputs are ranked by human evaluators, the wins, ties and losses are counted, then the Wilcoxon signed-rank test is applied.", "oracle_answer_rank": 2}
{"question_id": "806fefe0e331ddb3c17245d6a9fa7433798e367f", "question": "what are the existing online systems?", "references": ["Existing online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).", "Google Translate", "Google Translate, DeepL"], "best_model_answer": "Existing online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).", "oracle_answer": "Google Translate, DeepL", "oracle_answer_rank": 3}
{"question_id": "458e5ed506883bfec6623102ec9f43c071f0616f", "question": "What are their baselines?", "references": ["an unoptimised linear-kernel SVM, a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms", "unoptimised linear-kernel SVM, keyword-based system", "Linear-kernel SVM based on word n-grams, vocabulary-based classifier."], "best_model_answer": "an unoptimised linear-kernel SVM, a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms", "oracle_answer": "unoptimised linear-kernel SVM, keyword-based system", "oracle_answer_rank": 2}
{"question_id": "85ab5f773b297bcf48a274634d402a35e1d57446", "question": "Do they report the annotation agreement?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "5154f63c50729b8ac04939588c2f5ffeb916e3df", "question": "How long is the test dataset for Dutch?", "references": ["Random 10 percent out of 78381 posts.", "sample ( INLINEFORM2 ) of all data", "78387"], "best_model_answer": "Random 10 percent out of 78381 posts.", "oracle_answer": "Random 10 percent out of 78381 posts.", "oracle_answer_rank": 1}
{"question_id": "42dcf1bb19b8470993c05e55413eed487b0f2559", "question": "Where is the propaganda material sourced from?", "references": [" online English magazine called Dabiq", "Dabiq", "English magazine called Dabiq"], "best_model_answer": " online English magazine called Dabiq", "oracle_answer": "English magazine called Dabiq", "oracle_answer_rank": 3}
{"question_id": "2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "question": "Which behavioural features are used?", "references": ["frequency of tweets posted, followers/following ratio, degree of influence each user has over their network", "frequency of tweets posted,  followers/following ratio, using hashtags, using mention action", "frequency of tweets posted, followers/following ratio, users' interactions with others through using hashtags, engagement in discussions using mention action"], "best_model_answer": "frequency of tweets posted, followers/following ratio, degree of influence each user has over their network", "oracle_answer": "frequency of tweets posted,  followers/following ratio, using hashtags, using mention action", "oracle_answer_rank": 2}
{"question_id": "824629b36a75753b1500d9dcaee0fc3c758297b1", "question": "Which psychological features are used?", "references": ["Analytically thinking, Clout, Tone, Authentic, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism, positive emotions, negative emotions, personal drives, namely power, reward, risk, achievement, and affiliation, number of 1st, 2nd, and 3rd personal pronouns used.", "Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism", "summary variable - analytically thinking, clout, tone, authentic, Big five variable - openness, conscientiousness, extraversion, agreeableness, neuroticism, Emotional variables - positive emotions in the text, negative emotions in the text, personal drives - power, reward, risk, achievement, affiliation, personal pronouns -  counts the number of 1st, 2nd, and 3rd personal pronouns used, Minkowski distance between each profile and average values of these features created from the ISIS magazines"], "best_model_answer": "Analytically thinking, Clout, Tone, Authentic, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism, positive emotions, negative emotions, personal drives, namely power, reward, risk, achievement, and affiliation, number of 1st, 2nd, and 3rd personal pronouns used.", "oracle_answer": "Analytically thinking, Clout, Tone, Authentic, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism, positive emotions, negative emotions, personal drives, namely power, reward, risk, achievement, and affiliation, number of 1st, 2nd, and 3rd personal pronouns used.", "oracle_answer_rank": 1}
{"question_id": "31894361833b3e329a1fb9ebf85a78841cff229f", "question": "Which textual features are used?", "references": ["N-grams, word2vec", "uni-grams, bi-grams, tri-grams", "ratio of violent words in the tweet, ratio of curse words in the tweet, frequency of words with all capital letters, 200 dimension sized vector for the tweet calculated using word embedding, tf-idf scores for top scoring uni-grams, bi-grams and tri-grams"], "best_model_answer": "N-grams, word2vec", "oracle_answer": "uni-grams, bi-grams, tri-grams", "oracle_answer_rank": 2}
{"question_id": "cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "question": "what is the cold-start problem?", "references": ["Dealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\"", "hotels/items appear infrequently or never in historical data, Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data", "Unanswerable"], "best_model_answer": "Dealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\"", "oracle_answer": "Dealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\"", "oracle_answer_rank": 1}
{"question_id": "1d047286ac63e5dca1ab811172b89d7d125679e5", "question": "what dataset was used for training?", "references": ["Our dataset contains more than 40M user click sessions", " dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels", "A dataset containing 40M user click sessions with more than 1.1M unique hotels."], "best_model_answer": "Our dataset contains more than 40M user click sessions", "oracle_answer": "A dataset containing 40M user click sessions with more than 1.1M unique hotels.", "oracle_answer_rank": 3}
{"question_id": "9887ca3d25e2109f41d1da80eeea05c465053fbc", "question": "How big is the dataset?", "references": ["7451 sentences", "total of 7451 sentences ", "7451 sentences, 6949 polar expressions, 5289 targets, 635 holders"], "best_model_answer": "7451 sentences", "oracle_answer": "7451 sentences", "oracle_answer_rank": 1}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "question": "What are all the domains the corpus came from?", "references": ["Unanswerable", " a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.", "professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc"], "best_model_answer": "Unanswerable", "oracle_answer": " a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.", "oracle_answer_rank": 2}
{"question_id": "f2b1e87f61c65aaa99bcf9825de11ae237260270", "question": "What are state-of-the-art baselines?", "references": ["guo2019towards who achieve state-of-the-art performance", "For SParC, context-dependent seq2seq and syntaxSQL-con. For Spider, a recursive decoding procedure, graph neural networks, and intermediate representation models.", "SQLNet, SyntaxSQLNet,\nSyntxSQLNet + data augmentation,\nRecursive Decodoing Procedure Lee(2019),\nGNN,\nIRNet and IRNet(BERT)"], "best_model_answer": "guo2019towards who achieve state-of-the-art performance", "oracle_answer": "For SParC, context-dependent seq2seq and syntaxSQL-con. For Spider, a recursive decoding procedure, graph neural networks, and intermediate representation models.", "oracle_answer_rank": 2}
{"question_id": "78c7318b2218b906a67d8854f3e511034075f79a", "question": "Which dialogue data do they use to evaluate on?", "references": ["Datasets from PersonaChat and Wizard of Wikipedia tasks.", "PersonaChat, Wizard of Wikipedia", "PersonaChat BIBREF5, Wizard of Wikipedia BIBREF7"], "best_model_answer": "Datasets from PersonaChat and Wizard of Wikipedia tasks.", "oracle_answer": "PersonaChat, Wizard of Wikipedia", "oracle_answer_rank": 2}
{"question_id": "3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "question": "What are the five domains?", "references": ["subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software)", "subtitles, medical, legal, Koran, IT", "subtitles, medical text, legal text, translations of the Koran, IT-related text"], "best_model_answer": "subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software)", "oracle_answer": "subtitles, medical text, legal text, translations of the Koran, IT-related text", "oracle_answer_rank": 3}
{"question_id": "c0847af3958d791beaa14c4040ada2d364251c4d", "question": "Which pre-trained language models are used?", "references": ["BERT, DistilBERT, RoBERTa", "BERT, DistilBERT, RoBERTa, GPT-2, XLNet", "BERT, DistilBERT, RoBERTa, GPT-2, XLNet"], "best_model_answer": "BERT, DistilBERT, RoBERTa", "oracle_answer": "BERT, DistilBERT, RoBERTa, GPT-2, XLNet", "oracle_answer_rank": 2}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "question": "Do they report results only on English data?", "references": ["Yes", "Unanswerable", "Unanswerable"], "best_model_answer": "Yes", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "ce23849e9e9a22626965f1ca8ca948a5c87280e9", "question": "What are the hyperparameter setting of the MTL model?", "references": ["size of word embeddings is 200, size of position embedding is 100, the number of attention heads in transformer block is 6, the number of attention block Is 2, dropout of multi-head attention is 0.7, minibatch size is 64, the initiall learning rate is .001. In fake news detection, the dropout rate is 0.3 and lambda is 0.6.", "The sizes of word embeddings and position embeddings are set to 200 and 100, attention heads and blocks are set to 6 and 2 respectively, and the dropout of multi-head attention is set to 0.7,  the minibatch size is 64, the initial learning rate is set to 0.001, the dropout rate to 0.3, and $\\lambda $ to 0.6 for fake news detection.", "Size of word embeddings is 200, size of position embeddings is 100, 6 attention heads and 2 blocks in encoder, dropout in multi-head attention is 0.7, minibatch size is 64, initial learning rate is 0.001, dropout rate is 0.3, lambda is 0.6."], "best_model_answer": "size of word embeddings is 200, size of position embedding is 100, the number of attention heads in transformer block is 6, the number of attention block Is 2, dropout of multi-head attention is 0.7, minibatch size is 64, the initiall learning rate is .001. In fake news detection, the dropout rate is 0.3 and lambda is 0.6.", "oracle_answer": "Size of word embeddings is 200, size of position embeddings is 100, 6 attention heads and 2 blocks in encoder, dropout in multi-head attention is 0.7, minibatch size is 64, initial learning rate is 0.001, dropout rate is 0.3, lambda is 0.6.", "oracle_answer_rank": 3}
{"question_id": "6c1614991647705265fb348d28ba60dd3b63b799", "question": "what is the test set size?", "references": ["2,169 sentences", "2,169 sentences", "2,169 sentences"], "best_model_answer": "2,169 sentences", "oracle_answer": "2,169 sentences", "oracle_answer_rank": 1}
{"question_id": "b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "question": "what are the evaluation metrics used?", "references": ["BLEU-4, slot error rate", "informativeness , naturalness ", "BLEU-4, slot error rate, informativeness, naturalness"], "best_model_answer": "BLEU-4, slot error rate", "oracle_answer": "BLEU-4, slot error rate, informativeness, naturalness", "oracle_answer_rank": 3}
{"question_id": "e4ea0569b637d5f56f63e933b8f269695fe1a926", "question": "What baseline did they use?", "references": ["we compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF", "HexaF", "HexaF - UCL "], "best_model_answer": "we compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF", "oracle_answer": "HexaF", "oracle_answer_rank": 2}
{"question_id": "905a8d775973882227549e960c7028e4a3561752", "question": "How was the masking done?", "references": ["The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "Unanswerable", "similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank"], "best_model_answer": "The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "oracle_answer": "The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "oracle_answer_rank": 1}
{"question_id": "0ef114d24a7a32821967e912dff23c016c4eab41", "question": "Which prior approaches for style transfer do they test with?", "references": ["Multi-Attribute Style Transfer, Adversarial Style Transfer ", "AttnCopy-S2S, Rule-based Method, Multi-Attribute Style Transfer (MAST) BIBREF11, Adversarial Style Transfer (AdvST) BIBREF12", "Multi-Attribute Style Transfer, Adversarial Style Transfer"], "best_model_answer": "Multi-Attribute Style Transfer, Adversarial Style Transfer ", "oracle_answer": "Multi-Attribute Style Transfer, Adversarial Style Transfer ", "oracle_answer_rank": 1}
{"question_id": "c32fc488f0527f330273263fa8956788bd071efc", "question": "Which content coverage constraints do they design?", "references": ["Answer with content missing: (Content Coverage Constraint section) We thus devise an additional learning constraint based on the nature of content description\u2014each data tuple in the content record should usually be mentioned exactly once in the generated sentence.\nThe copy mechanism over content record x enables a simple yet effective way to encourage the behavior. Intuitively, we want each tuple to be copied once and only once on average.", "Unanswerable", "Unanswerable"], "best_model_answer": "Answer with content missing: (Content Coverage Constraint section) We thus devise an additional learning constraint based on the nature of content description\u2014each data tuple in the content record should usually be mentioned exactly once in the generated sentence.\nThe copy mechanism over content record x enables a simple yet effective way to encourage the behavior. Intuitively, we want each tuple to be copied once and only once on average.", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "d207f78beb6cd754268881bf575c8f98000667ea", "question": "how many sentiment labels do they explore?", "references": ["3", "3", "3"], "best_model_answer": "3", "oracle_answer": "3", "oracle_answer_rank": 1}
{"question_id": "c077519ea42c9649fb78da34485de2262a0df779", "question": "Is it possible for the DCM module to correct text-relevant content?", "references": ["No", "Unanswerable", "Yes"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "question": "What statistical test is performed?", "references": ["t-test", "t-test", "Unanswerable"], "best_model_answer": "t-test", "oracle_answer": "t-test", "oracle_answer_rank": 1}
{"question_id": "9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "question": "Which datasets are used for evaluation?", "references": ["Argument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus", "Arguments, ACE 2005 dataset, part-of-speech tags from Universal Dependencies v. 1.3 for English, CoNLL 2000 shared task dataset on chunking, CoNLL 2003 shared task on named entity recognition, GENIA NER, WNUT16", "For the first experiment, the datasets used were: argument component detection persuasive essays, ACE 2005 dataset of entities/essays, POS tags from Universal Dependencies, CoNLL 2000 shared task on chunking, CoNLL 2003\nshared task on named entity recognition, the Bio-Entity Recognition Task dataset, WNUT 16 dataset on NER over tweets. For the second experiment, they used the CoNLL 2003 NER\ndataset, the Stanford Sentiment Treebank (SST5) dataset, the constituency parsing model for the\nPenn TreeBank as dataset, and the Stanford Natural Language Inference Corpus (SNLI) dataset."], "best_model_answer": "Argument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus", "oracle_answer": "Argument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus", "oracle_answer_rank": 1}
{"question_id": "5cc5e2db82f5d40a5244224dad94da50b4f673db", "question": "What does the human-in-the-loop do to help their system?", "references": ["identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it", "appropriately modify the text to create an unbiased version", "modify the text to create an unbiased version"], "best_model_answer": "identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it", "oracle_answer": "modify the text to create an unbiased version", "oracle_answer_rank": 3}
{"question_id": "6c5a64b5150305c584326882d37af5b0e58de2fd", "question": "How do they evaluate their de-biasing approach?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "f7a27de3eb6447377eb48ef6d2201205ff943751", "question": "Is there a metric that also rewards good stylistic response?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "question": "On what two languages is experimented on?", "references": ["Chinese and English", "Chinese, English", "English and Chinese"], "best_model_answer": "Chinese and English", "oracle_answer": "Chinese and English", "oracle_answer_rank": 1}
{"question_id": "044cb5ef850c0a2073682bb31d919d504667f907", "question": "What IS versification?", "references": ["the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d)", "the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.", "Unanswerable"], "best_model_answer": "the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d)", "oracle_answer": "the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d)", "oracle_answer_rank": 1}
{"question_id": "2301424672cb79297cf7ad95f23b58515e4acce8", "question": "Is Henry VIII reflective of Shakespeare in general?", "references": ["No", "Yes", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "9925e7d8757e8fd7411bcb5250bc08158a244fb3", "question": "What are the modifications by Thomas Merriam?", "references": ["Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.", "a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.", "measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. "], "best_model_answer": "Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.", "oracle_answer": "a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.", "oracle_answer_rank": 2}
{"question_id": "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "question": "What sources of less sensitive data are available?", "references": ["MIMICII(I), THYME, results from i2b2 and ShARe/CLEF shared task, MiPACQ, Blulab, EMC Dutch Clinical Corpus, 2010 i2b2/VA, VetCompass", "deceased persons, surrogate data, derived data, veterinary texts", "personal health information of deceased persons, surrogate data, derived data. Data that can not be used to reconstruct the original text, veterinary texts"], "best_model_answer": "MIMICII(I), THYME, results from i2b2 and ShARe/CLEF shared task, MiPACQ, Blulab, EMC Dutch Clinical Corpus, 2010 i2b2/VA, VetCompass", "oracle_answer": "deceased persons, surrogate data, derived data, veterinary texts", "oracle_answer_rank": 2}
{"question_id": "8126c6b8a0cab3e22661d3d71d96aa57360da65c", "question": "what evaluation metrics were used?", "references": ["EM-outline, EM-sec, Rouge", "EMoutline, EMsec, Rougehead", "EM INLINEFORM0 , EM INLINEFORM0, Rouge INLINEFORM0"], "best_model_answer": "EM-outline, EM-sec, Rouge", "oracle_answer": "EM-outline, EM-sec, Rouge", "oracle_answer_rank": 1}
{"question_id": "1a419468d255d40ae82ed7777618072a48f0091b", "question": "How to extract affect attributes from the sentence?", "references": ["Using a dictionary of emotional words, LIWC, they perform keyword spotting.", "A sentence is represented by five features that each mark presence or absence of an emotion: positive emotion, angry, sad, anxious, and negative emotion.", "either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$"], "best_model_answer": "Using a dictionary of emotional words, LIWC, they perform keyword spotting.", "oracle_answer": "either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$", "oracle_answer_rank": 3}
{"question_id": "baad4b6f834d5944f61bd12f30908e3cf3739dcd", "question": "Which BERT-based baselines do they compare to?", "references": ["BERT. We add a linear layer on top of BERT and we fine-tune it, BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b)., BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC", "BERT, BERT-Joint, BERT-Granularity", "BERT with one separately trained linear layer for each of the two tasks, BERT-Joint, which trains a layer for both tasks jointly, BERT-Granularity,  a modification of BERT-Joint which transfers information from the less granular task to the more granular task. "], "best_model_answer": "BERT. We add a linear layer on top of BERT and we fine-tune it, BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b)., BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC", "oracle_answer": "BERT with one separately trained linear layer for each of the two tasks, BERT-Joint, which trains a layer for both tasks jointly, BERT-Granularity,  a modification of BERT-Joint which transfers information from the less granular task to the more granular task. ", "oracle_answer_rank": 3}
{"question_id": "0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "question": "What datasets did they use in their experiment?", "references": ["retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques", "A dataset of news articles from different news outlets collected by the authors.", "451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4"], "best_model_answer": "retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques", "oracle_answer": "retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques", "oracle_answer_rank": 1}
{"question_id": "7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "question": "What size ngram models performed best? e.g. bigram, trigram, etc.", "references": ["bigram ", "the trigram language model performed better on Subtask B, the bigram language model performed better on Subtask A", "advantage of bigrams on Subtask A was very slight"], "best_model_answer": "bigram ", "oracle_answer": "the trigram language model performed better on Subtask B, the bigram language model performed better on Subtask A", "oracle_answer_rank": 2}
{"question_id": "8b1af67e3905244653b4cf66ba0acec8d6bff81f", "question": "How were the ngram models used to generate predictions on the data?", "references": ["The n-gram models were used to calculate the logarithm of the probability for each tweet", "system sorts all the tweets for each hashtag and orders them based on their log probability score", "The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first"], "best_model_answer": "The n-gram models were used to calculate the logarithm of the probability for each tweet", "oracle_answer": "The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first", "oracle_answer_rank": 3}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "question": "What package was used to build the ngram language models?", "references": ["KenLM Toolkit", "KenLM Toolkit", "KenLM Toolkit"], "best_model_answer": "KenLM Toolkit", "oracle_answer": "KenLM Toolkit", "oracle_answer_rank": 1}
{"question_id": "fd8a8eb69f07c584a76633f8802c2746f7236d64", "question": "Do the authors report only on English", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "159025c44c0115ab4cdc253885384f72e592e83a", "question": "How does hard debiasing affect gender bias in prediction and performance?", "references": ["mitigating the difference in F1 scores for all relations, debiased embeddings increases absolute score", "Unanswerable", "Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations"], "best_model_answer": "mitigating the difference in F1 scores for all relations, debiased embeddings increases absolute score", "oracle_answer": "mitigating the difference in F1 scores for all relations, debiased embeddings increases absolute score", "oracle_answer_rank": 1}
{"question_id": "cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "question": "what crowdsourcing platform did they use?", "references": ["Amazon Mechanical Turk", "Amazon Mechanical Turk", " Amazon Mechanical Turk"], "best_model_answer": "Amazon Mechanical Turk", "oracle_answer": "Amazon Mechanical Turk", "oracle_answer_rank": 1}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "question": "did they crowdsource annotations?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "29794bda61665a1fbe736111e107fd181eacba1b", "question": "where does their data come from?", "references": ["Snopes", "Snopes ", "Snopes fact-checking website"], "best_model_answer": "Snopes", "oracle_answer": "Snopes", "oracle_answer_rank": 1}
{"question_id": "fc62549a8f0922c09996a119b2b6a8b5e829e989", "question": "Which metrics are used for quantitative analysis?", "references": ["perplexity, cross entropy", "Cross entropy between the trained model and models trained on different corpora.", "a measure that calculates the cross entropy between the word distribution of the model output and that of the target data"], "best_model_answer": "perplexity, cross entropy", "oracle_answer": "Cross entropy between the trained model and models trained on different corpora.", "oracle_answer_rank": 2}
{"question_id": "e2a507749a4a3201edd6413c77ad0d4c23e9c6ce", "question": "Is their data open sourced?", "references": ["No", "Unanswerable", "Unanswerable"], "best_model_answer": "No", "oracle_answer": "Unanswerable", "oracle_answer_rank": 2}
{"question_id": "0035b351df63971ec57e36d4bfc6f7594bed41ae", "question": "How is the data labeled?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "3699927c6c1146f5057576034d226a99946d52cb", "question": "what languages did they evaluate on?", "references": ["Arabic, Chinese, French, English, and German", "Arabic, Chinese, French, English, and German", "Arabic, Chinese, English, French, and German"], "best_model_answer": "Arabic, Chinese, French, English, and German", "oracle_answer": "Arabic, Chinese, French, English, and German", "oracle_answer_rank": 1}
{"question_id": "6606160e210d05b94f7cbd9c5ff91947339f9d02", "question": "were these categories human evaluated?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "4beb50ba020f624446ff1ef5bf4adca5ed318b98", "question": "What languages are evaluated?", "references": ["Unanswerable", "Unanswerable", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "1c8958ec50976a9b1088c51e8f73a767fb3973fa", "question": "what rnn classifiers were used?", "references": ["LSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.", "experiment with combining various LSTM models enhanced with a number of novel features (O No additional features,  NS Neutral & Sexism,  NR Neutral & Racism, RS Racism & Sexism,  NRS Neutral, Racism & Sexism) in an ensemble.", "Long Short-Term Memory Network (LSTM)"], "best_model_answer": "LSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.", "oracle_answer": "LSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.", "oracle_answer_rank": 1}
{"question_id": "363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "question": "what results did their system obtain?", "references": ["Best authors' system achieved 0.9320 F1 score.", "The best model achieved a 0.9320 F-score", "The best performing single classifier produces F1 0.9265. The best ensemble classifier  (O+NS+RS+NR+NRS) produce  F1 0.9320."], "best_model_answer": "Best authors' system achieved 0.9320 F1 score.", "oracle_answer": "Best authors' system achieved 0.9320 F1 score.", "oracle_answer_rank": 1}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "question": "Which dataset do they use?", "references": ["500 rescored intent annotations found in the lattices in cancellations and refunds domain", "dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain", "dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain"], "best_model_answer": "500 rescored intent annotations found in the lattices in cancellations and refunds domain", "oracle_answer": "dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain", "oracle_answer_rank": 2}
{"question_id": "9186b2c5b7000ab7f15a46a47da73ea45544bace", "question": "How is the model evaluated against the original recursive training algorithm?", "references": ["The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.", "We perform an error analysis, with the purpose of gaining more insight into the ability of the methods to model particular aspects of morphology.", "boundary precision, boundary recall,  boundary $F_{1}$-score", "Morfessor EM+Prune configuration significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi. Morfessor EM+Prune is less responsive to tuning than Morfessor Baseline."], "best_model_answer": "The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.", "oracle_answer": "The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.", "oracle_answer_rank": 1}
{"question_id": "2d91554c3f320a4bcfeb00aa466309074a206712", "question": "Which metrics do they use to evaluate results?", "references": ["BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics, precision, recall and F-measure", "BLEU , METEOR , chrF", "BLEU BIBREF35, METEOR BIBREF36, chrF BIBREF37, precision, recall , F-measure"], "best_model_answer": "BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics, precision, recall and F-measure", "oracle_answer": "BLEU BIBREF35, METEOR BIBREF36, chrF BIBREF37, precision, recall , F-measure", "oracle_answer_rank": 3}
{"question_id": "25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "question": "Which baselines did they compare to?", "references": ["Fine tuned DIaloGPT and GPT2 on Interview without speaker information.", "finetune (FT) DialoGPT and GPT2 on Interview without speaker information", "two models (GPT2 and DialoGPT) on two datasets (DailyDialog and CALLHOME)"], "best_model_answer": "Fine tuned DIaloGPT and GPT2 on Interview without speaker information.", "oracle_answer": "Fine tuned DIaloGPT and GPT2 on Interview without speaker information.", "oracle_answer_rank": 1}
{"question_id": "d68cc9aaf0466b97354600a5646c3be4512fc096", "question": "What dialog tasks was it experimented on?", "references": ["role modeling in media dialog , role change detection ", "1) generating appropriate responses for a specific role given a conversation history (speaker role modeling), 2) predicting whether a new speaker will interject on the next sentence of a conversation", "role modeling in media dialog and role change detection on Interview"], "best_model_answer": "role modeling in media dialog , role change detection ", "oracle_answer": "role modeling in media dialog and role change detection on Interview", "oracle_answer_rank": 3}
{"question_id": "d038e5d2a6f85e68422caaf8b96cb046db6599fa", "question": "How was annotation done?", "references": ["Unanswerable", "annotations for each turn", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "question": "Which news outlets did they focus on?", "references": ["natural dialog", "Unanswerable", "NPR"], "best_model_answer": "natural dialog", "oracle_answer": "natural dialog", "oracle_answer_rank": 1}
{"question_id": "7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "question": "Which real world datasets do they experiment on?", "references": ["Reuters, 20 Newsgroup, New York Times, Web Snippet, Tag My News, ABC News", "Reuters, 20NG, 20 Newsgroup, NYT, New York Times, WS, Web Snippet, TMN, Tag My News, AN, ABC News", "Reuters-21578 dataset, 20 Newsgroup, New York Times, Tag My News, ABC News, Web Snippet"], "best_model_answer": "Reuters, 20 Newsgroup, New York Times, Web Snippet, Tag My News, ABC News", "oracle_answer": "Reuters, 20 Newsgroup, New York Times, Web Snippet, Tag My News, ABC News", "oracle_answer_rank": 1}
{"question_id": "e12166fa9d6f63c4e92252c95c6a7bc96977ebf4", "question": "Is this an English language corpus?", "references": ["No", "Yes", "Yes"], "best_model_answer": "No", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "f1f7a040545c9501215d3391e267c7874f9a6004", "question": "what dataset was used?", "references": ["CoNLL_X bosque data, News data by Lusa agency, Sports news data", "News, Sports news", "News, Sports news"], "best_model_answer": "CoNLL_X bosque data, News data by Lusa agency, Sports news data", "oracle_answer": "News, Sports news", "oracle_answer_rank": 2}
{"question_id": "3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "question": "did they experiment with other languages besides portuguese?", "references": ["No", "No", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "question": "How are labels for trolls obtained?", "references": ["the ground truth labels for the troll users are available, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves", "We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.", "manual labeling, more realistic scenario assumes that labels for troll accounts are not available"], "best_model_answer": "the ground truth labels for the troll users are available, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves", "oracle_answer": "We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.", "oracle_answer_rank": 2}
{"question_id": "70512cc9dcd45157e40c8d1f85e82d21ade7645b", "question": "What datasets is the model evaluated on?", "references": ["CNN , Daily Mail,  CBT CN and NE", "CNN, Daily Mail and CBT", "CNN, Daily Mail, Children's Book Test"], "best_model_answer": "CNN , Daily Mail,  CBT CN and NE", "oracle_answer": "CNN, Daily Mail and CBT", "oracle_answer_rank": 2}
{"question_id": "5b95665d44666a1dc9e568d2471e5edf8614859f", "question": "What are the performances of LSTMs and humans on the task?", "references": ["mean AUC of 0.56 for the LTSM and of 0.58 for humans", "LTSM 0.56 AUC, humans 0.58 AUC", "LSTM obtains an overall score of 0.56 while humans' score is 0.58"], "best_model_answer": "mean AUC of 0.56 for the LTSM and of 0.58 for humans", "oracle_answer": "mean AUC of 0.56 for the LTSM and of 0.58 for humans", "oracle_answer_rank": 1}
{"question_id": "740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "question": "What social media platforms are represented?", "references": ["Twitter", "Twitter ", "Twitter"], "best_model_answer": "Twitter", "oracle_answer": "Twitter", "oracle_answer_rank": 1}
{"question_id": "bc05503eef25c732f1785e29d59b6022f12ba094", "question": "What other evaluation metrics did they use other than ROUGE-L??", "references": ["they also use ROUGE-1 and ROUGE-2", "Rouge-1, Rouge-2, Rouge Recall, Rouge F1", "ROUGE-1 and ROUGE-2", "ROUGE-1 and ROUGE-2"], "best_model_answer": "they also use ROUGE-1 and ROUGE-2", "oracle_answer": "ROUGE-1 and ROUGE-2", "oracle_answer_rank": 3}
{"question_id": "b57ad10468e1ba2a7a34396688dbb10a575d89f5", "question": "What loss function is used?", "references": ["negative log-likelihood", "negative log-likelihood", "negative log-likelihood"], "best_model_answer": "negative log-likelihood", "oracle_answer": "negative log-likelihood", "oracle_answer_rank": 1}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "question": "Does the query-bag matching model use a neural network?", "references": ["Yes", "Yes", "Yes"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "question": "What datasets are used for experiments?", "references": ["the AliMe and Quora dataset", "AliMe and Quora", "AliMe , Quora"], "best_model_answer": "the AliMe and Quora dataset", "oracle_answer": "AliMe and Quora", "oracle_answer_rank": 2}
{"question_id": "de4cc9e7fa5d700f5046d60789770f47911b3dd7", "question": "Which natural language(s) is/are studied?", "references": ["Unanswerable", "Chinese and English", "Unanswerable"], "best_model_answer": "Unanswerable", "oracle_answer": "Unanswerable", "oracle_answer_rank": 1}
{"question_id": "b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "question": "How many lexical features are considered?", "references": ["No", "3: In addition to word embedding, there is a POS tag embedding and a dependcy relation embedding. ", "Unanswerable"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "4cb2e80da73ae36de372190b4c1c490b72977ef8", "question": "Did they represent tie strength only as number of social ties in a networks? ", "references": ["Yes", "Yes, a normalized mutual friends metric", "No"], "best_model_answer": "Yes", "oracle_answer": "Yes", "oracle_answer_rank": 1}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "question": "How do they preprocess Tweets?", "references": ["They removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.", "re-tweets do not bring any additional information to our study, thus we removed them,  removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags, downcased and stripped the punctuation", "removing URLs, emoticons, mentions of other users, hashtags; downcasing and stripping punctuations"], "best_model_answer": "They removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.", "oracle_answer": "They removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.", "oracle_answer_rank": 1}
{"question_id": "2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "question": "What kind of inference model do they build to estimate socioeconomic status?", "references": ["XGBoost", "XGBoost algorithm BIBREF43", "XGBoost, an ensemble of gradient-based decision trees algorithm "], "best_model_answer": "XGBoost", "oracle_answer": "XGBoost algorithm BIBREF43", "oracle_answer_rank": 2}
{"question_id": "892ee7c2765b3764312c3c2b6f4538322efbed4e", "question": "How much data do they gather in total?", "references": ["90,369,215 tweets written in French, posted by 1.3 Million users", "They created 3 datasets with combined size of 37193.", "90,369,215 tweets"], "best_model_answer": "90,369,215 tweets written in French, posted by 1.3 Million users", "oracle_answer": "90,369,215 tweets written in French, posted by 1.3 Million users", "oracle_answer_rank": 1}
{"question_id": "c68946ae2e548ec8517c7902585c032b3f3876e6", "question": "Do they analyze features which help indicate socioeconomic status?", "references": ["No", "Yes", "Yes"], "best_model_answer": "No", "oracle_answer": "Yes", "oracle_answer_rank": 2}
{"question_id": "7de0b2df60d3161dd581ed7915837d460020bc11", "question": "How many tags are included in the ENE tag set?", "references": ["141 ", "200 fine-grained categories", "200"], "best_model_answer": "141 ", "oracle_answer": "200 fine-grained categories", "oracle_answer_rank": 2}
{"question_id": "97abc2e7b39869f660986b91fc68be4ba196805c", "question": "How better does x-vectors perform than knowlege-based features in same-language corpora?", "references": ["For OSA detection x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by 8%.", "For Portuguese PD corpus, x-vector outperform KB for segment and speaker level  for 2.2 and 2.2 F1 respectively.\nFor Portuguese OSA corpus, x-vector outperform KB for segment and speaker level  for 8.5 and 0.1 F1 respectively.", "Portuguese PD Corpus: for segment level i-vectors had better F1 score comparing to KB by 2.1% and for speaker level by 3.5%\nIn case of Spanish PD corpus, KB had higher F1 scores in terms of Segment level and Speaker level by 3.3% and 2.0%.  "], "best_model_answer": "For OSA detection x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by 8%.", "oracle_answer": "For Portuguese PD corpus, x-vector outperform KB for segment and speaker level  for 2.2 and 2.2 F1 respectively.\nFor Portuguese OSA corpus, x-vector outperform KB for segment and speaker level  for 8.5 and 0.1 F1 respectively.", "oracle_answer_rank": 2}
{"question_id": "d9354c0bb32ec037ff2aacfed58d57887a713163", "question": "What languages are used as input?", "references": ["English ", "English", "English"], "best_model_answer": "English ", "oracle_answer": "English ", "oracle_answer_rank": 1}
{"question_id": "8b8adb1d5a1824c8995b3eba668745c44f61c9c6", "question": "Is this analysis performed only on English data?", "references": ["No", "No", "Yes"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "question": "Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?", "references": ["No", "Yes", "No"], "best_model_answer": "No", "oracle_answer": "No", "oracle_answer_rank": 1}
{"question_id": "e14e3e0944ec3290d1985e9a3da82a7df17575cd", "question": "Which dataset do they evaluate on?", "references": [" a large scale Chinese conversation corpus", "Chinese conversation corpus comprised of 20 million context-response pairs", "Chinese dataset containing human-human context response pairs collected from Douban Group "], "best_model_answer": " a large scale Chinese conversation corpus", "oracle_answer": "Chinese conversation corpus comprised of 20 million context-response pairs", "oracle_answer_rank": 2}
{"question_id": "f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "question": "What model architecture do they use for the decoder?", "references": ["a GRU language model", "a GRU language model", "GRU"], "best_model_answer": "a GRU language model", "oracle_answer": "a GRU language model", "oracle_answer_rank": 1}
{"question_id": "74acaa205a5998af4ad7edbed66837a6f2b5c58b", "question": "What are the baselines for this paper?", "references": ["LSTM-Att BIBREF7 , a LSTM model with spatial attention, MemAUG BIBREF33 : a memory-augmented model for VQA, MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling, MLAN BIBREF11 : an advanced multi-level attention model", "Ablated versions of the full model (without external knowledge, without memory network); alternative VQA methods: LSTM-Att, MemAUG, MCB+Att, MLAN", "LSTM with attention, memory augmented model, "], "best_model_answer": "LSTM-Att BIBREF7 , a LSTM model with spatial attention, MemAUG BIBREF33 : a memory-augmented model for VQA, MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling, MLAN BIBREF11 : an advanced multi-level attention model", "oracle_answer": "LSTM-Att BIBREF7 , a LSTM model with spatial attention, MemAUG BIBREF33 : a memory-augmented model for VQA, MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling, MLAN BIBREF11 : an advanced multi-level attention model", "oracle_answer_rank": 1}
