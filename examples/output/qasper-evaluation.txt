Question ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
Gold:
	BIBREF19, BIBREF20
	multilingual NMT (MNMT) BIBREF19
Predicted:
	Since MLM and MLN are based on multi-multi-platform languages, we implement
Answer F1:
	0
Precision:
	0
Recall:
	0

Question ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
Gold:
	pivoting, pivoting$_{\rm m}$
	firstly translates a source language into the pivot language which is later translated to the target language
Predicted:
	What is the difference between high- and low-target translation? For example, we choose
Answer F1:
	0.07692307692307691
Precision:
	0.08333333333333333
Recall:
	0.07142857142857142

Question ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
Gold:
	Europarl, MultiUN
	Europarl BIBREF31, MultiUN BIBREF32
Predicted:
	On multilingual multiplatform data training we are evaluating two different types of MLM. We
Answer F1:
	0
Precision:
	0
Recall:
	0

Question ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
Gold:
	De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru
	French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation
Predicted:
	We use the language trainee scheme with the model BIBREF17 before we test one
Answer F1:
	0
Precision:
	0
Recall:
	0

Question ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
Gold:
	Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer
	Stanford NER, spaCy 2.0, recurrent model with a CRF top layer
Predicted:
	the linear implementation we used is the classical method of programming for the Armenian language and provides a
Answer F1:
	0
Precision:
	0
Recall:
	0

Question ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
Gold:
	ilur.am
	links between Wikipedia articles to generate sequences of named-entity annotated tokens
Predicted:
	when applying this technique for a corpus. It becomes possible to derive the name information by using
Answer F1:
	0.07692307692307691
Precision:
	0.06666666666666667
Recall:
	0.09090909090909091

Question ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
Gold:
	No
	No
Predicted:
	To compare for specific criteria, We used the weighted algorithms in which you do not have to
Answer F1:
	0
Precision:
	0
Recall:
	0

Question ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
Gold:
	politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. 
	training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit
Predicted:
	We show, in the below we quantify these data points, and use their data to predict
Answer F1:
	0.15
Precision:
	0.2
Recall:
	0.12

Question ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
Gold:
	logistic regression models
	logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.
Predicted:
	and for dogmatism, they think you’re dogmatic.The similarity we
Answer F1:
	0.06060606060606061
Precision:
	0.1111111111111111
Recall:
	0.041666666666666664

Question ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
Gold:
	F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data 
	F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)
Predicted:
	is a 2×2-distributed pattern BIB{c}$. Using a function
Answer F1:
	0
Precision:
	0
Recall:
	0

========================================
Evaluation metrics:
{
    "Answer F1": 0.03644522144522144,
    "Answer F1 by type": {
        "extractive": 0.03574203574203574,
        "abstractive": 0.049999999999999996,
        "boolean": 0.0,
        "none": 0.0
    },
    "Missing predictions": 0,
    "Freq of types": {
        "extractive": 6,
        "abstractive": 3,
        "boolean": 1,
        "none": 0
    }
}
