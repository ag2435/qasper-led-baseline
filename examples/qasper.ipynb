{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QASPER evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ag2435/anaconda3/envs/arxiv-agent/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from qasper.dataset_reader import QasperReader\n",
    "from qasper.models import qasper, gpt35, agent_v1\n",
    "from qasper.utils import print_wrap\n",
    "from qasper.evaluator import token_f1_score, get_answers_and_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'qasper'\n",
    "# MODEL_NAME = 'gpt35'\n",
    "# MODEL_NAME = 'agent_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    qasper.qasper_led.cuda()\n",
    "    \n",
    "print(qasper.qasper_led.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate question instances\n",
    "\n",
    "Each article has multiple questions, and each question has multiple references (i.e., answers). We first extract the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "    num_rows: 281\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of years based on id\n",
    "dates = []\n",
    "for article in dataset['validation']:\n",
    "    yy = int( article['id'][:2])\n",
    "    mm = int( article['id'][2:4])\n",
    "    dates.append(datetime.date(year=2000+yy, month=mm, day=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "x=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": [
          "2019-12-01",
          "2018-10-01",
          "2016-09-01",
          "2018-01-01",
          "2018-11-01",
          "2019-09-01",
          "2017-04-01",
          "2019-09-01",
          "2020-03-01",
          "2017-08-01",
          "2020-02-01",
          "2019-09-01",
          "2016-05-01",
          "2018-04-01",
          "2018-09-01",
          "2019-01-01",
          "2016-12-01",
          "2019-12-01",
          "2019-10-01",
          "2016-03-01",
          "2019-11-01",
          "2020-01-01",
          "2020-02-01",
          "2018-08-01",
          "2019-06-01",
          "2018-09-01",
          "2020-04-01",
          "2016-11-01",
          "2019-12-01",
          "2018-09-01",
          "2018-09-01",
          "2016-04-01",
          "2020-02-01",
          "2018-02-01",
          "2018-09-01",
          "2017-10-01",
          "2019-12-01",
          "2019-12-01",
          "2020-02-01",
          "2019-09-01",
          "2018-02-01",
          "2020-04-01",
          "2017-10-01",
          "2016-12-01",
          "2018-09-01",
          "2018-09-01",
          "2019-09-01",
          "2019-11-01",
          "2019-06-01",
          "2018-12-01",
          "2016-11-01",
          "2019-04-01",
          "2019-11-01",
          "2017-06-01",
          "2019-09-01",
          "2019-10-01",
          "2016-09-01",
          "2016-04-01",
          "2016-12-01",
          "2016-08-01",
          "2019-04-01",
          "2019-12-01",
          "2016-03-01",
          "2019-08-01",
          "2017-02-01",
          "2019-04-01",
          "2016-06-01",
          "2018-02-01",
          "2019-09-01",
          "2019-11-01",
          "2017-12-01",
          "2018-05-01",
          "2019-07-01",
          "2020-03-01",
          "2020-01-01",
          "2019-09-01",
          "2017-01-01",
          "2019-04-01",
          "2020-03-01",
          "2019-09-01",
          "2016-04-01",
          "2016-08-01",
          "2020-01-01",
          "2016-06-01",
          "2018-03-01",
          "2019-10-01",
          "2017-01-01",
          "2019-12-01",
          "2017-09-01",
          "2018-06-01",
          "2018-11-01",
          "2018-06-01",
          "2016-10-01",
          "2020-03-01",
          "2018-11-01",
          "2018-05-01",
          "2019-04-01",
          "2019-09-01",
          "2019-10-01",
          "2019-09-01",
          "2018-01-01",
          "2019-07-01",
          "2020-02-01",
          "2017-10-01",
          "2018-04-01",
          "2019-11-01",
          "2018-12-01",
          "2019-11-01",
          "2018-11-01",
          "2019-09-01",
          "2017-01-01",
          "2018-06-01",
          "2019-08-01",
          "2019-09-01",
          "2020-02-01",
          "2016-11-01",
          "2019-08-01",
          "2017-07-01",
          "2018-06-01",
          "2019-04-01",
          "2019-12-01",
          "2019-12-01",
          "2017-03-01",
          "2020-03-01",
          "2019-10-01",
          "2017-06-01",
          "2019-02-01",
          "2016-04-01",
          "2019-05-01",
          "2018-10-01",
          "2017-07-01",
          "2018-10-01",
          "2020-04-01",
          "2019-09-01",
          "2017-06-01",
          "2019-08-01",
          "2019-08-01",
          "2016-12-01",
          "2019-07-01",
          "2019-11-01",
          "2015-03-01",
          "2018-04-01",
          "2016-11-01",
          "2019-10-01",
          "2019-09-01",
          "2019-10-01",
          "2020-02-01",
          "2019-09-01",
          "2019-10-01",
          "2019-06-01",
          "2019-03-01",
          "2017-12-01",
          "2018-09-01",
          "2018-06-01",
          "2019-03-01",
          "2018-06-01",
          "2019-10-01",
          "2018-08-01",
          "2019-10-01",
          "2017-02-01",
          "2018-05-01",
          "2018-07-01",
          "2017-06-01",
          "2019-06-01",
          "2020-02-01",
          "2019-08-01",
          "2018-11-01",
          "2019-09-01",
          "2018-04-01",
          "2019-05-01",
          "2016-03-01",
          "2019-08-01",
          "2016-10-01",
          "2016-01-01",
          "2019-12-01",
          "2018-10-01",
          "2016-10-01",
          "2020-03-01",
          "2016-10-01",
          "2019-09-01",
          "2020-01-01",
          "2017-08-01",
          "2017-05-01",
          "2019-08-01",
          "2020-01-01",
          "2019-12-01",
          "2019-03-01",
          "2017-03-01",
          "2019-09-01",
          "2019-07-01",
          "2017-03-01",
          "2018-10-01",
          "2017-07-01",
          "2019-03-01",
          "2019-12-01",
          "2020-03-01",
          "2019-09-01",
          "2019-09-01",
          "2020-01-01",
          "2018-01-01",
          "2018-09-01",
          "2020-03-01",
          "2019-08-01",
          "2016-11-01",
          "2016-04-01",
          "2018-02-01",
          "2020-01-01",
          "2020-02-01",
          "2016-05-01",
          "2020-03-01",
          "2019-01-01",
          "2019-11-01",
          "2016-05-01",
          "2019-08-01",
          "2019-01-01",
          "2018-09-01",
          "2020-02-01",
          "2018-03-01",
          "2019-09-01",
          "2019-05-01",
          "2019-06-01",
          "2020-02-01",
          "2018-10-01",
          "2018-10-01",
          "2018-05-01",
          "2017-06-01",
          "2019-09-01",
          "2020-04-01",
          "2019-08-01",
          "2019-06-01",
          "2019-09-01",
          "2017-05-01",
          "2020-03-01",
          "2019-10-01",
          "2020-01-01",
          "2019-10-01",
          "2018-05-01",
          "2020-03-01",
          "2017-04-01",
          "2020-03-01",
          "2016-08-01",
          "2019-06-01",
          "2019-01-01",
          "2018-05-01",
          "2018-06-01",
          "2019-10-01",
          "2019-11-01",
          "2019-08-01",
          "2019-07-01",
          "2016-08-01",
          "2017-05-01",
          "2019-06-01",
          "2017-12-01",
          "2020-01-01",
          "2018-08-01",
          "2019-10-01",
          "2019-09-01",
          "2019-02-01",
          "2018-01-01",
          "2018-01-01",
          "2019-09-01",
          "2016-03-01",
          "2017-03-01",
          "2019-06-01",
          "2019-09-01",
          "2018-05-01",
          "2019-03-01",
          "2017-09-01",
          "2017-06-01",
          "2017-05-01",
          "2019-12-01",
          "2018-08-01",
          "2020-04-01",
          "2019-05-01",
          "2020-02-01",
          "2019-08-01",
          "2017-09-01",
          "2019-09-01",
          "2019-08-01",
          "2017-01-01",
          "2019-07-01"
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.histogram(x=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = QasperReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_generator(split):\n",
    "    for article in split:\n",
    "        for question in reader._article_to_instances(article):\n",
    "            yield question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_questions = list(question_generator(dataset['validation']))\n",
    "# instances = random.sample(instances, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'number of documents': 281,\n",
       "             'number of questions': 1005,\n",
       "             'number of answers': 3015,\n",
       "             'questions with multiple answers': 1005,\n",
       "             'extractive questions': 962,\n",
       "             'extractive questions with multiple spans': 406,\n",
       "             'multiple_evidence_spans_count': 536,\n",
       "             'answers with table or figure as evidence': 212,\n",
       "             'freeform answers': 431,\n",
       "             'yes/no questions': 208,\n",
       "             'answers with no evidence': 212,\n",
       "             'unanswerable questions': 163,\n",
       "             'number of truncated contexts': 15})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader._stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question_with_context', 's_question_with_context', 'paragraph_indices', 'global_attention_mask', 'evidence', 'answer', 'metadata'])\n"
     ]
    }
   ],
   "source": [
    "example_question = val_questions[0]\n",
    "print(example_question.keys())\n",
    "# print('QUESTION WITH CONTEXT:')\n",
    "# print_wrap(instance['s_question_with_context'])\n",
    "# print(example_instance['s_question_with_context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_question)\n",
    "# print_wrap(' '.join(example_instance['question_with_context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: which multilingual approaches do they compare with?\n",
      "ANSWER: BIBREF19, BIBREF20\n"
     ]
    }
   ],
   "source": [
    "print('QUESTION:', example_question['metadata']['question'])\n",
    "print('ANSWER:', example_question['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1912.01214'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question['metadata']['article_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "variable=0<br>value=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "0",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "0",
         "offsetgroup": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "histogram",
         "x": [
          5733,
          5733,
          5731,
          5730,
          4513,
          4516,
          4518,
          5774,
          5773,
          9106,
          9105,
          3078,
          3086,
          3078,
          3700,
          3700,
          7140,
          7139,
          7137,
          7139,
          6328,
          6323,
          6266,
          6262,
          6268,
          6265,
          4365,
          4363,
          4364,
          4364,
          5011,
          5008,
          5009,
          5013,
          5012,
          5023,
          5011,
          2844,
          2845,
          2852,
          2845,
          5833,
          5836,
          5802,
          2088,
          1901,
          1887,
          1887,
          6574,
          6576,
          6576,
          3748,
          3757,
          3306,
          3308,
          2717,
          2714,
          2712,
          2712,
          8783,
          8776,
          8779,
          3293,
          3294,
          3295,
          6143,
          6144,
          6144,
          3295,
          3297,
          3292,
          4405,
          4407,
          4406,
          5519,
          5518,
          5970,
          5971,
          5980,
          5974,
          9547,
          9551,
          9545,
          9546,
          9549,
          3071,
          3068,
          3069,
          3071,
          3071,
          3069,
          3071,
          2525,
          2528,
          2529,
          3443,
          3317,
          3314,
          3315,
          8898,
          8899,
          8897,
          8908,
          8910,
          8900,
          7094,
          7094,
          7092,
          7094,
          5009,
          5008,
          4775,
          4787,
          4774,
          4784,
          4781,
          4752,
          4749,
          4750,
          4749,
          2445,
          2438,
          2440,
          3587,
          3589,
          3586,
          3587,
          5803,
          5796,
          5791,
          5793,
          5793,
          7004,
          7010,
          4490,
          4488,
          4492,
          2360,
          2365,
          2364,
          2356,
          8719,
          5391,
          5393,
          3235,
          3234,
          3236,
          3235,
          5710,
          5716,
          5722,
          5704,
          5219,
          5223,
          5227,
          16384,
          5040,
          5038,
          5038,
          2439,
          2442,
          2457,
          2992,
          2993,
          3329,
          3327,
          6000,
          6000,
          4292,
          4290,
          4293,
          5760,
          5754,
          5759,
          5655,
          5656,
          7139,
          7134,
          6456,
          3187,
          3187,
          3190,
          4163,
          4156,
          6353,
          3565,
          3568,
          3567,
          3566,
          2043,
          2040,
          2044,
          2398,
          2389,
          3818,
          3816,
          11074,
          11074,
          11068,
          5279,
          1920,
          4290,
          4291,
          4287,
          4290,
          4288,
          4289,
          4291,
          2406,
          1725,
          6755,
          6756,
          6747,
          6747,
          1350,
          1352,
          7931,
          7934,
          3908,
          3257,
          3260,
          3258,
          3259,
          3311,
          3313,
          3311,
          3315,
          7236,
          7227,
          3216,
          3212,
          4236,
          4234,
          3354,
          3362,
          3357,
          2981,
          4021,
          4019,
          6001,
          2785,
          2786,
          2784,
          2785,
          5133,
          5138,
          3868,
          3863,
          3861,
          1821,
          1822,
          1823,
          1825,
          6591,
          6584,
          6037,
          3126,
          3126,
          3125,
          1924,
          11591,
          6053,
          6055,
          2829,
          2830,
          1607,
          1606,
          1326,
          1312,
          1316,
          1315,
          5716,
          5717,
          5715,
          1355,
          1354,
          6947,
          5796,
          6755,
          4012,
          4651,
          4648,
          2859,
          4779,
          4777,
          3529,
          6541,
          5796,
          5582,
          4467,
          4469,
          4476,
          4466,
          4469,
          4469,
          4469,
          4472,
          4472,
          4472,
          4482,
          4473,
          3970,
          3963,
          3967,
          3972,
          3965,
          3965,
          3962,
          3966,
          2272,
          2267,
          2264,
          2265,
          2263,
          2262,
          2268,
          6080,
          6079,
          6078,
          6078,
          6076,
          6075,
          6032,
          6035,
          6032,
          6038,
          6034,
          6035,
          3171,
          3170,
          3167,
          3169,
          3170,
          3170,
          3534,
          3536,
          3531,
          3530,
          3535,
          3531,
          3359,
          3344,
          3346,
          3349,
          3347,
          3345,
          6846,
          6845,
          6845,
          6848,
          6847,
          6849,
          2368,
          2356,
          2357,
          2357,
          2358,
          2358,
          10482,
          10479,
          10480,
          10486,
          10488,
          10488,
          3688,
          3688,
          3687,
          3686,
          3690,
          3687,
          2963,
          2966,
          2964,
          2961,
          2961,
          2964,
          2970,
          2974,
          2966,
          2970,
          2965,
          5805,
          5803,
          5805,
          5805,
          5806,
          3078,
          3083,
          3077,
          3079,
          3078,
          3077,
          3076,
          3078,
          3080,
          3081,
          6430,
          6431,
          6434,
          6427,
          6455,
          3311,
          3310,
          3316,
          3313,
          3312,
          4734,
          4730,
          4729,
          4731,
          4731,
          2707,
          2709,
          2707,
          2709,
          2709,
          3225,
          3224,
          3225,
          3228,
          3225,
          8185,
          8189,
          8180,
          8180,
          8188,
          6247,
          6252,
          6252,
          6252,
          6254,
          2887,
          2890,
          2885,
          2887,
          2887,
          3792,
          3808,
          3791,
          3798,
          3794,
          5706,
          5708,
          5702,
          5705,
          5717,
          5376,
          5378,
          5377,
          5377,
          5378,
          3218,
          3222,
          3214,
          3216,
          3226,
          2539,
          2541,
          2539,
          2537,
          2539,
          2542,
          2548,
          2542,
          2546,
          5168,
          5169,
          5169,
          5171,
          2625,
          2632,
          2629,
          2625,
          5654,
          5655,
          5653,
          5654,
          5388,
          5383,
          5383,
          5393,
          3920,
          3916,
          3915,
          3915,
          5922,
          5917,
          5917,
          5918,
          10339,
          10339,
          10340,
          10341,
          6080,
          6084,
          6085,
          6080,
          6774,
          6785,
          6771,
          6770,
          5603,
          5602,
          5601,
          5599,
          4814,
          4821,
          4816,
          4820,
          4136,
          4138,
          4136,
          4133,
          6096,
          6102,
          6095,
          6106,
          3676,
          3675,
          3674,
          3676,
          2708,
          2699,
          2701,
          2700,
          3666,
          3669,
          3668,
          3666,
          5435,
          5433,
          5427,
          5429,
          2961,
          2960,
          2961,
          2972,
          5848,
          5858,
          5846,
          5844,
          5849,
          5845,
          5846,
          5847,
          5540,
          5543,
          5542,
          5547,
          3397,
          3396,
          3397,
          3394,
          8311,
          8315,
          8315,
          8312,
          2588,
          2592,
          2590,
          2596,
          5011,
          5009,
          5013,
          5012,
          6658,
          6662,
          6659,
          6664,
          8070,
          8072,
          8071,
          8069,
          5888,
          5891,
          5890,
          5902,
          4291,
          4288,
          4285,
          4286,
          6038,
          6040,
          6036,
          6040,
          6057,
          6055,
          6056,
          6076,
          6059,
          6063,
          6055,
          5273,
          5277,
          5274,
          5281,
          5283,
          5275,
          5282,
          4648,
          4647,
          4645,
          4646,
          4645,
          4646,
          5362,
          5374,
          5366,
          16384,
          16384,
          16384,
          16384,
          16384,
          16384,
          5123,
          5123,
          5125,
          5123,
          5125,
          5125,
          4048,
          4053,
          4047,
          2352,
          2351,
          2350,
          2206,
          2208,
          2208,
          6689,
          6686,
          6688,
          8231,
          8235,
          8238,
          5167,
          5170,
          5171,
          5173,
          5173,
          5171,
          2398,
          2401,
          2403,
          16384,
          16384,
          16384,
          5427,
          5434,
          5429,
          2649,
          2646,
          2642,
          4696,
          4697,
          4696,
          5791,
          5790,
          5795,
          5788,
          5789,
          5790,
          2001,
          2002,
          2004,
          6642,
          6649,
          6649,
          4812,
          4813,
          4817,
          5693,
          5690,
          5693,
          4395,
          4393,
          4391,
          3775,
          3777,
          3776,
          2829,
          2826,
          2825,
          7396,
          7397,
          7413,
          5952,
          5945,
          5950,
          1679,
          1680,
          1675,
          4004,
          4009,
          4006,
          4003,
          4004,
          4005,
          3382,
          3385,
          3387,
          3383,
          3386,
          3385,
          5598,
          5591,
          5589,
          5697,
          5694,
          5696,
          5700,
          5701,
          5696,
          16384,
          16384,
          16384,
          6748,
          6745,
          6745,
          4425,
          4425,
          4425,
          3747,
          3736,
          3732,
          3982,
          3979,
          3977,
          6320,
          6321,
          6321,
          3600,
          3600,
          3598,
          3600,
          3604,
          3604,
          3604,
          3603,
          3399,
          3390,
          3389,
          3392,
          3392,
          5204,
          5198,
          5205,
          5200,
          5197,
          1591,
          1581,
          1580,
          1578,
          1580,
          5245,
          5247,
          5243,
          5243,
          5247,
          3630,
          3626,
          3621,
          3623,
          3623,
          3164,
          3166,
          3168,
          3163,
          3172,
          5815,
          5813,
          5815,
          5820,
          5813,
          5815,
          5811,
          5812,
          5815,
          4730,
          4733,
          4729,
          4728,
          7209,
          7213,
          7212,
          7211,
          5846,
          5848,
          5847,
          5846,
          2562,
          2558,
          2560,
          2556,
          2558,
          2567,
          4925,
          4932,
          4928,
          4928,
          2476,
          2480,
          5594,
          5588,
          5586,
          5587,
          5585,
          5585,
          6345,
          6347,
          6602,
          6604,
          6605,
          6606,
          6601,
          6601,
          2579,
          2582,
          4335,
          4337,
          4340,
          4337,
          6748,
          6746,
          2977,
          2980,
          9890,
          9891,
          4386,
          4392,
          5310,
          5308,
          5313,
          5314,
          4040,
          4045,
          3517,
          3516,
          2192,
          2189,
          2190,
          2188,
          2188,
          2191,
          7173,
          7171,
          7173,
          7182,
          7232,
          7236,
          3163,
          3163,
          3162,
          3161,
          3165,
          3164,
          16384,
          16384,
          5582,
          5584,
          5580,
          5588,
          5585,
          5583,
          2817,
          2814,
          5069,
          5072,
          2988,
          2988,
          10441,
          10446,
          6473,
          6470,
          8290,
          8288,
          4108,
          4097,
          4095,
          4102,
          3610,
          3616,
          5276,
          5283,
          5270,
          5272,
          5271,
          5272,
          2548,
          2548,
          3370,
          3367,
          3367,
          3366,
          3679,
          3684,
          3690,
          3677,
          3779,
          3782,
          3779,
          3780,
          3890,
          3892,
          3893,
          3896,
          3897,
          3894,
          919,
          924,
          5276,
          5276,
          7310,
          7310,
          7316,
          7308,
          3975,
          3977,
          3974,
          3984,
          6001,
          6003,
          6009,
          5996,
          2727,
          2723,
          2715,
          2713,
          4367,
          4370,
          3091,
          3089,
          3061,
          3058,
          4907,
          4912,
          6586,
          6593,
          3045,
          3043,
          5984,
          5978,
          5980,
          5986,
          6352,
          6359,
          6354,
          6355,
          3437,
          3444,
          6259,
          6252,
          6776,
          6776,
          3057,
          3047,
          4755,
          4759,
          5615,
          5617,
          5619,
          5617,
          5621,
          5614,
          5616,
          5758,
          5756,
          5761,
          5757,
          5757,
          2111,
          2113,
          2118,
          2112,
          2112,
          4682,
          4681,
          4683,
          4684,
          4683,
          4547,
          4551,
          4546,
          4550,
          4549,
          6552,
          6562,
          6553,
          6555,
          6552,
          6433,
          6434,
          6437,
          6430,
          6432,
          4966,
          4965,
          4967,
          4967,
          4964,
          10870,
          10869,
          10871,
          10867,
          10881
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Histogram of question + context token lengths (total=1005)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of full_text lengths\n",
    "question_token_lengths = []\n",
    "for question in val_questions:\n",
    "    question_token_lengths.append(len(question['question_with_context']))\n",
    "    \n",
    "px.histogram(question_token_lengths, title=f'Histogram of question + context token lengths (total={len(question_token_lengths)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GT Distribution\n",
    "\n",
    "Now let's extract the references into a dictionary called `gold_answers_and_evidence` that has `question_id`s as keys and a list of answers/references as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_answers_and_evidence = get_answers_and_evidence(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54\n",
      "f5e6f43454332e0521a778db0b769481e23e7682\n",
      "9a05a5f4351db75da371f7ac12eb0b03607c4b87\n",
      "5eda469a8a77f028d0c5f1acd296111085614537\n",
      "18c5d366b1da8447b5404eab71f4cc658ba12e6f\n",
      "b5e4866f0685299f1d7af267bbcc4afe2aab806f\n",
      "1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590\n",
      "b6ae8e10c6a0d34c834f18f66ab730b670fb528c\n",
      "a87a009c242d57c51fc94fe312af5e02070f898b\n",
      "ef4dba073d24042f24886580ae77add5326f2130\n"
     ]
    }
   ],
   "source": [
    "# list the first 10 question ids\n",
    "for question_id in list(gold_answers_and_evidence.keys())[:10]:\n",
    "    print(question_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'BIBREF19, BIBREF20',\n",
       "  'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "  'type': 'extractive'},\n",
       " {'answer': 'multilingual NMT (MNMT) BIBREF19',\n",
       "  'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "  'type': 'extractive'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_answers_and_evidence['b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequency of types\n",
    "types = Counter()\n",
    "n_refs = defaultdict(list)\n",
    "for question_id, references in gold_answers_and_evidence.items():\n",
    "    ref_types = []\n",
    "    for ref in references:\n",
    "        ref_types.append(ref['type'])\n",
    "        types[ref['type']] += 1\n",
    "        n_refs[ref['type']].append(len(references))\n",
    "\n",
    "    # # check if all refs are the same\n",
    "    # assert len(set(refs)) == 1, f\"question_id={question_id}: {refs}\"\n",
    "    # NOTE: not all refs associated with the question have the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'extractive': 962, 'abstractive': 431, 'boolean': 208, 'none': 163})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "extractive",
         "type": "histogram",
         "x": [
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          1,
          3,
          3,
          3,
          3,
          3,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          3,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          1,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1
         ]
        },
        {
         "name": "abstractive",
         "type": "histogram",
         "x": [
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          3,
          1,
          2,
          1,
          3,
          3,
          3,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          3,
          3,
          3,
          3,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          1,
          1,
          2,
          1,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          1,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          1,
          1,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2
         ]
        },
        {
         "name": "boolean",
         "type": "histogram",
         "x": [
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          3,
          3,
          1,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          2
         ]
        },
        {
         "name": "none",
         "type": "histogram",
         "x": [
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          1,
          1,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          1,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          1,
          3,
          3,
          2,
          2,
          1,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          1,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "barmode": "stack",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "for type_, counts in n_refs.items():\n",
    "    fig.add_trace(go.Histogram(x=counts, name=type_))\n",
    "# stacked bars\n",
    "fig.update_layout(barmode='stack')\n",
    "# fig.update_layout(barmode='overlay')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QASPER LED baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 5733 to 6144 to be a multiple of `config.attention_window`: 1024\n",
      "/Users/ag2435/anaconda3/envs/arxiv-agent/lib/python3.12/site-packages/transformers/generation/utils.py:1132: UserWarning:\n",
      "\n",
      "Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED ANSWER: Our experimenters use a multilingual method called interlatable neural networks (multilingual)\n"
     ]
    }
   ],
   "source": [
    "qasper_answer = qasper.predict(example_question)[0]\n",
    "print('PREDICTED ANSWER:', qasper_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_f1_score(qasper_answer, example_question['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3.5 zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED ANSWER:\n",
      "In the provided technical documentation, the approach of cross-lingual\n",
      "pretraining based transfer for Neural Machine Translation (NMT) is thoroughly\n",
      "explained. The document compares this approach with several existing methods for\n",
      "multilingual translation tasks. Here are some of the key points highlighted in\n",
      "the documentation:\n",
      "\n",
      "\n",
      "1. **Existing Methods Comparison**:\n",
      "\n",
      "   - The document compares the proposed cross-lingual pretraining based transfer\n",
      "approach with pivot-based methods, transfer learning, multilingual NMT, and\n",
      "unsupervised NMT. It discusses the strengths and limitations of each approach in\n",
      "handling zero-resource or low-resource translation scenarios.\n",
      "\n",
      "\n",
      "2. **Approach Description**:\n",
      "\n",
      "   - The approach involves pretraining a universal encoder with source/pivot\n",
      "monolingual or source-pivot bilingual data, training a pivot-target parent\n",
      "model, and then directly translating source sentences into target sentences\n",
      "using the trained model. The document explains the steps involved in the\n",
      "pretraining and transfer phases in detail.\n",
      "\n",
      "\n",
      "3. **Cross-lingual Pretraining Methods**:\n",
      "\n",
      "   - The document introduces two existing methods, Masked Language Modeling\n",
      "(MLM) and Translation Language Modeling (TLM), and a novel pretraining method\n",
      "called BRidge Language Modeling (BRLM). It explains how these methods are used\n",
      "to achieve language invariant representations for zero-shot translation.\n",
      "\n",
      "\n",
      "4. **Experimental Setup and Results**:\n",
      "\n",
      "   - The document details the experimental setup, datasets used (Europarl,\n",
      "MultiUN), evaluation metrics (BLEU), and compares the proposed approach with\n",
      "baselines like pivoting, multilingual NMT, and cross-lingual transfer without\n",
      "pretraining. It presents results showing the superior performance of the\n",
      "proposed approach in zero-shot translation scenarios.\n",
      "\n",
      "\n",
      "5. **Analysis and Insights**:\n",
      "\n",
      "   - The document includes analysis sections that evaluate sentence\n",
      "representations, contextualized word representations, the effect of freezing\n",
      "parameters, and other insights gained from the experiments. It provides\n",
      "visualizations and comparisons to illustrate the effectiveness of the proposed\n",
      "approach.\n",
      "\n",
      "\n",
      "6. **Conclusion and Acknowledgments**:\n",
      "\n",
      "   - The document concludes by summarizing the key findings and contributions of\n",
      "the proposed approach. It also acknowledges the support received for the\n",
      "research work.\n",
      "\n",
      "\n",
      "Overall, the technical documentation provides a comprehensive overview of the\n",
      "proposed cross-lingual pretraining based transfer approach for NMT, highlighting\n",
      "its advantages over existing methods and showcasing its performance in zero-shot\n",
      "translation settings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt35_answer = gpt35.predict(example_question)\n",
    "print(\"PREDICTED ANSWER:\")\n",
    "print_wrap(gpt35_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_f1_score(gpt35_answer, example_question['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1. Initial_Response\n",
      "content='' additional_kwargs={'tool_calls': [{'id':\n",
      "'call_TkZBHyD8ZFa8BEHxkov3oEQE', 'function': {'arguments': '{\"Thought\":\"I need\n",
      "to find the multilingual approaches compared in the paper with arxiv identifier \n",
      "1912.01214.\",\"Actions\":[{\"tool\":\"arxiv_lookup\",\"argument\":{\"arxiv_id\":\"1912.0121\n",
      "4\",\"query\":\"multilingual approaches\"}}]}', 'name': 'Answer'}, 'type':\n",
      "'function'}]} response_metadata={'token_usage': {'completion_tokens': 53,\n",
      "'prompt_tokens': 1153, 'total_tokens': 1206}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None} id='9507ba53-e983-4e2b-8471-9562839f3d4d'\n",
      "\n",
      "---\n",
      "## 2. execute_tools\n",
      "[ToolMessage(content='{\"{\\'arxiv_id\\': \\'1912.01214\\', \\'query\\': \\'multilingual\n",
      "approaches\\'}\": \"In [Sx1] Introduction:\\\\nOur proposed approach significantly\n",
      "improves zero-shot translation performance, consistently surpassing pivoting and\n",
      "multilingual approaches. Meanwhile, the performance on supervised translation\n",
      "direction remains the same level or even better when using our method.\"}',\n",
      "id='b2b44579-b786-4cdd-9edc-2df32451c915',\n",
      "tool_call_id='call_TkZBHyD8ZFa8BEHxkov3oEQE')]\n",
      "\n",
      "---\n",
      "## 3. Initial_Response\n",
      "content='' additional_kwargs={'tool_calls': [{'id':\n",
      "'call_yW5FWf4L30LOYXV7xq69llqe', 'function': {'arguments': '{\"Thought\":\"The\n",
      "paper with arxiv identifier 1912.01214 compares the proposed approach with\n",
      "pivoting and other multilingual\n",
      "approaches.\",\"Actions\":[{\"tool\":\"final_answer\",\"argument\":\"pivoting and other\n",
      "multilingual approaches\"}]}', 'name': 'Answer'}, 'type': 'function'}]}\n",
      "response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens':\n",
      "1295, 'total_tokens': 1342}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None} id='57472159-dee6-40b6-9084-f6542a2046f1'\n",
      "\n",
      "---\n",
      "## 4. execute_tools\n",
      "[ToolMessage(content='{\"pivoting and other multilingual approaches\": \"pivoting\n",
      "and other multilingual approaches\"}', id='cf8edf4b-318b-4ca1-90f7-cc49f2875093',\n",
      "tool_call_id='call_yW5FWf4L30LOYXV7xq69llqe')]\n",
      "\n",
      "---\n",
      "## 5. Initial_Response\n",
      "content='' additional_kwargs={'tool_calls': [{'id':\n",
      "'call_ZWrhCV2lb4ptFjKVRDUqtrfF', 'function': {'arguments': '{\"Thought\":\"The\n",
      "paper with arxiv identifier 1912.01214 compares the proposed approach with\n",
      "pivoting and other multilingual\n",
      "approaches.\",\"Actions\":[{\"tool\":\"final_answer\",\"argument\":\"pivoting and other\n",
      "multilingual approaches\"}]}', 'name': 'Answer'}, 'type': 'function'}]}\n",
      "response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens':\n",
      "1378, 'total_tokens': 1425}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None} id='1b79c0b0-ac7f-4244-afc1-2580e17100de'\n",
      "\n",
      "---\n",
      "## 6. execute_tools\n",
      "[ToolMessage(content='{\"pivoting and other multilingual approaches\": \"pivoting\n",
      "and other multilingual approaches\"}', id='6e491d94-36cd-48d6-b3d8-0a300d9874df',\n",
      "tool_call_id='call_ZWrhCV2lb4ptFjKVRDUqtrfF')]\n",
      "\n",
      "---\n",
      "## 7. Initial_Response\n",
      "content='' additional_kwargs={'tool_calls': [{'id':\n",
      "'call_3cKiZ632IyfhdSnnH0LuLB55', 'function': {'arguments': '{\"Thought\":\"The\n",
      "multilingual approaches compared in the paper with arxiv identifier 1912.01214\n",
      "are pivoting and other multilingual\n",
      "approaches.\",\"Actions\":[{\"tool\":\"final_answer\",\"argument\":\"pivoting and other\n",
      "multilingual approaches\"}]}', 'name': 'Answer'}, 'type': 'function'}]}\n",
      "response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens':\n",
      "1461, 'total_tokens': 1510}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None} id='08f29e62-2368-4b71-bf7a-a83996bcd468'\n",
      "\n",
      "---\n",
      "## 8. __end__\n",
      "[HumanMessage(content='In the paper with arxiv identifier 1912.01214, which\n",
      "multilingual approaches do they compare with?',\n",
      "id='cb1160e2-aa59-4d00-938a-459799561f33'), AIMessage(content='',\n",
      "additional_kwargs={'tool_calls': [{'id': 'call_TkZBHyD8ZFa8BEHxkov3oEQE',\n",
      "'function': {'arguments': '{\"Thought\":\"I need to find the multilingual\n",
      "approaches compared in the paper with arxiv identifier 1912.01214.\",\"Actions\":[{\n",
      "\"tool\":\"arxiv_lookup\",\"argument\":{\"arxiv_id\":\"1912.01214\",\"query\":\"multilingual\n",
      "approaches\"}}]}', 'name': 'Answer'}, 'type': 'function'}]},\n",
      "response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens':\n",
      "1153, 'total_tokens': 1206}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None}, id='9507ba53-e983-4e2b-8471-9562839f3d4d'),\n",
      "ToolMessage(content='{\"{\\'arxiv_id\\': \\'1912.01214\\', \\'query\\': \\'multilingual\n",
      "approaches\\'}\": \"In [Sx1] Introduction:\\\\nOur proposed approach significantly\n",
      "improves zero-shot translation performance, consistently surpassing pivoting and\n",
      "multilingual approaches. Meanwhile, the performance on supervised translation\n",
      "direction remains the same level or even better when using our method.\"}',\n",
      "id='b2b44579-b786-4cdd-9edc-2df32451c915',\n",
      "tool_call_id='call_TkZBHyD8ZFa8BEHxkov3oEQE'), AIMessage(content='',\n",
      "additional_kwargs={'tool_calls': [{'id': 'call_yW5FWf4L30LOYXV7xq69llqe',\n",
      "'function': {'arguments': '{\"Thought\":\"The paper with arxiv identifier\n",
      "1912.01214 compares the proposed approach with pivoting and other multilingual\n",
      "approaches.\",\"Actions\":[{\"tool\":\"final_answer\",\"argument\":\"pivoting and other\n",
      "multilingual approaches\"}]}', 'name': 'Answer'}, 'type': 'function'}]},\n",
      "response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens':\n",
      "1295, 'total_tokens': 1342}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None}, id='57472159-dee6-40b6-9084-f6542a2046f1'),\n",
      "ToolMessage(content='{\"pivoting and other multilingual approaches\": \"pivoting\n",
      "and other multilingual approaches\"}', id='cf8edf4b-318b-4ca1-90f7-cc49f2875093',\n",
      "tool_call_id='call_yW5FWf4L30LOYXV7xq69llqe'), AIMessage(content='',\n",
      "additional_kwargs={'tool_calls': [{'id': 'call_ZWrhCV2lb4ptFjKVRDUqtrfF',\n",
      "'function': {'arguments': '{\"Thought\":\"The paper with arxiv identifier\n",
      "1912.01214 compares the proposed approach with pivoting and other multilingual\n",
      "approaches.\",\"Actions\":[{\"tool\":\"final_answer\",\"argument\":\"pivoting and other\n",
      "multilingual approaches\"}]}', 'name': 'Answer'}, 'type': 'function'}]},\n",
      "response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens':\n",
      "1378, 'total_tokens': 1425}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None}, id='1b79c0b0-ac7f-4244-afc1-2580e17100de'),\n",
      "ToolMessage(content='{\"pivoting and other multilingual approaches\": \"pivoting\n",
      "and other multilingual approaches\"}', id='6e491d94-36cd-48d6-b3d8-0a300d9874df',\n",
      "tool_call_id='call_ZWrhCV2lb4ptFjKVRDUqtrfF'), AIMessage(content='',\n",
      "additional_kwargs={'tool_calls': [{'id': 'call_3cKiZ632IyfhdSnnH0LuLB55',\n",
      "'function': {'arguments': '{\"Thought\":\"The multilingual approaches compared in\n",
      "the paper with arxiv identifier 1912.01214 are pivoting and other multilingual\n",
      "approaches.\",\"Actions\":[{\"tool\":\"final_answer\",\"argument\":\"pivoting and other\n",
      "multilingual approaches\"}]}', 'name': 'Answer'}, 'type': 'function'}]},\n",
      "response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens':\n",
      "1461, 'total_tokens': 1510}, 'model_name': 'gpt-3.5-turbo',\n",
      "'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs':\n",
      "None}, id='08f29e62-2368-4b71-bf7a-a83996bcd468')]\n",
      "\n",
      "---\n",
      "PREDICTED ANSWER: pivoting and other multilingual approaches\n"
     ]
    }
   ],
   "source": [
    "agent_v1_answer = agent_v1.predict(example_question, verbose=1)\n",
    "print(\"PREDICTED ANSWER:\", agent_v1_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factory(model_name):\n",
    "    if model_name == 'qasper':\n",
    "        return qasper.predict\n",
    "    elif model_name == 'gpt35':\n",
    "        return gpt35.predict\n",
    "    elif model_name == 'agent_v1':\n",
    "        return agent_v1.predict\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_func = model_factory(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, predicted, verbose=0, output_file=None):\n",
    "    max_answer_f1s = []\n",
    "    max_evidence_f1s = []\n",
    "    max_answer_f1s_by_type = {\n",
    "        \"extractive\": [],\n",
    "        \"abstractive\": [],\n",
    "        \"boolean\": [],\n",
    "        \"none\": [],\n",
    "    }\n",
    "    num_missing_predictions = 0\n",
    "    for question_id in gold:\n",
    "        if question_id not in predicted:\n",
    "            num_missing_predictions += 1\n",
    "            max_answer_f1s.append(0.0)\n",
    "            max_evidence_f1s.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        answer_f1s_and_types = [\n",
    "            (token_f1_score(predicted[question_id][\"answer\"], reference[\"answer\"]),\n",
    "             reference[\"type\"])\n",
    "            for reference in gold[question_id]\n",
    "        ]\n",
    "        # take the reference with the highest F1 score (if there are multiple references)\n",
    "        (max_answer_f1, precision, recall), answer_type = sorted(answer_f1s_and_types, key=lambda x: x[0][0], reverse=True)[0]\n",
    "        max_answer_f1s.append(max_answer_f1)\n",
    "        max_answer_f1s_by_type[answer_type].append(max_answer_f1)\n",
    "\n",
    "        # evidence_f1s = [\n",
    "        #     paragraph_f1_score(predicted[question_id][\"evidence\"], reference[\"evidence\"])\n",
    "        #     for reference in gold[question_id]\n",
    "        # ]\n",
    "        # max_evidence_f1s.append(max(evidence_f1s))\n",
    "\n",
    "        if verbose:\n",
    "            fprint = partial(print, file=output_file) if output_file else print\n",
    "            fprint(\"Question ID:\", question_id)\n",
    "            fprint(\"Question:\", predicted[question_id][\"question\"])\n",
    "            fprint(\"Gold:\")\n",
    "            for reference in gold[question_id]:\n",
    "                fprint('\\t' + reference['answer'])\n",
    "            fprint(\"Predicted:\")\n",
    "            fprint('\\t' + predicted[question_id]['answer'])\n",
    "            fprint(\"Answer F1:\")\n",
    "            fprint('\\t' + str(max_answer_f1))\n",
    "            fprint(\"Precision:\")\n",
    "            fprint('\\t' + str(precision))\n",
    "            fprint(\"Recall:\")\n",
    "            fprint('\\t' + str(recall))\n",
    "            fprint(\"\")\n",
    "\n",
    "    mean = lambda x: sum(x) / len(x) if x else 0.0\n",
    "    return {\n",
    "        \"Answer F1\": mean(max_answer_f1s),\n",
    "        \"Answer F1 by type\": {key: mean(value) for key, value in max_answer_f1s_by_type.items()},\n",
    "        # \"Evidence F1\": mean(max_evidence_f1s),\n",
    "        \"Missing predictions\": num_missing_predictions,\n",
    "        # \"max_answer_f1s_by_type\" : max_answer_f1s_by_type,\n",
    "        \"Freq of types\": {key: len(value) for key, value in max_answer_f1s_by_type.items()},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers_and_evidence = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [01:00<00:00,  6.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 100 questions\n",
    "# sample_val_questions = random.sample(val_questions, 100)\n",
    "\n",
    "# for now let's just use the first 10 questions\n",
    "# but later we should turn this off so that we aren't over-indexing on these examples\n",
    "sample_val_questions = val_questions[:10]\n",
    "\n",
    "for question in tqdm(sample_val_questions):\n",
    "    question_id = question[\"metadata\"][\"question_id\"]\n",
    "\n",
    "    if question_id in predicted_answers_and_evidence: # keep this to conserve API requests\n",
    "        continue\n",
    "\n",
    "    out = predict_func(question)\n",
    "    if isinstance(out, list):\n",
    "        pred_answer = out[0]\n",
    "    else:\n",
    "        pred_answer = out\n",
    "\n",
    "    predicted_answers_and_evidence[question_id] = {\n",
    "        \"question\": question[\"metadata\"][\"question\"],\n",
    "        \"answer\": pred_answer,\n",
    "        \"out\" : out,\n",
    "        # \"evidence\": prediction_data[\"predicted_evidence\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54': {'question': 'which multilingual approaches do they compare with?',\n",
       "  'answer': 'The document discusses a novel approach for zero-shot translation in Neural Machine Translation (NMT) by leveraging cross-lingual pre-training. The approach aims to address the challenge of translating between low-resource or zero-resource language pairs without direct parallel data. Here is a breakdown of the key points in the document:\\n\\n1. **Introduction:**\\n   - Highlights the limitations of existing methods like pivoting and transfer learning in zero-shot translation scenarios.\\n   - Introduces the concept of domain shift problem affecting transfer learning in NMT.\\n   - Proposes a transfer approach based on cross-lingual pre-training to improve zero-shot translation performance.\\n\\n2. **Related Work:**\\n   - Discusses pivot-based methods, transfer learning, multilingual NMT, and unsupervised NMT approaches in zero-shot translation.\\n   - Compares the advantages and limitations of various methods in addressing the zero-shot translation challenge.\\n\\n3. **Approach:**\\n   - Describes the cross-lingual pre-training based transfer approach in detail.\\n   - Introduces Masked Language Modeling (MLM), Translation Language Modeling (TLM), and BRidge Language Modeling (BRLM) for pre-training.\\n   - Outlines the transfer protocol involving pre-training a universal encoder and training pivot$\\\\rightarrow $target NMT model for zero-shot translation.\\n\\n4. **Experiments:**\\n   - Provides details on the experimental setup using Europarl and MultiUN datasets for evaluation.\\n   - Compares the proposed approach with baselines including pivoting, multilingual NMT, and cross-lingual transfer without pretraining.\\n   - Reports results showing the superior performance of the proposed approach in zero-shot translation scenarios.\\n\\n5. **Analysis:**\\n   - Evaluates sentence representations, contextualized word representations, and the effect of freezing parameters in transfer learning.\\n   - Discusses the impact of freezing different layers on the performance of the transfer model.\\n\\n6. **Conclusion:**\\n   - Summarizes the contributions of the research in addressing the zero-shot translation challenge through cross-lingual pre-training.\\n   - Acknowledges the support received for the research work.\\n\\nThe document provides a comprehensive overview of the proposed approach, its experimental validation, and the implications of using cross-lingual pre-training for enhancing zero-shot translation capabilities in NMT.',\n",
       "  'out': 'The document discusses a novel approach for zero-shot translation in Neural Machine Translation (NMT) by leveraging cross-lingual pre-training. The approach aims to address the challenge of translating between low-resource or zero-resource language pairs without direct parallel data. Here is a breakdown of the key points in the document:\\n\\n1. **Introduction:**\\n   - Highlights the limitations of existing methods like pivoting and transfer learning in zero-shot translation scenarios.\\n   - Introduces the concept of domain shift problem affecting transfer learning in NMT.\\n   - Proposes a transfer approach based on cross-lingual pre-training to improve zero-shot translation performance.\\n\\n2. **Related Work:**\\n   - Discusses pivot-based methods, transfer learning, multilingual NMT, and unsupervised NMT approaches in zero-shot translation.\\n   - Compares the advantages and limitations of various methods in addressing the zero-shot translation challenge.\\n\\n3. **Approach:**\\n   - Describes the cross-lingual pre-training based transfer approach in detail.\\n   - Introduces Masked Language Modeling (MLM), Translation Language Modeling (TLM), and BRidge Language Modeling (BRLM) for pre-training.\\n   - Outlines the transfer protocol involving pre-training a universal encoder and training pivot$\\\\rightarrow $target NMT model for zero-shot translation.\\n\\n4. **Experiments:**\\n   - Provides details on the experimental setup using Europarl and MultiUN datasets for evaluation.\\n   - Compares the proposed approach with baselines including pivoting, multilingual NMT, and cross-lingual transfer without pretraining.\\n   - Reports results showing the superior performance of the proposed approach in zero-shot translation scenarios.\\n\\n5. **Analysis:**\\n   - Evaluates sentence representations, contextualized word representations, and the effect of freezing parameters in transfer learning.\\n   - Discusses the impact of freezing different layers on the performance of the transfer model.\\n\\n6. **Conclusion:**\\n   - Summarizes the contributions of the research in addressing the zero-shot translation challenge through cross-lingual pre-training.\\n   - Acknowledges the support received for the research work.\\n\\nThe document provides a comprehensive overview of the proposed approach, its experimental validation, and the implications of using cross-lingual pre-training for enhancing zero-shot translation capabilities in NMT.'},\n",
       " 'f5e6f43454332e0521a778db0b769481e23e7682': {'question': 'what are the pivot-based baselines?',\n",
       "  'answer': 'The pivot-based baselines in the context of Neural Machine Translation (NMT) refer to the common strategy of using a pivot language to facilitate translation between two low-resource languages that lack direct parallel data. In this approach, the source language is first translated into the pivot language and then further translated into the target language. This method aims to overcome the challenges of poor performance in low-resource language pairs by leveraging the availability of rich-resource languages.\\n\\nSome common pivot-based methods include:\\n\\n1. Pivoting: This method involves translating the source language to the pivot language and then translating the pivot language to the target language. It is a sequential translation process that relies on the pivot language as an intermediary step.\\n\\n2. Pivot-synthetic: This approach involves training a source$\\\\rightarrow $target model using pseudo data generated from source-pivot or pivot-target parallel data. It aims to improve translation performance by utilizing synthetic data for training.\\n\\nWhile pivot-based methods can achieve reasonable performance, they often face challenges such as increased computational complexity, parameter scalability with the number of source languages, and error propagation issues resulting from multiple translation steps.\\n\\nIn the realm of NMT, pivot-based methods have been a popular strategy for handling translation tasks involving low-resource language pairs. However, they may not always be the most efficient or effective solution, especially in scenarios where direct parallel data between the source and target languages is not available. Other approaches such as transfer learning, multilingual NMT, and unsupervised NMT have also been explored to address the challenges of translation between low-resource or zero-resource language pairs.',\n",
       "  'out': 'The pivot-based baselines in the context of Neural Machine Translation (NMT) refer to the common strategy of using a pivot language to facilitate translation between two low-resource languages that lack direct parallel data. In this approach, the source language is first translated into the pivot language and then further translated into the target language. This method aims to overcome the challenges of poor performance in low-resource language pairs by leveraging the availability of rich-resource languages.\\n\\nSome common pivot-based methods include:\\n\\n1. Pivoting: This method involves translating the source language to the pivot language and then translating the pivot language to the target language. It is a sequential translation process that relies on the pivot language as an intermediary step.\\n\\n2. Pivot-synthetic: This approach involves training a source$\\\\rightarrow $target model using pseudo data generated from source-pivot or pivot-target parallel data. It aims to improve translation performance by utilizing synthetic data for training.\\n\\nWhile pivot-based methods can achieve reasonable performance, they often face challenges such as increased computational complexity, parameter scalability with the number of source languages, and error propagation issues resulting from multiple translation steps.\\n\\nIn the realm of NMT, pivot-based methods have been a popular strategy for handling translation tasks involving low-resource language pairs. However, they may not always be the most efficient or effective solution, especially in scenarios where direct parallel data between the source and target languages is not available. Other approaches such as transfer learning, multilingual NMT, and unsupervised NMT have also been explored to address the challenges of translation between low-resource or zero-resource language pairs.'},\n",
       " '9a05a5f4351db75da371f7ac12eb0b03607c4b87': {'question': 'which datasets did they experiment with?',\n",
       "  'answer': 'The experiments conducted in the research paper focused on evaluating the proposed cross-lingual pretraining based transfer approach for zero-shot translation tasks. The datasets used in the experiments were Europarl and MultiUN.\\n\\n1. **Europarl Dataset:**\\n   - The Europarl corpus was utilized for evaluating the zero-shot translation performance in language pairs such as French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr), and Romanian-English-German (Ro-En-De).\\n   - The validation set used was devtest2006, and the test set was test2006 for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For Ro$\\\\rightarrow $De, 1,000 overlapping sentences from newstest2016 were used as the test set.\\n   - The vocabulary employed was 60K sub-word tokens based on Byte Pair Encoding (BPE).\\n\\n2. **MultiUN Dataset:**\\n   - The MultiUN corpus consisted of English (En) as the pivot language with parallel data available for Arabic (Ar), Spanish (Es), and Russian (Ru).\\n   - Six zero-shot translation directions were evaluated, including En $\\\\rightarrow $ Ar, En $\\\\rightarrow $ Es, En $\\\\rightarrow $ Ru, Ar $\\\\rightarrow $ En, Es $\\\\rightarrow $ En, and Ru $\\\\rightarrow $ En.\\n   - The vocabulary used was 80K BPE splits.\\n\\n### Experimental Details:\\n- **Model Architecture:** Transformer-big model with specific configurations (e.g., embedding/hidden units, feed-forward filter size, layers, heads per layer) was used for all translation models.\\n- **Training Setup:** Adam optimizer was employed with specific learning rate, warm-up steps, dropout rate, and batch size.\\n- **Pre-training Methods:** MLM, TLM, and BRLM were pre-trained on monolingual and bilingual data to build a cross-lingual encoder shared by source and pivot languages.\\n- **Evaluation Metric:** BLEU score was used as the automatic metric for translation evaluation.\\n\\n### Results:\\n- The proposed approaches consistently outperformed baselines such as pivoting, multilingual NMT, and cross-lingual transfer without pretraining across languages and datasets.\\n- The best approach, MLM+BRLM-SA, showed significant improvement in zero-shot translation, surpassing pivoting and achieving better results than traditional transfer methods.\\n\\n### Conclusion:\\nThe research demonstrated the effectiveness of the cross-lingual pretraining based transfer approach for zero-shot translation, showcasing improvements in translation performance and highlighting the importance of language invariant representations for successful transfer learning.',\n",
       "  'out': 'The experiments conducted in the research paper focused on evaluating the proposed cross-lingual pretraining based transfer approach for zero-shot translation tasks. The datasets used in the experiments were Europarl and MultiUN.\\n\\n1. **Europarl Dataset:**\\n   - The Europarl corpus was utilized for evaluating the zero-shot translation performance in language pairs such as French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr), and Romanian-English-German (Ro-En-De).\\n   - The validation set used was devtest2006, and the test set was test2006 for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For Ro$\\\\rightarrow $De, 1,000 overlapping sentences from newstest2016 were used as the test set.\\n   - The vocabulary employed was 60K sub-word tokens based on Byte Pair Encoding (BPE).\\n\\n2. **MultiUN Dataset:**\\n   - The MultiUN corpus consisted of English (En) as the pivot language with parallel data available for Arabic (Ar), Spanish (Es), and Russian (Ru).\\n   - Six zero-shot translation directions were evaluated, including En $\\\\rightarrow $ Ar, En $\\\\rightarrow $ Es, En $\\\\rightarrow $ Ru, Ar $\\\\rightarrow $ En, Es $\\\\rightarrow $ En, and Ru $\\\\rightarrow $ En.\\n   - The vocabulary used was 80K BPE splits.\\n\\n### Experimental Details:\\n- **Model Architecture:** Transformer-big model with specific configurations (e.g., embedding/hidden units, feed-forward filter size, layers, heads per layer) was used for all translation models.\\n- **Training Setup:** Adam optimizer was employed with specific learning rate, warm-up steps, dropout rate, and batch size.\\n- **Pre-training Methods:** MLM, TLM, and BRLM were pre-trained on monolingual and bilingual data to build a cross-lingual encoder shared by source and pivot languages.\\n- **Evaluation Metric:** BLEU score was used as the automatic metric for translation evaluation.\\n\\n### Results:\\n- The proposed approaches consistently outperformed baselines such as pivoting, multilingual NMT, and cross-lingual transfer without pretraining across languages and datasets.\\n- The best approach, MLM+BRLM-SA, showed significant improvement in zero-shot translation, surpassing pivoting and achieving better results than traditional transfer methods.\\n\\n### Conclusion:\\nThe research demonstrated the effectiveness of the cross-lingual pretraining based transfer approach for zero-shot translation, showcasing improvements in translation performance and highlighting the importance of language invariant representations for successful transfer learning.'},\n",
       " '5eda469a8a77f028d0c5f1acd296111085614537': {'question': 'what language pairs are explored?',\n",
       "  'answer': \"The document discusses the exploration of language pairs in the context of Neural Machine Translation (NMT) research. Key points covered include:\\n\\n1. **Challenge of Low-Resource Languages**: NMT struggles with low-resource or zero-resource language pairs due to the reliance on large-scale parallel data.\\n2. **Pivot-Based Translation**: Translation between low-resource languages often involves pivoting through a rich-resource language, such as translating source language to English and then to the target language.\\n3. **Transfer Learning**: Transfer learning is proposed as an alternative to pivoting, leveraging a high-resource pivottarget model to initialize a low-resource sourcetarget model.\\n4. **Domain Shift Problem**: The domain shift problem affects zero-shot translation in transfer learning due to discrepancies in feature distributions between languages.\\n5. **Proposed Approach**: The document introduces a transfer approach using cross-lingual pre-training to address zero-shot translation challenges.\\n6. **Pre-training Methods**: Detailed discussion on Masked Language Modeling (MLM), Translation Language Modeling (TLM), and BRidge Language Modeling (BRLM) for building a shared encoder for different languages.\\n7. **Experimental Results**: Evaluation of the proposed cross-lingual pre-training approach against strong baselines on Europarl and MultiUN datasets, showcasing improved zero-shot translation performance.\\n8. **Analysis**: Evaluation of sentence representations, contextualized word representations, and the effect of freezing parameters on transfer learning performance.\\n9. **Conclusion**: Summary of the proposed approach's effectiveness in achieving language invariant representations for improved zero-shot translation.\\n\\nThe document provides a comprehensive overview of the exploration of language pairs in NMT research, focusing on addressing challenges in translating low-resource or zero-resource languages.\",\n",
       "  'out': \"The document discusses the exploration of language pairs in the context of Neural Machine Translation (NMT) research. Key points covered include:\\n\\n1. **Challenge of Low-Resource Languages**: NMT struggles with low-resource or zero-resource language pairs due to the reliance on large-scale parallel data.\\n2. **Pivot-Based Translation**: Translation between low-resource languages often involves pivoting through a rich-resource language, such as translating source language to English and then to the target language.\\n3. **Transfer Learning**: Transfer learning is proposed as an alternative to pivoting, leveraging a high-resource pivottarget model to initialize a low-resource sourcetarget model.\\n4. **Domain Shift Problem**: The domain shift problem affects zero-shot translation in transfer learning due to discrepancies in feature distributions between languages.\\n5. **Proposed Approach**: The document introduces a transfer approach using cross-lingual pre-training to address zero-shot translation challenges.\\n6. **Pre-training Methods**: Detailed discussion on Masked Language Modeling (MLM), Translation Language Modeling (TLM), and BRidge Language Modeling (BRLM) for building a shared encoder for different languages.\\n7. **Experimental Results**: Evaluation of the proposed cross-lingual pre-training approach against strong baselines on Europarl and MultiUN datasets, showcasing improved zero-shot translation performance.\\n8. **Analysis**: Evaluation of sentence representations, contextualized word representations, and the effect of freezing parameters on transfer learning performance.\\n9. **Conclusion**: Summary of the proposed approach's effectiveness in achieving language invariant representations for improved zero-shot translation.\\n\\nThe document provides a comprehensive overview of the exploration of language pairs in NMT research, focusing on addressing challenges in translating low-resource or zero-resource languages.\"},\n",
       " '18c5d366b1da8447b5404eab71f4cc658ba12e6f': {'question': 'what ner models were evaluated?',\n",
       "  'answer': 'The following named entity recognition models were evaluated in the study:\\n\\n1. Stanford NER: Stanford NER is a conditional random fields (CRF) classifier that uses lexical and contextual features such as the current word, character-level n-grams, previous and next words, word shape, and sequence features.\\n\\n2. spaCy 2.0: spaCy 2.0 utilizes a CNN-based transition system for named entity recognition. It calculates Bloom embeddings for each token based on its lowercase form, prefix, suffix, and shape, and extracts contextual representations using residual CNNs.\\n\\n3. Recurrent model with CRF top layer: This model employs bidirectional LSTM cells for character-based feature extraction and CRF for sequence labeling. It uses a rule-based classifier to classify articles based on their Wikidata instance of and subclass of attributes.\\n\\nThese models were trained and evaluated using the generated annotated data and manually annotated gold dataset for the Armenian language. The experiments aimed to compare the performance of these models in recognizing named entities in Armenian texts. The results showed that deep learning models outperformed the feature-based Stanford recognizer in recall, with the recurrent model achieving the highest F1 score.\\n\\nThe study also highlighted the importance of the gold-standard test corpus for evaluating future named entity recognition models in Armenian and provided insights into potential areas for future work, such as further enriching the benchmark test set with additional annotation classes and exploring more efficient methods of utilizing Wikipedia for data generation.',\n",
       "  'out': 'The following named entity recognition models were evaluated in the study:\\n\\n1. Stanford NER: Stanford NER is a conditional random fields (CRF) classifier that uses lexical and contextual features such as the current word, character-level n-grams, previous and next words, word shape, and sequence features.\\n\\n2. spaCy 2.0: spaCy 2.0 utilizes a CNN-based transition system for named entity recognition. It calculates Bloom embeddings for each token based on its lowercase form, prefix, suffix, and shape, and extracts contextual representations using residual CNNs.\\n\\n3. Recurrent model with CRF top layer: This model employs bidirectional LSTM cells for character-based feature extraction and CRF for sequence labeling. It uses a rule-based classifier to classify articles based on their Wikidata instance of and subclass of attributes.\\n\\nThese models were trained and evaluated using the generated annotated data and manually annotated gold dataset for the Armenian language. The experiments aimed to compare the performance of these models in recognizing named entities in Armenian texts. The results showed that deep learning models outperformed the feature-based Stanford recognizer in recall, with the recurrent model achieving the highest F1 score.\\n\\nThe study also highlighted the importance of the gold-standard test corpus for evaluating future named entity recognition models in Armenian and provided insights into potential areas for future work, such as further enriching the benchmark test set with additional annotation classes and exploring more efficient methods of utilizing Wikipedia for data generation.'},\n",
       " 'b5e4866f0685299f1d7af267bbcc4afe2aab806f': {'question': 'what is the source of the news sentences?',\n",
       "  'answer': 'The source of the news sentences is a technical documentation on Named Entity Recognition for the Armenian language. The documentation discusses the challenges of creating annotated corpora for low-resource languages and proposes methods for automatically generating training data for named entity recognition models. It also outlines the process of dataset extraction from Wikipedia, the development of gold-standard test corpus, and the creation of GloVe word embeddings for training deep learning models. The documentation includes details on experiments conducted to compare the performance of popular named entity recognition algorithms on the generated data and the manually annotated test dataset. It also highlights future work in the field of named entity recognition for the Armenian language.',\n",
       "  'out': 'The source of the news sentences is a technical documentation on Named Entity Recognition for the Armenian language. The documentation discusses the challenges of creating annotated corpora for low-resource languages and proposes methods for automatically generating training data for named entity recognition models. It also outlines the process of dataset extraction from Wikipedia, the development of gold-standard test corpus, and the creation of GloVe word embeddings for training deep learning models. The documentation includes details on experiments conducted to compare the performance of popular named entity recognition algorithms on the generated data and the manually annotated test dataset. It also highlights future work in the field of named entity recognition for the Armenian language.'},\n",
       " '1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590': {'question': 'did they use a crowdsourcing platform for manual annotations?',\n",
       "  'answer': \"Yes, a crowdsourcing platform was not used for manual annotations in this specific project. Instead, an automated approach based on Wikipedia data was employed to generate annotated data for training a named entity recognizer for the Armenian language. The project used an algorithm to extract data from Wikipedia articles and generate sequences of named-entity annotated tokens. The classification algorithm utilized an article's Wikidata entry to determine the corresponding named entity type. The generated data was then manually annotated to create a gold-standard test corpus for evaluating named entity recognition models.\\n\\nWhile crowdsourcing platforms are commonly used for manual annotations in various projects, this particular project focused on automated data generation and annotation from Wikipedia sources, which provided a cost-effective alternative to manual annotation for the Armenian language.\",\n",
       "  'out': \"Yes, a crowdsourcing platform was not used for manual annotations in this specific project. Instead, an automated approach based on Wikipedia data was employed to generate annotated data for training a named entity recognizer for the Armenian language. The project used an algorithm to extract data from Wikipedia articles and generate sequences of named-entity annotated tokens. The classification algorithm utilized an article's Wikidata entry to determine the corresponding named entity type. The generated data was then manually annotated to create a gold-standard test corpus for evaluating named entity recognition models.\\n\\nWhile crowdsourcing platforms are commonly used for manual annotations in various projects, this particular project focused on automated data generation and annotation from Wikipedia sources, which provided a cost-effective alternative to manual annotation for the Armenian language.\"},\n",
       " 'b6ae8e10c6a0d34c834f18f66ab730b670fb528c': {'question': 'what are the topics pulled from Reddit?',\n",
       "  'answer': 'Based on the text provided, the following topics can be pulled from Reddit:\\n\\n1. Introduction\\n2. Dogmatism describes the tendency to lay down opinions as incontrovertibly true\\n3. Statistical model of dogmatism\\n4. Validation of psychological theories\\n5. Four research questions related to dogmatism\\n6. Data collection from Reddit\\n7. Dogmatism annotations\\n8. Approaches to identifying dogmatism\\n9. Predicting dogmatism\\n10. Dogmatism in the Reddit community\\n11. Subreddits with high and low levels of dogmatism\\n12. Clustering of dogmatic beliefs\\n13. Predictive user behaviors of dogmatism\\n14. Impact of dogmatism on conversations\\n15. Related work in psychology and linguistic analysis\\n16. Conclusion and future implications\\n\\nThese topics cover a range of aspects related to dogmatism analysis on Reddit, including data collection, annotation, feature identification, predictive modeling, community analysis, and behavioral predictors.',\n",
       "  'out': 'Based on the text provided, the following topics can be pulled from Reddit:\\n\\n1. Introduction\\n2. Dogmatism describes the tendency to lay down opinions as incontrovertibly true\\n3. Statistical model of dogmatism\\n4. Validation of psychological theories\\n5. Four research questions related to dogmatism\\n6. Data collection from Reddit\\n7. Dogmatism annotations\\n8. Approaches to identifying dogmatism\\n9. Predicting dogmatism\\n10. Dogmatism in the Reddit community\\n11. Subreddits with high and low levels of dogmatism\\n12. Clustering of dogmatic beliefs\\n13. Predictive user behaviors of dogmatism\\n14. Impact of dogmatism on conversations\\n15. Related work in psychology and linguistic analysis\\n16. Conclusion and future implications\\n\\nThese topics cover a range of aspects related to dogmatism analysis on Reddit, including data collection, annotation, feature identification, predictive modeling, community analysis, and behavioral predictors.'},\n",
       " 'a87a009c242d57c51fc94fe312af5e02070f898b': {'question': 'What predictive model do they build?',\n",
       "  'answer': \"The predictive model discussed in the documentation is aimed at classifying dogmatic posts from Reddit. It is a statistical model of dogmatism that utilizes linguistic signals to predict the level of dogmatism in a post. The model is trained on a corpus of 5,000 Reddit posts annotated with levels of dogmatism. The model's predictions are guided by various feature sets that align with psychological theories of dogmatism.\\n\\nThe model is used to answer several research questions, such as identifying topics that attract the highest levels of dogmatism, understanding how dogmatic beliefs cluster, exploring the influence of dogmatism on conversations in social media, and examining the relationship between user behaviors and dogmatism.\\n\\nTo build the classifier, linguistic features inspired by psychology are utilized, including certainty, tentativeness, insight, perception, comparison, relativity, pronouns, verb tense, sentiment, interrogative language, and negation. These features are analyzed using techniques like odds ratios and statistical tests to assess their predictive power in identifying dogmatic language.\\n\\nThe classification results show that linguistic features contribute significantly to predicting dogmatism, with the model achieving high accuracy within the Reddit dataset and on a separate dataset of New York Times comments. The model combines linguistic features with other behavioral metrics to create a comprehensive understanding of dogmatism in online conversations.\\n\\nOverall, the predictive model serves as a valuable tool for analyzing and understanding dogmatic language in online discussions, shedding light on the behaviors and patterns associated with dogmatism in social media interactions.\",\n",
       "  'out': \"The predictive model discussed in the documentation is aimed at classifying dogmatic posts from Reddit. It is a statistical model of dogmatism that utilizes linguistic signals to predict the level of dogmatism in a post. The model is trained on a corpus of 5,000 Reddit posts annotated with levels of dogmatism. The model's predictions are guided by various feature sets that align with psychological theories of dogmatism.\\n\\nThe model is used to answer several research questions, such as identifying topics that attract the highest levels of dogmatism, understanding how dogmatic beliefs cluster, exploring the influence of dogmatism on conversations in social media, and examining the relationship between user behaviors and dogmatism.\\n\\nTo build the classifier, linguistic features inspired by psychology are utilized, including certainty, tentativeness, insight, perception, comparison, relativity, pronouns, verb tense, sentiment, interrogative language, and negation. These features are analyzed using techniques like odds ratios and statistical tests to assess their predictive power in identifying dogmatic language.\\n\\nThe classification results show that linguistic features contribute significantly to predicting dogmatism, with the model achieving high accuracy within the Reddit dataset and on a separate dataset of New York Times comments. The model combines linguistic features with other behavioral metrics to create a comprehensive understanding of dogmatism in online conversations.\\n\\nOverall, the predictive model serves as a valuable tool for analyzing and understanding dogmatic language in online discussions, shedding light on the behaviors and patterns associated with dogmatism in social media interactions.\"},\n",
       " 'ef4dba073d24042f24886580ae77add5326f2130': {'question': 'What accuracy does the proposed system achieve?',\n",
       "  'answer': \"The proposed system, ALCrowd, achieves significant improvements in Chinese Named Entity Recognition (NER) tasks compared to other systems. Specifically, ALCrowd outperforms strong baseline systems such as CRF and LSTM-CRF, showing an advantage with +1.08 F1 improvement on the DL-PS dataset, +1.24 on EC-MT, and +2.38 on EC-UQ. The results indicate that adding crowd-annotation learning through adversarial training is highly effective in building NER systems.\\n\\nThe system demonstrates its ability to extract worker-independent features, leading to better performance in identifying entities in both dialog and e-commerce domains. The adversarial training approach utilized in ALCrowd provides a mechanism to reduce noise from non-expert annotations and improve the overall accuracy of the NER system.\\n\\nThe impact of pretrained character embeddings on the system's performance is also significant, as models with pretrained embeddings show a marked improvement over those with random embeddings. This highlights the importance of leveraging pretraining techniques to enhance feature representation in NER tasks.\\n\\nOverall, the results of the study suggest that the proposed system, ALCrowd, offers a promising solution for Chinese NER tasks by effectively leveraging crowd annotations and adversarial training to enhance the accuracy and robustness of the NER system.\",\n",
       "  'out': \"The proposed system, ALCrowd, achieves significant improvements in Chinese Named Entity Recognition (NER) tasks compared to other systems. Specifically, ALCrowd outperforms strong baseline systems such as CRF and LSTM-CRF, showing an advantage with +1.08 F1 improvement on the DL-PS dataset, +1.24 on EC-MT, and +2.38 on EC-UQ. The results indicate that adding crowd-annotation learning through adversarial training is highly effective in building NER systems.\\n\\nThe system demonstrates its ability to extract worker-independent features, leading to better performance in identifying entities in both dialog and e-commerce domains. The adversarial training approach utilized in ALCrowd provides a mechanism to reduce noise from non-expert annotations and improve the overall accuracy of the NER system.\\n\\nThe impact of pretrained character embeddings on the system's performance is also significant, as models with pretrained embeddings show a marked improvement over those with random embeddings. This highlights the importance of leveraging pretraining techniques to enhance feature representation in NER tasks.\\n\\nOverall, the results of the study suggest that the proposed system, ALCrowd, offers a promising solution for Chinese NER tasks by effectively leveraging crowd annotations and adversarial training to enhance the accuracy and robustness of the NER system.\"}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers_and_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "save_predictions_path = f'output/{MODEL_NAME}-predictions.json'\n",
    "\n",
    "# if it already exists, append a timestamp\n",
    "if os.path.exists(save_predictions_path):\n",
    "    save_predictions_path = save_predictions_path.replace('.json', f'-{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json')\n",
    "\n",
    "with open(save_predictions_path, 'w') as f:\n",
    "    json.dump(predicted_answers_and_evidence, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save evaluation output (easier to read)\n",
    "save_evaluation_path = f'output/{MODEL_NAME}-evaluation.txt'\n",
    "\n",
    "# if it already exists, append a timestamp\n",
    "if os.path.exists(save_evaluation_path):\n",
    "    save_evaluation_path = save_evaluation_path.replace('.txt', f'-{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.txt')\n",
    "\n",
    "with open(save_evaluation_path, 'w') as f:\n",
    "    \n",
    "    evaluation_output = evaluate(\n",
    "        {k:v for k, v in gold_answers_and_evidence.items() \\\n",
    "            if k in predicted_answers_and_evidence}, \n",
    "        predicted_answers_and_evidence,\n",
    "        verbose=1,\n",
    "        output_file=f,\n",
    "    )\n",
    "\n",
    "    # print evaluation metrics\n",
    "    print('='*40, file=f)\n",
    "    print(\"Evaluation metrics:\", file=f)\n",
    "    print(json.dumps(evaluation_output, indent=4), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_answers_and_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Answer F1': 0.06850330890624352,\n",
       " 'Answer F1 by type': {'extractive': 0.06079980442178291,\n",
       "  'abstractive': 0.1297172290549774,\n",
       "  'boolean': 0.0,\n",
       "  'none': 0.0},\n",
       " 'Missing predictions': 0,\n",
       " 'Freq of types': {'extractive': 7, 'abstractive': 2, 'boolean': 1, 'none': 0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
