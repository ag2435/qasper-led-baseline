{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ag2435/anaconda3/envs/arxiv-agent/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from qasper.dataset_reader import QasperReader\n",
    "from qasper.models import qasper, gpt35\n",
    "from qasper.utils import print_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ag2435/anaconda3/envs/arxiv-agent/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for allenai/qasper contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/allenai/qasper\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 5.95k/5.95k [00:00<00:00, 10.0MB/s]\n",
      "Downloading metadata: 100%|██████████| 8.14k/8.14k [00:00<00:00, 35.1MB/s]\n",
      "Downloading readme: 100%|██████████| 9.64k/9.64k [00:00<00:00, 24.8MB/s]\n",
      "Downloading data: 100%|██████████| 10.8M/10.8M [00:04<00:00, 2.65MB/s]\n",
      "Downloading data: 100%|██████████| 3.87M/3.87M [00:00<00:00, 6.59MB/s]\n",
      "Generating train split: 100%|██████████| 888/888 [00:00<00:00, 2034.19 examples/s]\n",
      "Generating validation split: 100%|██████████| 281/281 [00:00<00:00, 1568.52 examples/s]\n",
      "Generating test split: 100%|██████████| 416/416 [00:00<00:00, 1751.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "    num_rows: 281\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1912.01214',\n",
       " 'title': 'Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation',\n",
       " 'abstract': 'Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.',\n",
       " 'full_text': {'section_name': ['Introduction',\n",
       "   'Related Work',\n",
       "   'Approach',\n",
       "   'Approach ::: Masked and Translation Language Model Pretraining',\n",
       "   'Approach ::: Bridge Language Model Pretraining',\n",
       "   'Approach ::: Transfer Protocol',\n",
       "   'Experiments ::: Setup',\n",
       "   'Experiments ::: Setup ::: Datasets.',\n",
       "   'Experiments ::: Setup ::: Experimental Details.',\n",
       "   'Experiments ::: Main Results',\n",
       "   'Experiments ::: Main Results ::: Results on Europarl Dataset.',\n",
       "   'Experiments ::: Main Results ::: Results on MultiUN Dataset.',\n",
       "   'Experiments ::: Analysis ::: Sentence Representation.',\n",
       "   'Experiments ::: Analysis ::: Contextualized Word Representation.',\n",
       "   'Experiments ::: Analysis ::: The Effect of Freezing Parameters.',\n",
       "   'Conclusion',\n",
       "   'Acknowledgments'],\n",
       "  'paragraphs': [['Although Neural Machine Translation (NMT) has dominated recent research on translation tasks BIBREF0, BIBREF1, BIBREF2, NMT heavily relies on large-scale parallel data, resulting in poor performance on low-resource or zero-resource language pairs BIBREF3. Translation between these low-resource languages (e.g., Arabic$\\\\rightarrow $Spanish) is usually accomplished with pivoting through a rich-resource language (such as English), i.e., Arabic (source) sentence is translated to English (pivot) first which is later translated to Spanish (target) BIBREF4, BIBREF5. However, the pivot-based method requires doubled decoding time and suffers from the propagation of translation errors.',\n",
       "    'One common alternative to avoid pivoting in NMT is transfer learning BIBREF6, BIBREF7, BIBREF8, BIBREF9 which leverages a high-resource pivot$\\\\rightarrow $target model (parent) to initialize a low-resource source$\\\\rightarrow $target model (child) that is further optimized with a small amount of available parallel data. Although this approach has achieved success in some low-resource language pairs, it still performs very poorly in extremely low-resource or zero-resource translation scenario. Specifically, BIBREF8 reports that without any child model training data, the performance of the parent model on the child test set is miserable.',\n",
       "    'In this work, we argue that the language space mismatch problem, also named domain shift problem BIBREF10, brings about the zero-shot translation failure in transfer learning. It is because transfer learning has no explicit training process to guarantee that the source and pivot languages share the same feature distributions, causing that the child model inherited from the parent model fails in such a situation. For instance, as illustrated in the left of Figure FIGREF1, the points of the sentence pair with the same semantics are not overlapping in source space, resulting in that the shared decoder will generate different translations denoted by different points in target space. Actually, transfer learning for NMT can be viewed as a multi-domain problem where each source language forms a new domain. Minimizing the discrepancy between the feature distributions of different source languages, i.e., different domains, will ensure the smooth transition between the parent and child models, as shown in the right of Figure FIGREF1. One way to achieve this goal is the fine-tuning technique, which forces the model to forget the specific knowledge from parent data and learn new features from child data. However, the domain shift problem still exists, and the demand of parallel child data for fine-tuning heavily hinders transfer learning for NMT towards the zero-resource setting.',\n",
       "    'In this paper, we explore the transfer learning in a common zero-shot scenario where there are a lot of source$\\\\leftrightarrow $pivot and pivot$\\\\leftrightarrow $target parallel data but no source$\\\\leftrightarrow $target parallel data. In this scenario, we propose a simple but effective transfer approach, the key idea of which is to relieve the burden of the domain shift problem by means of cross-lingual pre-training. To this end, we firstly investigate the performance of two existing cross-lingual pre-training methods proposed by BIBREF11 in zero-shot translation scenario. Besides, a novel pre-training method called BRidge Language Modeling (BRLM) is designed to make full use of the source$\\\\leftrightarrow $pivot bilingual data to obtain a universal encoder for different languages. Once the universal encoder is constructed, we only need to train the pivot$\\\\rightarrow $target model and then test this model in source$\\\\rightarrow $target direction directly. The main contributions of this paper are as follows:',\n",
       "    'We propose a new transfer learning approach for NMT which uses the cross-lingual language model pre-training to enable a high performance on zero-shot translation.',\n",
       "    'We propose a novel pre-training method called BRLM, which can effectively alleviates the distance between different source language spaces.',\n",
       "    'Our proposed approach significantly improves zero-shot translation performance, consistently surpassing pivoting and multilingual approaches. Meanwhile, the performance on supervised translation direction remains the same level or even better when using our method.'],\n",
       "   ['In recent years, zero-shot translation in NMT has attracted widespread attention in academic research. Existing methods are mainly divided into four categories: pivot-based method, transfer learning, multilingual NMT, and unsupervised NMT.',\n",
       "    'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14. Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter-vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation problem BIBREF15.',\n",
       "    'Transfer Learning is firstly introduced for NMT by BIBREF6, which leverages a high-resource parent model to initialize the low-resource child model. On this basis, BIBREF7 and BIBREF8 use shared vocabularies for source/target language to improve transfer learning, while BIBREF16 relieve the vocabulary mismatch by mainly using cross-lingual word embedding. Although these methods are successful in the low-resource scene, they have limited effects in zero-shot translation.',\n",
       "    'Multilingual NMT (MNMT) enables training a single model that supports translation from multiple source languages into multiple target languages, even those unseen language pairs BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21. Aside from simpler deployment, MNMT benefits from transfer learning where low-resource language pairs are trained together with high-resource ones. However, BIBREF22 point out that MNMT for zero-shot translation easily fails, and is sensitive to the hyper-parameter setting. Also, MNMT usually performs worse than the pivot-based method in zero-shot translation setting BIBREF23.',\n",
       "    'Unsupervised NMT (UNMT) considers a harder setting, in which only large-scale monolingual corpora are available for training. Recently, many methods have been proposed to improve the performance of UNMT, including using denoising auto-encoder, statistic machine translation (SMT) and unsupervised pre-training BIBREF24, BIBREF25, BIBREF26, BIBREF11. Since UNMT performs well between similar languages (e.g., English-German translation), its performance between distant languages is still far from expectation.',\n",
       "    'Our proposed method belongs to the transfer learning, but it is different from traditional transfer methods which train a parent model as starting point. Before training a parent model, our approach fully leverages cross-lingual pre-training methods to make all source languages share the same feature space and thus enables a smooth transition for zero-shot translation.'],\n",
       "   ['In this section, we will present a cross-lingual pre-training based transfer approach. This method is designed for a common zero-shot scenario where there are a lot of source$\\\\leftrightarrow $pivot and pivot$\\\\leftrightarrow $target bilingual data but no source$\\\\leftrightarrow $target parallel data, and the whole training process can be summarized as follows step by step:',\n",
       "    'Pre-train a universal encoder with source/pivot monolingual or source$\\\\leftrightarrow $pivot bilingual data.',\n",
       "    'Train a pivot$\\\\rightarrow $target parent model built on the pre-trained universal encoder with the available parallel data. During the training process, we freeze several layers of the pre-trained universal encoder to avoid the degeneracy issue BIBREF27.',\n",
       "    'Directly translate source sentences into target sentences with the parent model, which benefits from the availability of the universal encoder.',\n",
       "    'The key difficulty of this method is to ensure the intermediate representations of the universal encoder are language invariant. In the rest of this section, we first present two existing methods yet to be explored in zero-shot translation, and then propose a straightforward but effective cross-lingual pre-training method. In the end, we present the whole training and inference protocol for transfer.'],\n",
       "   ['Two existing cross-lingual pre-training methods, Masked Language Modeling (MLM) and Translation Language Modeling (TLM), have shown their effectiveness on XNLI cross-lingual classification task BIBREF11, BIBREF28, but these methods have not been well studied on cross-lingual generation tasks in zero-shot condition. We attempt to take advantage of the cross-lingual ability of the two methods for zero-shot translation.',\n",
       "    'Specifically, MLM adopts the Cloze objective of BERT BIBREF29 and predicts the masked words that are randomly selected and replaced with [MASK] token on monolingual corpus. In practice, MLM takes different language monolingual corpora as input to find features shared across different languages. With this method, word pieces shared in all languages have been mapped into a shared space, which makes the sentence representations across different languages close BIBREF30.',\n",
       "    'Since MLM objective is unsupervised and only requires monolingual data, TLM is designed to leverage parallel data when it is available. Actually, TLM is a simple extension of MLM, with the difference that TLM concatenates sentence pair into a whole sentence, and then randomly masks words in both the source and target sentences. In this way, the model can either attend to surrounding words or to the translation sentence, implicitly encouraging the model to align the source and target language representations. Note that although each sentence pair is formed into one sentence, the positions of the target sentence are reset to count form zero.'],\n",
       "   ['Aside from MLM and TLM, we propose BRidge Language Modeling (BRLM) to further obtain word-level representation alignment between different languages. This method is inspired by the assumption that if the feature spaces of different languages are aligned very well, the masked words in the corrupted sentence can also be guessed by the context of the correspondingly aligned words on the other side. To achieve this goal, BRLM is designed to strengthen the ability to infer words across languages based on alignment information, instead of inferring words within monolingual sentence as in MLM or within the pseudo sentence formed by concatenating sentence pair as in TLM.',\n",
       "    'As illustrated in Figure FIGREF9, BRLM stacks shared encoder over both side sentences separately. In particular, we design two network structures for BRLM, which are divided into Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA) according to the way of generating the alignment information. These two structures actually extend MLM into a bilingual scenario, with the difference that BRLM leverages external aligner tool or additional attention layer to explicitly introduce alignment information during model training.',\n",
       "    'Hard Alignment (BRLM-HA). We first use external aligner tool on source$\\\\leftrightarrow $pivot parallel data to extract the alignment information of sentence pair. During model training, given source$\\\\leftrightarrow $pivot sentence pair, BRLM-HA randomly masks some words in source sentence and leverages alignment information to obtain the aligned words in pivot sentence for masked words. Based on the processed input, BRLM-HA adopts the Transformer BIBREF1 encoder to gain the hidden states for source and pivot sentences respectively. Then the training objective of BRLM-HA is to predict the masked words by not only the surrounding words in source sentence but also the encoder outputs of the aligned words. Note that this training process is also carried out in a symmetric situation, in which we mask some words in pivot sentence and obtain the aligned words in the source sentence.',\n",
       "    'Soft Alignment (BRLM-SA). Instead of using external aligner tool, BRLM-SA introduces an additional attention layer to learn the alignment information together with model training. In this way, BRLM-SA avoids the effect caused by external wrong alignment information and enables many-to-one soft alignment during model training. Similar with BRLM-HA, the training objective of BRLM-SA is to predict the masked words by not only the surrounding words in source sentence but also the outputs of attention layer. In our implementation, the attention layer is a multi-head attention layer adopted in Transformer, where the queries come from the masked source sentence, the keys and values come from the pivot sentence.',\n",
       "    'In principle, MLM and TLM can learn some implicit alignment information during model training. However, the alignment process in MLM is inefficient since the shared word pieces only account for a small proportion of the whole corpus, resulting in the difficulty of expanding the shared information to align the whole corpus. TLM also lacks effort in alignment between the source and target sentences since TLM concatenates the sentence pair into one sequence, making the explicit alignment between the source and target infeasible. BRLM fully utilizes the alignment information to obtain better word-level representation alignment between different languages, which better relieves the burden of the domain shift problem.'],\n",
       "   ['We consider the typical zero-shot translation scenario in which a high resource pivot language has parallel data with both source and target languages, while source and target languages has no parallel data between themselves. Our proposed cross-lingual pretraining based transfer approach for source$\\\\rightarrow $target zero-shot translation is mainly divided into two phrases: the pretraining phase and the transfer phase.',\n",
       "    'In the pretraining phase, we first pretrain MLM on monolingual corpora of both source and pivot languages, and continue to pretrain TLM or the proposed BRLM on the available parallel data between source and pivot languages, in order to build a cross-lingual encoder shared by the source and pivot languages.',\n",
       "    'In the transfer phase, we train pivot$\\\\rightarrow $target NMT model initialized by the cross-lingually pre-trained encoder, and finally transfer the trained NMT model to source$\\\\rightarrow $target translation thanks to the shared encoder. Note that during training pivot$\\\\rightarrow $target NMT model, we freeze several layers of the cross-lingually pre-trained encoder to avoid the degeneracy issue.',\n",
       "    'For the more complicated scenario that either the source side or the target side has multiple languages, the encoder and the decoder are also shared across each side languages for efficient deployment of translation between multiple languages.'],\n",
       "   ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'],\n",
       "   ['The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.',\n",
       "    'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.'],\n",
       "   [\"We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the $\\\\text{attn}\\\\_\\\\text{drop}=0$ (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions BIBREF22. For the model initialization, we use Facebook's cross-lingual pretrained models released by XLM to initialize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with $\\\\text{lr}=0.0001$, $t_{\\\\text{warm}\\\\_\\\\text{up}}=4000$ and $\\\\text{dropout}=0.1$. At decoding time, we generate greedily with length penalty $\\\\alpha =1.0$.\",\n",
       "    'Regarding MLM, TLM and BRLM, as mentioned in the pre-training phase of transfer protocol, we first pre-train MLM on monolingual data of both source and pivot languages, then leverage the parameters of MLM to initialize TLM and the proposed BRLM, which are continued to be optimized with source-pivot bilingual data. In our experiments, we use MLM+TLM, MLM+BRLM to represent this training process. For the masking strategy during training, following BIBREF29, $15\\\\%$ of BPE tokens are selected to be masked. Among the selected tokens, $80\\\\%$ of them are replaced with [MASK] token, $10\\\\%$ are replaced with a random BPE token, and $10\\\\%$ unchanged. The prediction accuracy of masked words is used as a stopping criterion in the pre-training stage. Besides, we use fastalign tool BIBREF34 to extract word alignments for BRLM-HA.'],\n",
       "   ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "   ['Regarding comparison between the baselines in table TABREF19, we find that pivoting is the strongest baseline that has significant advantage over other two baselines. Cross-lingual transfer for languages without shared vocabularies BIBREF16 manifests the worst performance because of not using source$\\\\leftrightarrow $pivot parallel data, which is utilized as beneficial supervised signal for the other two baselines.',\n",
       "    'Our best approach of MLM+BRLM-SA achieves the significant superior performance to all baselines in the zero-shot directions, improving by 0.9-4.8 BLEU points over the strong pivoting. Meanwhile, in the supervised direction of pivot$\\\\rightarrow $target, our approaches performs even better than the original supervised Transformer thanks to the shared encoder trained on both large-scale monolingual data and parallel data between multiple languages.',\n",
       "    'MLM alone that does not use source$\\\\leftrightarrow $pivot parallel data performs much better than the cross-lingual transfer, and achieves comparable results to pivoting. When MLM is combined with TLM or the proposed BRLM, the performance is further improved. MLM+BRLM-SA performs the best, and is better than MLM+BRLM-HA indicating that soft alignment is helpful than hard alignment for the cross-lingual pretraining.'],\n",
       "   ['Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches as shown in Table TABREF26. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs significantly better than MNMT BIBREF19.',\n",
       "    'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.'],\n",
       "   ['We first evaluate the representational invariance across languages for all cross-lingual pre-training methods. Following BIBREF23, we adopt max-pooling operation to collect the sentence representation of each encoder layer for all source-pivot sentence pairs in the Europarl validation sets. Then we calculate the cosine similarity for each sentence pair and average all cosine scores. As shown in Figure FIGREF27, we can observe that, MLM+BRLM-SA has the most stable and similar cross-lingual representations of sentence pairs on all layers, while it achieves the best performance in zero-shot translation. This demonstrates that better cross-lingual representations can benefit for the process of transfer learning. Besides, MLM+BRLM-HA is not as superior as MLM+BRLM-SA and even worse than MLM+TLM on Fr-En, since MLM+BRLM-HA may suffer from the wrong alignment knowledge from an external aligner tool. We also find an interesting phenomenon that as the number of layers increases, the cosine similarity decreases.'],\n",
       "   ['We further sample an English-Russian sentence pair from the MultiUN validation sets and visualize the cosine similarity between hidden states of the top encoder layer to further investigate the difference of all cross-lingual pre-training methods. As shown in Figure FIGREF38, the hidden states generated by MLM+BRLM-SA have higher similarity for two aligned words. It indicates that MLM+BRLM-SA can gain better word-level representation alignment between source and pivot languages, which better relieves the burden of the domain shift problem.'],\n",
       "   ['To freeze parameters is a common strategy to avoid catastrophic forgetting in transfer learning BIBREF27. Table TABREF43 shows the performance of transfer learning with freezing different layers on MultiUN test set, in which En$\\\\rightarrow $Ru denotes the parent model, Ar$\\\\rightarrow $Ru and Es$\\\\rightarrow $Ru are two child models, and all models are based on MLM+BRLM-SA. We can find that updating all parameters during training will cause a notable drop on the zero-shot direction due to the catastrophic forgetting. On the contrary, freezing all the parameters leads to the decline on supervised direction because the language features extracted during pre-training is not sufficient for MT task. Freezing the first four layers of the transformer shows the best performance and keeps the balance between pre-training and fine-tuning.'],\n",
       "   ['In this paper, we propose a cross-lingual pretraining based transfer approach for the challenging zero-shot translation task, in which source and target languages have no parallel data, while they both have parallel data with a high resource pivot language. With the aim of building the language invariant representation between source and pivot languages for smooth transfer of the parent model of pivot$\\\\rightarrow $target direction to the child model of source$\\\\rightarrow $target direction, we introduce one monolingual pretraining method and two bilingual pretraining methods to construct an universal encoder for the source and pivot languages. Experiments on public datasets show that our approaches significantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations.'],\n",
       "   ['We would like to thank the anonymous reviewers for the helpful comments. This work was supported by National Key R&D Program of China (Grant No. 2016YFE0132100), National Natural Science Foundation of China (Grant No. 61525205, 61673289). This work was also partially supported by Alibaba Group through Alibaba Innovative Research Program and the Priority Academic Program Development (PAPD) of Jiangsu Higher Education Institutions.']]},\n",
       " 'qas': {'question': ['which multilingual approaches do they compare with?',\n",
       "   'what are the pivot-based baselines?',\n",
       "   'which datasets did they experiment with?',\n",
       "   'what language pairs are explored?'],\n",
       "  'question_id': ['b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54',\n",
       "   'f5e6f43454332e0521a778db0b769481e23e7682',\n",
       "   '9a05a5f4351db75da371f7ac12eb0b03607c4b87',\n",
       "   '5eda469a8a77f028d0c5f1acd296111085614537'],\n",
       "  'nlp_background': ['', '', '', ''],\n",
       "  'topic_background': ['', '', '', ''],\n",
       "  'paper_read': ['', '', '', ''],\n",
       "  'search_query': ['', '', '', ''],\n",
       "  'question_writer': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4',\n",
       "   'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4',\n",
       "   'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4',\n",
       "   'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4'],\n",
       "  'answers': [{'answer': [{'unanswerable': False,\n",
       "      'extractive_spans': ['BIBREF19', 'BIBREF20'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "      'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. ',\n",
       "       'The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.']},\n",
       "     {'unanswerable': False,\n",
       "      'extractive_spans': ['multilingual NMT (MNMT) BIBREF19'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "      'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.']}],\n",
       "    'annotation_id': ['819898a2daf67225307aaf59d8048987c06b0c03',\n",
       "     'bbc549d0d6a598c0a93d0dde01d8cc6ffe316adc'],\n",
       "    'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "     '258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       "   {'answer': [{'unanswerable': False,\n",
       "      'extractive_spans': ['pivoting', 'pivoting$_{\\\\rm m}$'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.',\n",
       "       'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.'],\n",
       "      'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.',\n",
       "       'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. ']},\n",
       "     {'unanswerable': False,\n",
       "      'extractive_spans': ['firstly translates a source language into the pivot language which is later translated to the target language'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': [\"We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the $\\\\text{attn}\\\\_\\\\text{drop}=0$ (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions BIBREF22. For the model initialization, we use Facebook's cross-lingual pretrained models released by XLM to initialize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with $\\\\text{lr}=0.0001$, $t_{\\\\text{warm}\\\\_\\\\text{up}}=4000$ and $\\\\text{dropout}=0.1$. At decoding time, we generate greedily with length penalty $\\\\alpha =1.0$.\",\n",
       "       'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14. Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter-vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation problem BIBREF15.'],\n",
       "      'highlighted_evidence': ['We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines.',\n",
       "       'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14.']}],\n",
       "    'annotation_id': ['291a918eae943cf62b5d5ad4a9b6b24c4e3090f1',\n",
       "     '8ca8aaa07c7ee1d00c02322d47244d4489f151d1'],\n",
       "    'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "     '258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       "   {'answer': [{'unanswerable': False,\n",
       "      'extractive_spans': ['Europarl', 'MultiUN'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'],\n",
       "      'highlighted_evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.']},\n",
       "     {'unanswerable': False,\n",
       "      'extractive_spans': ['Europarl BIBREF31', 'MultiUN BIBREF32'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'],\n",
       "      'highlighted_evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.']}],\n",
       "    'annotation_id': ['ce1cbd643169ab70cc3b807401798b400472868f',\n",
       "     'f5d30fb867823556668418ecbbafde77fa834f2f'],\n",
       "    'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "     '258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       "   {'answer': [{'unanswerable': False,\n",
       "      'extractive_spans': [],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': 'De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru',\n",
       "      'evidence': ['For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.',\n",
       "       'The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.',\n",
       "       'FLOAT SELECTED: Table 1: Data Statistics.'],\n",
       "      'highlighted_evidence': ['For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. ',\n",
       "       'The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. ',\n",
       "       'FLOAT SELECTED: Table 1: Data Statistics.']},\n",
       "     {'unanswerable': False,\n",
       "      'extractive_spans': ['French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De)',\n",
       "       'Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation'],\n",
       "      'yes_no': None,\n",
       "      'free_form_answer': '',\n",
       "      'evidence': ['The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.',\n",
       "       'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.'],\n",
       "      'highlighted_evidence': ['For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets.',\n",
       "       'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation.']}],\n",
       "    'annotation_id': ['36d46e79bc56de8706a4a7001d5e74fc25ecf15c',\n",
       "     '81063a32b1f4c7450eb6c0fbbcb649870b47e344'],\n",
       "    'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "     '258ee4069f740c400c0049a2580945a1cc7f044c']}]},\n",
       " 'figures_and_tables': {'caption': ['Figure 1: The circle and triangle dots represent source sentences in different language l1 and l2, and the square dots means target sentences in language l3. A sample of translation pairs is connected by the dashed line. We would like to force each of the translation pairs has the same latent representation as the right part of the figure so as to transfer l1 → l3 model directly to l2 → l3 model.',\n",
       "   'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to pairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention layer to encourage word representation alignment across different languages.',\n",
       "   'Table 1: Data Statistics.',\n",
       "   'Table 2: Results on Europarl test sets. Three pivot settings are conducted in our experiments. In each setting, the left column presents the zero-shot performances (source→target), and the right column denotes the performances in the supervised parent model direction (pivot→target).',\n",
       "   'Table 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column “A-ZST\" reports averaged BLEU of zero-shot translation, while the column “A-ST\" reports averaged BLEU of supervised pivot→target direction.',\n",
       "   'Figure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the Europarl validation set.',\n",
       "   'Figure 4: Cosine similarity visualization at word level given an English-Russian sentence pair from the MultiUN validation sets. Brighter indicates higher similarity.',\n",
       "   'Table 4: BLEU score of freezing different layers. The number in Freezing Layers column denotes that the number of encoder layers will not be updated.'],\n",
       "  'file': ['1-Figure1-1.png',\n",
       "   '3-Figure2-1.png',\n",
       "   '4-Table1-1.png',\n",
       "   '5-Table2-1.png',\n",
       "   '6-Table3-1.png',\n",
       "   '6-Figure3-1.png',\n",
       "   '7-Figure4-1.png',\n",
       "   '7-Table4-1.png']}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = QasperReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_generator(split):\n",
    "    for article in split:\n",
    "        for instance in reader._article_to_instances(article):\n",
    "            yield instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 100 instances\n",
    "import random\n",
    "random.seed(42)\n",
    "instances = list(instance_generator(dataset['validation']))\n",
    "instances = random.sample(instances, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'number of documents': 281,\n",
       "             'number of questions': 1005,\n",
       "             'number of answers': 3015,\n",
       "             'questions with multiple answers': 1005,\n",
       "             'extractive questions': 962,\n",
       "             'extractive questions with multiple spans': 406,\n",
       "             'multiple_evidence_spans_count': 536,\n",
       "             'answers with table or figure as evidence': 212,\n",
       "             'freeform answers': 431,\n",
       "             'yes/no questions': 208,\n",
       "             'answers with no evidence': 212,\n",
       "             'unanswerable questions': 163,\n",
       "             'number of truncated contexts': 15})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader._stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question_with_context', 's_question_with_context', 'paragraph_indices', 'global_attention_mask', 'evidence', 'answer', 'metadata'])\n",
      "QUESTION WITH CONTEXT:\n",
      "Did they experiment with this new dataset?\n",
      "Introduction\n",
      "How humans process language has become increasingly relevant in natural language processing since physiological data during language understanding is more accessible and recorded with less effort. In this work, we focus on eye-tracking and electroencephalography (EEG) recordings to capture the reading process. On one hand, eye movement data provides millisecond-accurate records about where humans look when they are reading, and is highly correlated with the cognitive load associated with different stages of text processing. On the other hand, EEG records electrical brain activity across the scalp and is a direct measure of physiological processes, including language processing. The combination of both measurement methods enables us to study the language understanding process in a more natural setting, where participants read full sentences at a time, in their own speed. Eye-tracking then permits us to define exact word boundaries in the timeline of a subject reading a sentence, allowing the extraction of brain activity signals for each word.\n",
      "Human cognitive language processing data is immensely useful for NLP: Not only can it be leveraged to improve NLP applications (e.g. barrett2016weakly for part-of-speech tagging or klerke2016improving for sentence compression), but also to evaluate state-of-the-art machine learning systems. For example, hollenstein2019cognival evaluate word embeddings, or schwartz2019inducing fine-tune language models with brain-relevant bias.\n",
      "Additionally, the availability of labelled data plays a crucial role in all supervised machine learning applications. Physiological data can be used to understand and improve the labelling process (e.g. tokunaga2017eye), and, for instance, to build cost models for active learning scenarios BIBREF0. Is it possible to replace this expensive manual work with models trained on physiological activity data recorded from humans while reading? That is to say, can we find and extract relevant aspects of text understanding and annotation directly from the source, i.e. eye-tracking and brain activity signals during reading?\n",
      "Motivated by these questions and our previously released dataset, ZuCo 1.0 BIBREF1, we developed this new corpus, where we specifically aim to collect recordings during natural reading as well as during annotation.\n",
      "We provide the first dataset of simultaneous eye movement and brain activity recordings to analyze and compare normal reading to task-specific reading during annotation. The Zurich Cognitive Language Processing Corpus (ZuCo) 2.0, including raw and preprocessed eye-tracking and electroencephalography (EEG) data of 18 subjects, as well as the recording and preprocessing scripts, is publicly available at https://osf.io/2urht/. It contains physiological data of each subject reading 739 English sentences from Wikipedia (see example in Figure FIGREF1). We want to highlight the re-use potential of this data. In addition to the psycholinguistic motivation, this corpus is especially tailored for training and evaluating machine learning algorithms for NLP purposes. We conduct a detailed technical validation of the data as proof of the quality of the recordings.\n",
      "Related Work\n",
      "Some eye-tracking corpora of natural reading (e.g. the Dundee BIBREF2, Provo BIBREF3 and GECO corpus BIBREF4), and a few EEG corpora (for example, the UCL corpus BIBREF5) are available. It has been shown that this type of cognitive processing data is useful for improving and evaluating NLP methods (e.g. barrett2018sequence,hollenstein2019cognival, hale2018finding). However, before the Zurich Cognitive Language Processing Corpus (ZuCo 1.0), there was no available data for simultaneous eye-tracking and EEG recordings of natural reading. dimigen2011coregistration studied the linguistic effects of eye movements and EEG co-registration in natural reading and showed that they accurately represent lexical processing. Moreover, the simultaneous recordings are crucial to extract word-level brain activity signals.\n",
      "While the above mentioned studies analyze and leverage natural reading, some NLP work has used eye-tracking during annotation (but, as of yet, not EEG data). mishra2016predicting and joshi2014measuring recorded eye-tracking during binary sentiment annotation (positive/negative). This data was used to determine the annotation complexity of the text passages based on eye movement metrics and for sarcasm detection BIBREF6. Moreover, eye-tracking has been used to analyze the word sense annotation process in Hindi BIBREF7, named entity annotation in Japanese BIBREF8, and to leverage annotator gaze behaviour for coreference resolution BIBREF9. Finally, tomanek2010cognitive used eye-tracking data during entity annotation to build a cost model for active learning. However, until now there is no available data or research that analyzes the differences in the human processing of normal reading versus annotation.\n",
      "Related Work ::: ZuCo1.0\n",
      "In previous work, we recorded a first dataset of simultaneous eye-tracking and EEG during natural reading BIBREF1. ZuCo 1.0 consists of three reading tasks, two of which contain very similar reading material and experiments as presented in the current work. However, the main difference and reason for recording ZuCo 2.0, consists in the experiment procedure. For ZuCo 1.0 the normal reading and task-specific reading paradigms were recorded in different sessions on different days. Therefore, the recorded data is not appropriate as a means of comparison between natural reading and annotation, since the differences in the brain activity data might result mostly from the different sessions due to the sensitivity of EEG. This, and extending the dataset with more sentences and more subjects, were the main factors for recording the current corpus. We purposefully maintained an overlap of some sentences between both datasets to allow additional analyses (details are described in Section SECREF7).\n",
      "Corpus Construction\n",
      "In this section we describe the contents and experimental design of the ZuCo 2.0 corpus.\n",
      "Corpus Construction ::: Participants\n",
      "We recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. Hence, we share the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. Details on subject demographics can be found in Table TABREF4. All participants gave written consent for their participation and the re-use of the data prior to the start of the experiments. The study was approved by the Ethics Commission of the University of Zurich.\n",
      "Corpus Construction ::: Reading materials\n",
      "During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating. This corpus was chosen because it provides annotations of semantic relations. We included seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer. The sentences were chosen in the same length range as ZuCo 1.0, and with similar Flesch reading ease scores. The dataset statistics are shown in Table TABREF2.\n",
      "Of the 739 sentences, the participants read 349 sentences in a normal reading paradigm, and 390 sentences in a task-specific reading paradigm, in which they had to determine whether a certain relation type occurred in the sentence or not. Table TABREF3 shows the distribution of the different relation types in the sentences of the task-specific annotation paradigm.\n",
      "Purposefully, there are 63 duplicates between the normal reading and the task-specific sentences (8% of all sentences). The intention of these duplicate sentences is to provide a set of sentences read twice by all participants with a different task in mind. Hence, this enables the comparison of eye-tracking and brain activity data when reading normally and when annotating specific relations (see examples in Section SECREF4).\n",
      "Furthermore, there is also an overlap in the sentences between ZuCo 1.0 and ZuCo 2.0. 100 normal reading and 85 task-specific sentences recorded for this dataset were already recorded in ZuCo 1.0. This allows for comparisons between the different recording procedures (i.e. session-specific effects) and between more participants (subject-specific effects).\n",
      "Corpus Construction ::: Experimental design\n",
      "As mentioned above, we recorded two different reading tasks for the ZuCo 2.0 dataset. During both tasks the participants were able to read in their own speed, using a control pad to move to the next sentence and to answer the control questions, which allowed for natural reading. Since each subject reads at their own personal pace, the reading speed between varies between subjects. Table TABREF4 shows the average reading speed for each task, i.e. the average number of seconds a subject spends per sentence before switching to the next one.\n",
      "All 739 sentences were recorded in a single session for each participant. The duration of the recording sessions was between 100 and 180 minutes, depending on the time required to set up and calibrate the devices, and the personal reading speed of the participants.\n",
      "We recorded 14 blocks of approx. 50 sentences, alternating between tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences.\n",
      "Corpus Construction ::: Experimental design ::: Normal reading (NR)\n",
      "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions. Figure FIGREF8 (left) shows an example sentence as it was depicted on the screen during recording. As shown in Figure FIGREF8 (middle), the control condition for this task consisted of single-choice questions about the content of the previous sentence. 12% of randomly selected sentences were followed by such a comprehension question with three answer options on a separate screen.\n",
      "Corpus Construction ::: Experimental design ::: Task-specific reading (TSR)\n",
      "In the second task, the participants were instructed to search for a specific relation in each sentence they read. Instead of comprehension questions, the participants had to decide for each sentence whether it contains the relation or not, i.e. they were actively annotating each sentence. Figure FIGREF8 (right) shows an example screen for this task. 17% of the sentences did not include the relation type and were used as control conditions. All sentences within one block involved the same relation type. The blocks started with a practice round, which described the relation and was followed by three sample sentences, so that the participants would be familiar with the respective relation type.\n",
      "Corpus Construction ::: Linguistic assessment\n",
      "As a linguistic assessment, the vocabulary and language proficiency of the participants was tested with the LexTALE test (Lexical Test for Advanced Learners of English, lemhofer2012introducing). This is an unspeeded lexical decision task designed for intermediate to highly proficient language users. The average LexTALE score over all participants was 88.54%. Moreover, we also report the scores the participants achieved with their answers to the reading comprehension control questions and their relation annotations. The detailed scores for all participants are also presented in Table TABREF4.\n",
      "Corpus Construction ::: Data acquisition\n",
      "Data acquisition took place in a sound-attenuated and dark experiment room. Participants were seated at a distance of 68cm from a 24-inch monitor with a resolution of 800x600 pixels. A stable head position was ensured via a chin rest. Participants were instructed to stay as still as possible during the tasks to avoid motor EEG artifacts. Participants were also offered snacks and water during the breaks and were encouraged to rest. All sentences were presented at the same position on the screen and could span multiple lines. The sentences were presented in black on a light grey background with font size 20-point Arial, resulting in a letter height of 0.8 mm. The experiment was programmed in MATLAB 2016b BIBREF10, using PsychToolbox BIBREF11. Participants completed the tasks sitting alone in the room, while two research assistants were monitoring their progress in the adjoining room. All recording scripts including detailed participant instructions are available alongside the data.\n",
      "Corpus Construction ::: Data acquisition ::: Eye-tracking acquisition\n",
      "Eye position and pupil size were recorded with an infrared video-based eye tracker (EyeLink 1000 Plus, SR Research) at a sampling rate of 500 Hz. The eye tracker was calibrated with a 9-point grid at the beginning of the session and re-validated before each block of sentences.\n",
      "Corpus Construction ::: Data acquisition ::: EEG acquisition\n",
      "High-density EEG data were recorded at a sampling rate of 500 Hz with a bandpass of 0.1 to 100 Hz, using a 128-channel EEG Geodesic Hydrocel system (Electrical Geodesics). The recording reference was set at electrode Cz. The head circumference of each participant was measured to select an appropriately sized EEG net. To ensure good contact, the impedance of each electrode was checked prior to recording, and was kept below 40 kOhm. Electrode impedance levels were checked after every third block of 50 sentences (approx. every 30 mins) and reduced if necessary.\n",
      "Corpus Construction ::: Preprocessing and feature extraction ::: Eye-tracking\n",
      "The eye-tracking data consists of (x,y) gaze location entries for all individual fixations (Figure FIGREF1b). Coordinates were given in pixels with respect to the monitor coordinates (the upper left corner of the screen was (0,0) and down/right was positive). We provide this raw data as well as various engineered eye-tracking features. For this feature extraction only fixations within the boundaries of each displayed word were extracted. Data points distinctly not associated with reading (minimum distance of 50 pixels to the text) were excluded. Additionally, fixations shorter than 100 ms were excluded from the analyses, because these are unlikely to reflect fixations relevant for reading BIBREF12. On the basis of the GECO and ZuCo 1.0 corpora, we extracted the following features: (i) gaze duration (GD), the sum of all fixations on the current word in the first-pass reading before the eye moves out of the word; (ii) total reading time (TRT), the sum of all fixation durations on the current word, including regressions; (iii) first fixation duration (FFD), the duration of the first fixation on the prevailing word; (iv) single fixation duration (SFD), the duration of the first and only fixation on the current word; and (v) go-past time (GPT), the sum of all fixations prior to progressing to the right of the current word, including regressions to previous words that originated from the current word. For each of these eye-tracking features we additionally computed the pupil size. Furthermore, we extracted the number of fixations and mean pupil size for each word and sentence.\n",
      "Corpus Construction ::: Preprocessing and feature extraction ::: EEG\n",
      "The EEG data shared in this project are available as raw data, but also preprocessed with Automagic (version 1.4.6, pedroni2019automagic), a tool for automatic EEG data cleaning and validation. 105 EEG channels (i.e. electrodes) were used from the scalp recordings. 9 EOG channels were used for artifact removal and additional 14 channels lying mainly on the neck and face were discarded before data analysis. Bad channels were identified and interpolated. We used the Multiple Artifact Rejection Algorithm (MARA), a supervised machine learning algorithm that evaluates ICA components, for automatic artifact rejection. MARA has been trained on manual component classifications, and thus captures a wide range of artifacts. MARA is especially effective at detecting and removing eye and muscle artifact components. The effect of this preprocessing can be seen in Figure FIGREF1d.\n",
      "After preprocessing, we synchronized the EEG and eye-tracking data to enable EEG analyses time-locked to the onsets of fixations. To compute oscillatory power measures, we band-pass filtered the continuous EEG signals across an entire reading task for five different frequency bands resulting in a time-series for each frequency band. The independent frequency bands were determined as follows: theta$_1$ (4–6 Hz), theta$_2$ (6.5–8 Hz), alpha$_1$ (8.5–10 Hz), alpha$_2$ (10.5–13 Hz), beta$_1$ (13.5–18 Hz), beta$_2$ (18.5–30 Hz), gamma$_1$ (30.5–40 Hz), and gamma$_2$ (40–49.5 Hz). We then applied a Hilbert transformation to each of these time-series. We specifically chose the Hilbert transformation to maintain the temporal information of the amplitude of the frequency bands, to enable the power of the different frequencies for time segments defined through the fixations from the eye-tracking recording. Thus, for each eye-tracking feature we computed the corresponding EEG feature in each frequency band. Furthermore, we extracted sentence-level EEG features by calculating the power in each frequency band, and additionally, the difference of the power spectra between frontal left and right homologue electrodes pairs. For each eye-tracking based EEG feature, all channels were subject to an artifact rejection criterion of $90\\mu V$ to exclude trials with transient noise.\n",
      "Data Validation\n",
      "The aim of the technical validation of the data is to guarantee good recording quality and to replicate findings of previous studies investigating co-registration of EEG and eye movement data during natural reading tasks (e.g. dimigen2011coregistration). We also compare the results to ZuCo 1.0 BIBREF1, which allows a more direct comparison due to the analogous recording procedure.\n",
      "Data Validation ::: Eye-tracking\n",
      "We validated the recorded eye-tracking data by analyzing the fixations made by all subjects through their reading speed and omission rate on sentence level. The omission rate is defined as the percentage of words that is not fixated in a sentence. Figure FIGREF10 (middle) shows the mean reading speed over all subjects, measured in seconds per sentence and Figure FIGREF10 (right) shows the mean omission rates aggregated over all subjects for each task. Clearly, the participants made less fixations during the task-specific reading, which lead to faster reading speed.\n",
      "Moreover, we corroborated these sentence-level metrics by visualizing the skipping proportion on word level (Figure FIGREF13). The skipping proportion is the average rate of words being skipped (i.e. not being fixated) in a sentence. As expected, this also increases in the task-specific reading.\n",
      "Although the reading material is from the same source and of the same length range (see Figure FIGREF10 (left)), in the first task (NR) passive reading was recorded, while in the second task (TSR) the subjects had to annotate a specific relation type in each sentence. Thus, the task-specific annotation reading lead to shorter passes because the goal was merely to recognize a relation in the text, but not necessarily to process the every word in each sentence. This distinct reading behavior is shown in Figure FIGREF15, where fixations occur until the end of the sentence during normal reading, while during task-specific reading the fixations stop after the decisive words to detect a given relation type. Finally, we also analyzed the average reading times for each of the extracted eye-tracking features. The means and distributions for both tasks are shown in Figure FIGREF21. These results are in line with the recorded data in ZuCo 1.0, as well as with the features extracted in the GECO corpus BIBREF4.\n",
      "Data Validation ::: EEG\n",
      "As a first validation step, we extracted fixation-related potentials (FRPs), where the EEG signal during all fixations of one task are averaged. Figure FIGREF24 shows the time-series of the resulting FRPs for two electrodes (PO8 and Cz), as well as topographies of the voltage distributions across the scalp at selected points in time. The five components (for which the scalp topographies are plotted) are highly similar in the time-course of the chosen electrodes to dimigen2011coregistration as well as to ZuCo 1.0.\n",
      "Moreover, these previous studies were able to show an effect of fixation duration on the resulting FRPs. To show this dependency we followed two approaches. First, for each reading task, all single-trial FRPs were ordered by fixation duration and a vertical sliding time-window was used to smooth the data BIBREF13. Figure FIGREF25 (bottom) shows the resulting plots. In line with this previous work, a first positivation can be identified at 100 ms post-fixation onset. A second positive peak is located dependent on the duration of the fixation, which can be explained by the time-jittered succeeding fixation. The second approach is based on henderson2013co in which single trial EEG segments are clustered by the duration of the current fixation. As shown in Figure FIGREF25 (top), we chose four clusters and averaged the data within each cluster to four distinct FRPs, depending on the fixation duration. Again, the same positivation peaks become apparent. Both findings are consistent with the previous work mentioned and with our findings from ZuCo 1.0.\n",
      "Conclusion\n",
      "We presented a new, freely available corpus of eye movement and electrical brain activity recordings during natural reading as well as during annotation. This is the first dataset that allows for the comparison between these two reading paradigms. We described the materials and experiment design in detail and conducted an extensive validation to ensure the quality of the recorded data. Since this corpus is tailored to cognitively-inspired NLP, the applications and re-use potentials of this data are extensive. The provided word-level and sentence-level eye-tracking and EEG features can be used to improve and evaluate NLP and machine learning methods, for instance, to evaluate linguistic phenomena in neural models via psycholinguistic data. In addition, because the sentences contains semantic relation labels and the annotations of all participants, it can also be widely used for relation extraction and classification. Finally, the two carefully constructed reading paradigms allow for the comparison between normal reading and reading during annotation, which can be relevant to improve the manual labelling process as well as the quality of the annotations for supervised machine learning.\n"
     ]
    }
   ],
   "source": [
    "instance = instances[0]\n",
    "print(instance.keys())\n",
    "print('QUESTION WITH CONTEXT:')\n",
    "# print_wrap(instance['s_question_with_context'])\n",
    "print(instance['s_question_with_context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Did Ġthey Ġexperiment Ġwith Ġthis Ġnew Ġdataset ? </s> Introduction </s> How\n",
      "Ġhumans Ġprocess Ġlanguage Ġhas Ġbecome Ġincreasingly Ġrelevant Ġin Ġnatural\n",
      "Ġlanguage Ġprocessing Ġsince Ġphysiological Ġdata Ġduring Ġlanguage\n",
      "Ġunderstanding Ġis Ġmore Ġaccessible Ġand Ġrecorded Ġwith Ġless Ġeffort . ĠIn\n",
      "Ġthis Ġwork , Ġwe Ġfocus Ġon Ġeye - tracking Ġand Ġelectro ence phal ography Ġ(\n",
      "EE G ) Ġrecordings Ġto Ġcapture Ġthe Ġreading Ġprocess . ĠOn Ġone Ġhand , Ġeye\n",
      "Ġmovement Ġdata Ġprovides Ġmillisec ond - acc urate Ġrecords Ġabout Ġwhere\n",
      "Ġhumans Ġlook Ġwhen Ġthey Ġare Ġreading , Ġand Ġis Ġhighly Ġcorrelated Ġwith\n",
      "Ġthe Ġcognitive Ġload Ġassociated Ġwith Ġdifferent Ġstages Ġof Ġtext Ġprocessing\n",
      ". ĠOn Ġthe Ġother Ġhand , ĠEEG Ġrecords Ġelectrical Ġbrain Ġactivity Ġacross\n",
      "Ġthe Ġscalp Ġand Ġis Ġa Ġdirect Ġmeasure Ġof Ġphysiological Ġprocesses ,\n",
      "Ġincluding Ġlanguage Ġprocessing . ĠThe Ġcombination Ġof Ġboth Ġmeasurement\n",
      "Ġmethods Ġenables Ġus Ġto Ġstudy Ġthe Ġlanguage Ġunderstanding Ġprocess Ġin Ġa\n",
      "Ġmore Ġnatural Ġsetting , Ġwhere Ġparticipants Ġread Ġfull Ġsentences Ġat Ġa\n",
      "Ġtime , Ġin Ġtheir Ġown Ġspeed . ĠEye - tracking Ġthen Ġpermits Ġus Ġto Ġdefine\n",
      "Ġexact Ġword Ġboundaries Ġin Ġthe Ġtimeline Ġof Ġa Ġsubject Ġreading Ġa\n",
      "Ġsentence , Ġallowing Ġthe Ġextraction Ġof Ġbrain Ġactivity Ġsignals Ġfor Ġeach\n",
      "Ġword . </s> Human Ġcognitive Ġlanguage Ġprocessing Ġdata Ġis Ġimmensely Ġuseful\n",
      "Ġfor ĠN LP : ĠNot Ġonly Ġcan Ġit Ġbe Ġlever aged Ġto Ġimprove ĠN LP\n",
      "Ġapplications Ġ( e . g . Ġbar rett 2016 weak ly Ġfor Ġpart - of - speech\n",
      "Ġtagging Ġor Ġk ler ke 2016 impro ving Ġfor Ġsentence Ġcompression ), Ġbut Ġalso\n",
      "Ġto Ġevaluate Ġstate - of - the - art Ġmachine Ġlearning Ġsystems . ĠFor\n",
      "Ġexample , Ġh ollen stein 2019 c ogn ival Ġevaluate Ġword Ġembed d ings , Ġor\n",
      "Ġsch w artz 2019 inducing Ġfine - t une Ġlanguage Ġmodels Ġwith Ġbrain -\n",
      "relevant Ġbias . </s> Additionally , Ġthe Ġavailability Ġof Ġlabelled Ġdata\n",
      "Ġplays Ġa Ġcrucial Ġrole Ġin Ġall Ġsupervised Ġmachine Ġlearning Ġapplications .\n",
      "ĠPhys iological Ġdata Ġcan Ġbe Ġused Ġto Ġunderstand Ġand Ġimprove Ġthe Ġlab\n",
      "elling Ġprocess Ġ( e . g . Ġto kun aga 2017 eye ), Ġand , Ġfor Ġinstance , Ġto\n",
      "Ġbuild Ġcost Ġmodels Ġfor Ġactive Ġlearning Ġscenarios ĠB IB REF 0 . ĠIs Ġit\n",
      "Ġpossible Ġto Ġreplace Ġthis Ġexpensive Ġmanual Ġwork Ġwith Ġmodels Ġtrained Ġon\n",
      "Ġphysiological Ġactivity Ġdata Ġrecorded Ġfrom Ġhumans Ġwhile Ġreading ? ĠThat\n",
      "Ġis Ġto Ġsay , Ġcan Ġwe Ġfind Ġand Ġextract Ġrelevant Ġaspects Ġof Ġtext\n",
      "Ġunderstanding Ġand Ġannotation Ġdirectly Ġfrom Ġthe Ġsource , Ġi . e . Ġeye -\n",
      "tracking Ġand Ġbrain Ġactivity Ġsignals Ġduring Ġreading ? </s> Mot ivated Ġby\n",
      "Ġthese Ġquestions Ġand Ġour Ġpreviously Ġreleased Ġdataset , ĠZ u Co Ġ1 . 0 ĠB\n",
      "IB REF 1 , Ġwe Ġdeveloped Ġthis Ġnew Ġcorpus , Ġwhere Ġwe Ġspecifically Ġaim Ġto\n",
      "Ġcollect Ġrecordings Ġduring Ġnatural Ġreading Ġas Ġwell Ġas Ġduring Ġannotation\n",
      ". </s> We Ġprovide Ġthe Ġfirst Ġdataset Ġof Ġsimultaneous Ġeye Ġmovement Ġand\n",
      "Ġbrain Ġactivity Ġrecordings Ġto Ġanalyze Ġand Ġcompare Ġnormal Ġreading Ġto\n",
      "Ġtask - specific Ġreading Ġduring Ġannotation . ĠThe ĠZurich ĠCognitive\n",
      "ĠLanguage ĠProcessing ĠCorpus Ġ( Z u Co ) Ġ2 . 0 , Ġincluding Ġraw Ġand Ġpre\n",
      "process ed Ġeye - tracking Ġand Ġelectro ence phal ography Ġ( EE G ) Ġdata Ġof\n",
      "Ġ18 Ġsubjects , Ġas Ġwell Ġas Ġthe Ġrecording Ġand Ġpre processing Ġscripts ,\n",
      "Ġis Ġpublicly Ġavailable Ġat Ġhttps :// os f . io / 2 ur ht /. ĠIt Ġcontains\n",
      "Ġphysiological Ġdata Ġof Ġeach Ġsubject Ġreading Ġ7 39 ĠEnglish Ġsentences Ġfrom\n",
      "ĠWikipedia Ġ( see Ġexample Ġin ĠFigure ĠFIG REF 1 ). ĠWe Ġwant Ġto Ġhighlight\n",
      "Ġthe Ġre - use Ġpotential Ġof Ġthis Ġdata . ĠIn Ġaddition Ġto Ġthe Ġpsych ol\n",
      "ingu istic Ġmotivation , Ġthis Ġcorpus Ġis Ġespecially Ġtailored Ġfor Ġtraining\n",
      "Ġand Ġevaluating Ġmachine Ġlearning Ġalgorithms Ġfor ĠN LP Ġpurposes . ĠWe\n",
      "Ġconduct Ġa Ġdetailed Ġtechnical Ġvalidation Ġof Ġthe Ġdata Ġas Ġproof Ġof Ġthe\n",
      "Ġquality Ġof Ġthe Ġrecordings . </s> Related ĠWork </s> Some Ġeye - tracking\n",
      "Ġcorpor a Ġof Ġnatural Ġreading Ġ( e . g . Ġthe ĠDund ee ĠB IB REF 2 , ĠProv o\n",
      "ĠB IB REF 3 Ġand ĠG EC O Ġcorpus ĠB IB REF 4 ), Ġand Ġa Ġfew ĠEEG Ġcorpor a Ġ(\n",
      "for Ġexample , Ġthe ĠU CL Ġcorpus ĠB IB REF 5 ) Ġare Ġavailable . ĠIt Ġhas Ġbeen\n",
      "Ġshown Ġthat Ġthis Ġtype Ġof Ġcognitive Ġprocessing Ġdata Ġis Ġuseful Ġfor\n",
      "Ġimproving Ġand Ġevaluating ĠN LP Ġmethods Ġ( e . g . Ġbar rett 2018 sequence ,\n",
      "h ollen stein 2019 c ogn ival , Ġh ale 2018 finding ). ĠHowever , Ġbefore Ġthe\n",
      "ĠZurich ĠCognitive ĠLanguage ĠProcessing ĠCorpus Ġ( Z u Co Ġ1 . 0 ), Ġthere Ġwas\n",
      "Ġno Ġavailable Ġdata Ġfor Ġsimultaneous Ġeye - tracking Ġand ĠEEG Ġrecordings\n",
      "Ġof Ġnatural Ġreading . Ġdim igen 2011 core g istration Ġstudied Ġthe\n",
      "Ġlinguistic Ġeffects Ġof Ġeye Ġmovements Ġand ĠEEG Ġco - reg istration Ġin\n",
      "Ġnatural Ġreading Ġand Ġshowed Ġthat Ġthey Ġaccurately Ġrepresent Ġlex ical\n",
      "Ġprocessing . ĠMoreover , Ġthe Ġsimultaneous Ġrecordings Ġare Ġcrucial Ġto\n",
      "Ġextract Ġword - level Ġbrain Ġactivity Ġsignals . </s> While Ġthe Ġabove\n",
      "Ġmentioned Ġstudies Ġanalyze Ġand Ġleverage Ġnatural Ġreading , Ġsome ĠN LP\n",
      "Ġwork Ġhas Ġused Ġeye - tracking Ġduring Ġannotation Ġ( but , Ġas Ġof Ġyet ,\n",
      "Ġnot ĠEEG Ġdata ). Ġmish ra 2016 p redict ing Ġand Ġj oshi 2014 me asuring\n",
      "Ġrecorded Ġeye - tracking Ġduring Ġbinary Ġsentiment Ġannotation Ġ( positive /\n",
      "negative ). ĠThis Ġdata Ġwas Ġused Ġto Ġdetermine Ġthe Ġannotation Ġcomplexity\n",
      "Ġof Ġthe Ġtext Ġpassages Ġbased Ġon Ġeye Ġmovement Ġmetrics Ġand Ġfor Ġsarc asm\n",
      "Ġdetection ĠB IB REF 6 . ĠMoreover , Ġeye - tracking Ġhas Ġbeen Ġused Ġto\n",
      "Ġanalyze Ġthe Ġword Ġsense Ġannotation Ġprocess Ġin ĠHindi ĠB IB REF 7 , Ġnamed\n",
      "Ġentity Ġannotation Ġin ĠJapanese ĠB IB REF 8 , Ġand Ġto Ġleverage Ġannot ator\n",
      "Ġgaze Ġbehaviour Ġfor Ġcore ference Ġresolution ĠB IB REF 9 . ĠFinally , Ġto man\n",
      "ek 2010 c ognitive Ġused Ġeye - tracking Ġdata Ġduring Ġentity Ġannotation Ġto\n",
      "Ġbuild Ġa Ġcost Ġmodel Ġfor Ġactive Ġlearning . ĠHowever , Ġuntil Ġnow Ġthere\n",
      "Ġis Ġno Ġavailable Ġdata Ġor Ġresearch Ġthat Ġanaly zes Ġthe Ġdifferences Ġin\n",
      "Ġthe Ġhuman Ġprocessing Ġof Ġnormal Ġreading Ġversus Ġannotation . </s> Related\n",
      "ĠWork Ġ: :: ĠZ u Co 1 . 0 </s> In Ġprevious Ġwork , Ġwe Ġrecorded Ġa Ġfirst\n",
      "Ġdataset Ġof Ġsimultaneous Ġeye - tracking Ġand ĠEEG Ġduring Ġnatural Ġreading\n",
      "ĠB IB REF 1 . ĠZ u Co Ġ1 . 0 Ġconsists Ġof Ġthree Ġreading Ġtasks , Ġtwo Ġof\n",
      "Ġwhich Ġcontain Ġvery Ġsimilar Ġreading Ġmaterial Ġand Ġexperiments Ġas\n",
      "Ġpresented Ġin Ġthe Ġcurrent Ġwork . ĠHowever , Ġthe Ġmain Ġdifference Ġand\n",
      "Ġreason Ġfor Ġrecording ĠZ u Co Ġ2 . 0 , Ġconsists Ġin Ġthe Ġexperiment\n",
      "Ġprocedure . ĠFor ĠZ u Co Ġ1 . 0 Ġthe Ġnormal Ġreading Ġand Ġtask - specific\n",
      "Ġreading Ġparad ig ms Ġwere Ġrecorded Ġin Ġdifferent Ġsessions Ġon Ġdifferent\n",
      "Ġdays . ĠTherefore , Ġthe Ġrecorded Ġdata Ġis Ġnot Ġappropriate Ġas Ġa Ġmeans\n",
      "Ġof Ġcomparison Ġbetween Ġnatural Ġreading Ġand Ġannotation , Ġsince Ġthe\n",
      "Ġdifferences Ġin Ġthe Ġbrain Ġactivity Ġdata Ġmight Ġresult Ġmostly Ġfrom Ġthe\n",
      "Ġdifferent Ġsessions Ġdue Ġto Ġthe Ġsensitivity Ġof ĠEEG . ĠThis , Ġand\n",
      "Ġextending Ġthe Ġdataset Ġwith Ġmore Ġsentences Ġand Ġmore Ġsubjects , Ġwere\n",
      "Ġthe Ġmain Ġfactors Ġfor Ġrecording Ġthe Ġcurrent Ġcorpus . ĠWe Ġpurposefully\n",
      "Ġmaintained Ġan Ġoverlap Ġof Ġsome Ġsentences Ġbetween Ġboth Ġdatasets Ġto\n",
      "Ġallow Ġadditional Ġanalyses Ġ( details Ġare Ġdescribed Ġin ĠSection ĠSEC REF 7\n",
      "). </s> Corp us ĠConstruction </s> In Ġthis Ġsection Ġwe Ġdescribe Ġthe\n",
      "Ġcontents Ġand Ġexperimental Ġdesign Ġof Ġthe ĠZ u Co Ġ2 . 0 Ġcorpus . </s> Corp\n",
      "us ĠConstruction Ġ: :: ĠParticipants </s> We Ġrecorded Ġdata Ġfrom Ġ19\n",
      "Ġparticipants Ġand Ġdiscarded Ġthe Ġdata Ġof Ġone Ġof Ġthem Ġdue Ġto Ġtechnical\n",
      "Ġdifficulties Ġwith Ġthe Ġeye - tracking Ġcalibration . ĠHence , Ġwe Ġshare Ġthe\n",
      "Ġdata Ġof Ġ18 Ġparticipants . ĠAll Ġparticipants Ġare Ġhealthy Ġadults Ġ( mean\n",
      "Ġage Ġ= Ġ34 Ġ( SD = 8 . 3 ), Ġ10 Ġfemales ). ĠTheir Ġnative Ġlanguage Ġis\n",
      "ĠEnglish , Ġoriginating Ġfrom ĠAustralia , ĠCanada , ĠUK , ĠUSA Ġor ĠSouth\n",
      "ĠAfrica . ĠTwo Ġparticipants Ġare Ġleft - handed Ġand Ġthree Ġparticipants Ġwear\n",
      "Ġglasses Ġfor Ġreading . ĠDetails Ġon Ġsubject Ġdemographics Ġcan Ġbe Ġfound Ġin\n",
      "ĠTable ĠT AB REF 4 . ĠAll Ġparticipants Ġgave Ġwritten Ġconsent Ġfor Ġtheir\n",
      "Ġparticipation Ġand Ġthe Ġre - use Ġof Ġthe Ġdata Ġprior Ġto Ġthe Ġstart Ġof\n",
      "Ġthe Ġexperiments . ĠThe Ġstudy Ġwas Ġapproved Ġby Ġthe ĠEthics ĠCommission Ġof\n",
      "Ġthe ĠUniversity Ġof ĠZurich . </s> Corp us ĠConstruction Ġ: :: ĠReading\n",
      "Ġmaterials </s> During Ġthe Ġrecording Ġsession , Ġthe Ġparticipants Ġread Ġ7 39\n",
      "Ġsentences Ġthat Ġwere Ġselected Ġfrom Ġthe ĠWikipedia Ġcorpus Ġprovided Ġby\n",
      "Ġcul otta 2006 integ rating . ĠThis Ġcorpus Ġwas Ġchosen Ġbecause Ġit Ġprovides\n",
      "Ġannotations Ġof Ġsemantic Ġrelations . ĠWe Ġincluded Ġseven Ġof Ġthe\n",
      "Ġoriginally Ġdefined Ġrelation Ġtypes : Ġpolitical _ aff iliation , Ġeducation ,\n",
      "Ġfounder , Ġwife / husband , Ġjob _ title , Ġnationality , Ġand Ġemployer . ĠThe\n",
      "Ġsentences Ġwere Ġchosen Ġin Ġthe Ġsame Ġlength Ġrange Ġas ĠZ u Co Ġ1 . 0 , Ġand\n",
      "Ġwith Ġsimilar ĠF les ch Ġreading Ġease Ġscores . ĠThe Ġdataset Ġstatistics Ġare\n",
      "Ġshown Ġin ĠTable ĠT AB REF 2 . </s> Of Ġthe Ġ7 39 Ġsentences , Ġthe\n",
      "Ġparticipants Ġread Ġ349 Ġsentences Ġin Ġa Ġnormal Ġreading Ġparadigm , Ġand\n",
      "Ġ390 Ġsentences Ġin Ġa Ġtask - specific Ġreading Ġparadigm , Ġin Ġwhich Ġthey\n",
      "Ġhad Ġto Ġdetermine Ġwhether Ġa Ġcertain Ġrelation Ġtype Ġoccurred Ġin Ġthe\n",
      "Ġsentence Ġor Ġnot . ĠTable ĠT AB REF 3 Ġshows Ġthe Ġdistribution Ġof Ġthe\n",
      "Ġdifferent Ġrelation Ġtypes Ġin Ġthe Ġsentences Ġof Ġthe Ġtask - specific\n",
      "Ġannotation Ġparadigm . </s> Pur pose fully , Ġthere Ġare Ġ63 Ġdupl icates\n",
      "Ġbetween Ġthe Ġnormal Ġreading Ġand Ġthe Ġtask - specific Ġsentences Ġ( 8 % Ġof\n",
      "Ġall Ġsentences ). ĠThe Ġintention Ġof Ġthese Ġduplicate Ġsentences Ġis Ġto\n",
      "Ġprovide Ġa Ġset Ġof Ġsentences Ġread Ġtwice Ġby Ġall Ġparticipants Ġwith Ġa\n",
      "Ġdifferent Ġtask Ġin Ġmind . ĠHence , Ġthis Ġenables Ġthe Ġcomparison Ġof Ġeye -\n",
      "tracking Ġand Ġbrain Ġactivity Ġdata Ġwhen Ġreading Ġnormally Ġand Ġwhen Ġannot\n",
      "ating Ġspecific Ġrelations Ġ( see Ġexamples Ġin ĠSection ĠSEC REF 4 ). </s>\n",
      "Furthermore , Ġthere Ġis Ġalso Ġan Ġoverlap Ġin Ġthe Ġsentences Ġbetween ĠZ u Co\n",
      "Ġ1 . 0 Ġand ĠZ u Co Ġ2 . 0 . Ġ100 Ġnormal Ġreading Ġand Ġ85 Ġtask - specific\n",
      "Ġsentences Ġrecorded Ġfor Ġthis Ġdataset Ġwere Ġalready Ġrecorded Ġin ĠZ u Co Ġ1\n",
      ". 0 . ĠThis Ġallows Ġfor Ġcomparisons Ġbetween Ġthe Ġdifferent Ġrecording\n",
      "Ġprocedures Ġ( i . e . Ġsession - specific Ġeffects ) Ġand Ġbetween Ġmore\n",
      "Ġparticipants Ġ( subject - specific Ġeffects ). </s> Corp us ĠConstruction Ġ: ::\n",
      "ĠExperimental Ġdesign </s> As Ġmentioned Ġabove , Ġwe Ġrecorded Ġtwo Ġdifferent\n",
      "Ġreading Ġtasks Ġfor Ġthe ĠZ u Co Ġ2 . 0 Ġdataset . ĠDuring Ġboth Ġtasks Ġthe\n",
      "Ġparticipants Ġwere Ġable Ġto Ġread Ġin Ġtheir Ġown Ġspeed , Ġusing Ġa Ġcontrol\n",
      "Ġpad Ġto Ġmove Ġto Ġthe Ġnext Ġsentence Ġand Ġto Ġanswer Ġthe Ġcontrol\n",
      "Ġquestions , Ġwhich Ġallowed Ġfor Ġnatural Ġreading . ĠSince Ġeach Ġsubject\n",
      "Ġreads Ġat Ġtheir Ġown Ġpersonal Ġpace , Ġthe Ġreading Ġspeed Ġbetween Ġvaries\n",
      "Ġbetween Ġsubjects . ĠTable ĠT AB REF 4 Ġshows Ġthe Ġaverage Ġreading Ġspeed\n",
      "Ġfor Ġeach Ġtask , Ġi . e . Ġthe Ġaverage Ġnumber Ġof Ġseconds Ġa Ġsubject\n",
      "Ġspends Ġper Ġsentence Ġbefore Ġswitching Ġto Ġthe Ġnext Ġone . </s> All Ġ7 39\n",
      "Ġsentences Ġwere Ġrecorded Ġin Ġa Ġsingle Ġsession Ġfor Ġeach Ġparticipant .\n",
      "ĠThe Ġduration Ġof Ġthe Ġrecording Ġsessions Ġwas Ġbetween Ġ100 Ġand Ġ180\n",
      "Ġminutes , Ġdepending Ġon Ġthe Ġtime Ġrequired Ġto Ġset Ġup Ġand Ġcalibr ate\n",
      "Ġthe Ġdevices , Ġand Ġthe Ġpersonal Ġreading Ġspeed Ġof Ġthe Ġparticipants .\n",
      "</s> We Ġrecorded Ġ14 Ġblocks Ġof Ġapprox . Ġ50 Ġsentences , Ġalternating\n",
      "Ġbetween Ġtasks : Ġ50 Ġsentences Ġof Ġnormal Ġreading , Ġfollowed Ġby Ġ50\n",
      "Ġsentences Ġof Ġtask - specific Ġreading . ĠThe Ġorder Ġof Ġblocks Ġand\n",
      "Ġsentences Ġwithin Ġblocks Ġwas Ġidentical Ġfor Ġall Ġsubjects . ĠEach Ġsentence\n",
      "Ġblock Ġwas Ġpreceded Ġby Ġa Ġpractice Ġround Ġof Ġthree Ġsentences . </s> Corp\n",
      "us ĠConstruction Ġ: :: ĠExperimental Ġdesign Ġ: :: ĠNormal Ġreading Ġ( NR ) </s>\n",
      "In Ġthe Ġfirst Ġtask , Ġparticipants Ġwere Ġinstructed Ġto Ġread Ġthe Ġsentences\n",
      "Ġnaturally , Ġwithout Ġany Ġspecific Ġtask Ġother Ġthan Ġcomprehension .\n",
      "ĠParticipants Ġwere Ġtold Ġto Ġread Ġthe Ġsentences Ġnormally Ġwithout Ġany\n",
      "Ġspecial Ġinstructions . ĠFigure ĠFIG REF 8 Ġ( left ) Ġshows Ġan Ġexample\n",
      "Ġsentence Ġas Ġit Ġwas Ġdepicted Ġon Ġthe Ġscreen Ġduring Ġrecording . ĠAs\n",
      "Ġshown Ġin ĠFigure ĠFIG REF 8 Ġ( middle ), Ġthe Ġcontrol Ġcondition Ġfor Ġthis\n",
      "Ġtask Ġconsisted Ġof Ġsingle - choice Ġquestions Ġabout Ġthe Ġcontent Ġof Ġthe\n",
      "Ġprevious Ġsentence . Ġ12 % Ġof Ġrandomly Ġselected Ġsentences Ġwere Ġfollowed\n",
      "Ġby Ġsuch Ġa Ġcomprehension Ġquestion Ġwith Ġthree Ġanswer Ġoptions Ġon Ġa\n",
      "Ġseparate Ġscreen . </s> Corp us ĠConstruction Ġ: :: ĠExperimental Ġdesign Ġ: ::\n",
      "ĠTask - specific Ġreading Ġ( TS R ) </s> In Ġthe Ġsecond Ġtask , Ġthe\n",
      "Ġparticipants Ġwere Ġinstructed Ġto Ġsearch Ġfor Ġa Ġspecific Ġrelation Ġin\n",
      "Ġeach Ġsentence Ġthey Ġread . ĠInstead Ġof Ġcomprehension Ġquestions , Ġthe\n",
      "Ġparticipants Ġhad Ġto Ġdecide Ġfor Ġeach Ġsentence Ġwhether Ġit Ġcontains Ġthe\n",
      "Ġrelation Ġor Ġnot , Ġi . e . Ġthey Ġwere Ġactively Ġannot ating Ġeach Ġsentence\n",
      ". ĠFigure ĠFIG REF 8 Ġ( right ) Ġshows Ġan Ġexample Ġscreen Ġfor Ġthis Ġtask .\n",
      "Ġ17 % Ġof Ġthe Ġsentences Ġdid Ġnot Ġinclude Ġthe Ġrelation Ġtype Ġand Ġwere\n",
      "Ġused Ġas Ġcontrol Ġconditions . ĠAll Ġsentences Ġwithin Ġone Ġblock Ġinvolved\n",
      "Ġthe Ġsame Ġrelation Ġtype . ĠThe Ġblocks Ġstarted Ġwith Ġa Ġpractice Ġround ,\n",
      "Ġwhich Ġdescribed Ġthe Ġrelation Ġand Ġwas Ġfollowed Ġby Ġthree Ġsample\n",
      "Ġsentences , Ġso Ġthat Ġthe Ġparticipants Ġwould Ġbe Ġfamiliar Ġwith Ġthe\n",
      "Ġrespective Ġrelation Ġtype . </s> Corp us ĠConstruction Ġ: :: ĠL ingu istic\n",
      "Ġassessment </s> As Ġa Ġlinguistic Ġassessment , Ġthe Ġvocabulary Ġand Ġlanguage\n",
      "Ġproficiency Ġof Ġthe Ġparticipants Ġwas Ġtested Ġwith Ġthe ĠLex T ALE Ġtest Ġ(\n",
      "Lex ical ĠTest Ġfor ĠAdvanced ĠLear ners Ġof ĠEnglish , Ġle m h ofer 2012\n",
      "introdu cing ). ĠThis Ġis Ġan Ġun speed ed Ġlex ical Ġdecision Ġtask Ġdesigned\n",
      "Ġfor Ġintermediate Ġto Ġhighly Ġproficient Ġlanguage Ġusers . ĠThe Ġaverage ĠLex\n",
      "T ALE Ġscore Ġover Ġall Ġparticipants Ġwas Ġ88 . 54 %. ĠMoreover , Ġwe Ġalso\n",
      "Ġreport Ġthe Ġscores Ġthe Ġparticipants Ġachieved Ġwith Ġtheir Ġanswers Ġto Ġthe\n",
      "Ġreading Ġcomprehension Ġcontrol Ġquestions Ġand Ġtheir Ġrelation Ġannotations .\n",
      "ĠThe Ġdetailed Ġscores Ġfor Ġall Ġparticipants Ġare Ġalso Ġpresented Ġin ĠTable\n",
      "ĠT AB REF 4 . </s> Corp us ĠConstruction Ġ: :: ĠData Ġacquisition </s> Data\n",
      "Ġacquisition Ġtook Ġplace Ġin Ġa Ġsound - atten uated Ġand Ġdark Ġexperiment\n",
      "Ġroom . ĠParticipants Ġwere Ġseated Ġat Ġa Ġdistance Ġof Ġ68 cm Ġfrom Ġa Ġ24 -\n",
      "inch Ġmonitor Ġwith Ġa Ġresolution Ġof Ġ800 x 600 Ġpixels . ĠA Ġstable Ġhead\n",
      "Ġposition Ġwas Ġensured Ġvia Ġa Ġchin Ġrest . ĠParticipants Ġwere Ġinstructed\n",
      "Ġto Ġstay Ġas Ġstill Ġas Ġpossible Ġduring Ġthe Ġtasks Ġto Ġavoid Ġmotor ĠEEG\n",
      "Ġartifacts . ĠParticipants Ġwere Ġalso Ġoffered Ġsnacks Ġand Ġwater Ġduring Ġthe\n",
      "Ġbreaks Ġand Ġwere Ġencouraged Ġto Ġrest . ĠAll Ġsentences Ġwere Ġpresented Ġat\n",
      "Ġthe Ġsame Ġposition Ġon Ġthe Ġscreen Ġand Ġcould Ġspan Ġmultiple Ġlines . ĠThe\n",
      "Ġsentences Ġwere Ġpresented Ġin Ġblack Ġon Ġa Ġlight Ġgrey Ġbackground Ġwith\n",
      "Ġfont Ġsize Ġ20 - point ĠA rial , Ġresulting Ġin Ġa Ġletter Ġheight Ġof Ġ0 . 8\n",
      "Ġmm . ĠThe Ġexperiment Ġwas Ġprogrammed Ġin ĠMAT LAB Ġ2016 b ĠB IB REF 10 ,\n",
      "Ġusing ĠPsych Tool box ĠB IB REF 11 . ĠParticipants Ġcompleted Ġthe Ġtasks\n",
      "Ġsitting Ġalone Ġin Ġthe Ġroom , Ġwhile Ġtwo Ġresearch Ġassistants Ġwere\n",
      "Ġmonitoring Ġtheir Ġprogress Ġin Ġthe Ġadjoining Ġroom . ĠAll Ġrecording\n",
      "Ġscripts Ġincluding Ġdetailed Ġparticipant Ġinstructions Ġare Ġavailable\n",
      "Ġalongside Ġthe Ġdata . </s> Corp us ĠConstruction Ġ: :: ĠData Ġacquisition Ġ:\n",
      ":: ĠEye - tracking Ġacquisition </s> Eye Ġposition Ġand Ġpupil Ġsize Ġwere\n",
      "Ġrecorded Ġwith Ġan Ġinfrared Ġvideo - based Ġeye Ġtracker Ġ( Eye Link Ġ1000\n",
      "ĠPlus , ĠSR ĠResearch ) Ġat Ġa Ġsampling Ġrate Ġof Ġ500 ĠHz . ĠThe Ġeye Ġtracker\n",
      "Ġwas Ġcalibrated Ġwith Ġa Ġ9 - point Ġgrid Ġat Ġthe Ġbeginning Ġof Ġthe Ġsession\n",
      "Ġand Ġre - valid ated Ġbefore Ġeach Ġblock Ġof Ġsentences . </s> Corp us\n",
      "ĠConstruction Ġ: :: ĠData Ġacquisition Ġ: :: ĠEEG Ġacquisition </s> High -\n",
      "density ĠEEG Ġdata Ġwere Ġrecorded Ġat Ġa Ġsampling Ġrate Ġof Ġ500 ĠHz Ġwith Ġa\n",
      "Ġband pass Ġof Ġ0 . 1 Ġto Ġ100 ĠHz , Ġusing Ġa Ġ128 - channel ĠEEG ĠGe odes ic\n",
      "ĠHydro cel Ġsystem Ġ( Elect rical ĠGe odes ics ). ĠThe Ġrecording Ġreference\n",
      "Ġwas Ġset Ġat Ġelectrode ĠC z . ĠThe Ġhead Ġcircumference Ġof Ġeach Ġparticipant\n",
      "Ġwas Ġmeasured Ġto Ġselect Ġan Ġappropriately Ġsized ĠEEG Ġnet . ĠTo Ġensure\n",
      "Ġgood Ġcontact , Ġthe Ġimpedance Ġof Ġeach Ġelectrode Ġwas Ġchecked Ġprior Ġto\n",
      "Ġrecording , Ġand Ġwas Ġkept Ġbelow Ġ40 Ġk Oh m . ĠElectro de Ġimpedance Ġlevels\n",
      "Ġwere Ġchecked Ġafter Ġevery Ġthird Ġblock Ġof Ġ50 Ġsentences Ġ( app rox .\n",
      "Ġevery Ġ30 Ġmins ) Ġand Ġreduced Ġif Ġnecessary . </s> Corp us ĠConstruction Ġ:\n",
      ":: ĠPre processing Ġand Ġfeature Ġextraction Ġ: :: ĠEye - tracking </s> The Ġeye\n",
      "- tracking Ġdata Ġconsists Ġof Ġ( x , y ) Ġgaze Ġlocation Ġentries Ġfor Ġall\n",
      "Ġindividual Ġfix ations Ġ( Figure ĠFIG REF 1 b ). ĠCoord inates Ġwere Ġgiven Ġin\n",
      "Ġpixels Ġwith Ġrespect Ġto Ġthe Ġmonitor Ġcoordinates Ġ( the Ġupper Ġleft\n",
      "Ġcorner Ġof Ġthe Ġscreen Ġwas Ġ( 0 , 0 ) Ġand Ġdown / right Ġwas Ġpositive ).\n",
      "ĠWe Ġprovide Ġthis Ġraw Ġdata Ġas Ġwell Ġas Ġvarious Ġengineered Ġeye - tracking\n",
      "Ġfeatures . ĠFor Ġthis Ġfeature Ġextraction Ġonly Ġfix ations Ġwithin Ġthe\n",
      "Ġboundaries Ġof Ġeach Ġdisplayed Ġword Ġwere Ġextracted . ĠData Ġpoints\n",
      "Ġdistinctly Ġnot Ġassociated Ġwith Ġreading Ġ( minimum Ġdistance Ġof Ġ50 Ġpixels\n",
      "Ġto Ġthe Ġtext ) Ġwere Ġexcluded . ĠAdditionally , Ġfix ations Ġshorter Ġthan\n",
      "Ġ100 Ġms Ġwere Ġexcluded Ġfrom Ġthe Ġanalyses , Ġbecause Ġthese Ġare Ġunlikely\n",
      "Ġto Ġreflect Ġfix ations Ġrelevant Ġfor Ġreading ĠB IB REF 12 . ĠOn Ġthe Ġbasis\n",
      "Ġof Ġthe ĠG EC O Ġand ĠZ u Co Ġ1 . 0 Ġcorpor a , Ġwe Ġextracted Ġthe Ġfollowing\n",
      "Ġfeatures : Ġ( i ) Ġgaze Ġduration Ġ( GD ), Ġthe Ġsum Ġof Ġall Ġfix ations Ġon\n",
      "Ġthe Ġcurrent Ġword Ġin Ġthe Ġfirst - pass Ġreading Ġbefore Ġthe Ġeye Ġmoves\n",
      "Ġout Ġof Ġthe Ġword ; Ġ( ii ) Ġtotal Ġreading Ġtime Ġ( TR T ), Ġthe Ġsum Ġof\n",
      "Ġall Ġfixation Ġd urations Ġon Ġthe Ġcurrent Ġword , Ġincluding Ġregress ions ;\n",
      "Ġ( iii ) Ġfirst Ġfixation Ġduration Ġ( FF D ), Ġthe Ġduration Ġof Ġthe Ġfirst\n",
      "Ġfixation Ġon Ġthe Ġprevailing Ġword ; Ġ( iv ) Ġsingle Ġfixation Ġduration Ġ( SF\n",
      "D ), Ġthe Ġduration Ġof Ġthe Ġfirst Ġand Ġonly Ġfixation Ġon Ġthe Ġcurrent Ġword\n",
      "; Ġand Ġ( v ) Ġgo - past Ġtime Ġ( G PT ), Ġthe Ġsum Ġof Ġall Ġfix ations Ġprior\n",
      "Ġto Ġprogressing Ġto Ġthe Ġright Ġof Ġthe Ġcurrent Ġword , Ġincluding Ġregress\n",
      "ions Ġto Ġprevious Ġwords Ġthat Ġoriginated Ġfrom Ġthe Ġcurrent Ġword . ĠFor\n",
      "Ġeach Ġof Ġthese Ġeye - tracking Ġfeatures Ġwe Ġadditionally Ġcomputed Ġthe\n",
      "Ġpupil Ġsize . ĠFurthermore , Ġwe Ġextracted Ġthe Ġnumber Ġof Ġfix ations Ġand\n",
      "Ġmean Ġpupil Ġsize Ġfor Ġeach Ġword Ġand Ġsentence . </s> Corp us ĠConstruction\n",
      "Ġ: :: ĠPre processing Ġand Ġfeature Ġextraction Ġ: :: ĠEEG </s> The ĠEEG Ġdata\n",
      "Ġshared Ġin Ġthis Ġproject Ġare Ġavailable Ġas Ġraw Ġdata , Ġbut Ġalso Ġpre\n",
      "process ed Ġwith ĠAutom agic Ġ( version Ġ1 . 4 . 6 , Ġped ron i 2019 aut om agic\n",
      "), Ġa Ġtool Ġfor Ġautomatic ĠEEG Ġdata Ġcleaning Ġand Ġvalidation . Ġ105 ĠEEG\n",
      "Ġchannels Ġ( i . e . Ġelectrodes ) Ġwere Ġused Ġfrom Ġthe Ġscalp Ġrecordings .\n",
      "Ġ9 ĠE OG Ġchannels Ġwere Ġused Ġfor Ġartifact Ġremoval Ġand Ġadditional Ġ14\n",
      "Ġchannels Ġlying Ġmainly Ġon Ġthe Ġneck Ġand Ġface Ġwere Ġdiscarded Ġbefore\n",
      "Ġdata Ġanalysis . ĠBad Ġchannels Ġwere Ġidentified Ġand Ġinterpol ated . ĠWe\n",
      "Ġused Ġthe ĠMultiple ĠArtifact ĠRe jection ĠAl gorithm Ġ( M ARA ), Ġa\n",
      "Ġsupervised Ġmachine Ġlearning Ġalgorithm Ġthat Ġevaluates ĠI CA Ġcomponents ,\n",
      "Ġfor Ġautomatic Ġartifact Ġrejection . ĠMAR A Ġhas Ġbeen Ġtrained Ġon Ġmanual\n",
      "Ġcomponent Ġclass ifications , Ġand Ġthus Ġcaptures Ġa Ġwide Ġrange Ġof\n",
      "Ġartifacts . ĠMAR A Ġis Ġespecially Ġeffective Ġat Ġdetecting Ġand Ġremoving\n",
      "Ġeye Ġand Ġmuscle Ġartifact Ġcomponents . ĠThe Ġeffect Ġof Ġthis Ġpre processing\n",
      "Ġcan Ġbe Ġseen Ġin ĠFigure ĠFIG REF 1 d . </s> After Ġpre processing , Ġwe\n",
      "Ġsynchronized Ġthe ĠEEG Ġand Ġeye - tracking Ġdata Ġto Ġenable ĠEEG Ġanalyses\n",
      "Ġtime - locked Ġto Ġthe Ġon sets Ġof Ġfix ations . ĠTo Ġcompute Ġoscill atory\n",
      "Ġpower Ġmeasures , Ġwe Ġband - pass Ġfiltered Ġthe Ġcontinuous ĠEEG Ġsignals\n",
      "Ġacross Ġan Ġentire Ġreading Ġtask Ġfor Ġfive Ġdifferent Ġfrequency Ġbands\n",
      "Ġresulting Ġin Ġa Ġtime - series Ġfor Ġeach Ġfrequency Ġband . ĠThe Ġindependent\n",
      "Ġfrequency Ġbands Ġwere Ġdetermined Ġas Ġfollows : Ġthe ta $ _ 1 $ Ġ( 4 âĢĵ 6\n",
      "ĠHz ), Ġthe ta $ _ 2 $ Ġ( 6 . 5 âĢĵ 8 ĠHz ), Ġalpha $ _ 1 $ Ġ( 8 . 5 âĢĵ 10 ĠHz\n",
      "), Ġalpha $ _ 2 $ Ġ( 10 . 5 âĢĵ 13 ĠHz ), Ġbeta $ _ 1 $ Ġ( 13 . 5 âĢĵ 18 ĠHz ),\n",
      "Ġbeta $ _ 2 $ Ġ( 18 . 5 âĢĵ 30 ĠHz ), Ġgamma $ _ 1 $ Ġ( 30 . 5 âĢĵ 40 ĠHz ),\n",
      "Ġand Ġgamma $ _ 2 $ Ġ( 40 âĢĵ 49 . 5 ĠHz ). ĠWe Ġthen Ġapplied Ġa ĠHilbert\n",
      "Ġtransformation Ġto Ġeach Ġof Ġthese Ġtime - series . ĠWe Ġspecifically Ġchose\n",
      "Ġthe ĠHilbert Ġtransformation Ġto Ġmaintain Ġthe Ġtemporal Ġinformation Ġof Ġthe\n",
      "Ġamplitude Ġof Ġthe Ġfrequency Ġbands , Ġto Ġenable Ġthe Ġpower Ġof Ġthe\n",
      "Ġdifferent Ġfrequencies Ġfor Ġtime Ġsegments Ġdefined Ġthrough Ġthe Ġfix ations\n",
      "Ġfrom Ġthe Ġeye - tracking Ġrecording . ĠThus , Ġfor Ġeach Ġeye - tracking\n",
      "Ġfeature Ġwe Ġcomputed Ġthe Ġcorresponding ĠEEG Ġfeature Ġin Ġeach Ġfrequency\n",
      "Ġband . ĠFurthermore , Ġwe Ġextracted Ġsentence - level ĠEEG Ġfeatures Ġby\n",
      "Ġcalculating Ġthe Ġpower Ġin Ġeach Ġfrequency Ġband , Ġand Ġadditionally , Ġthe\n",
      "Ġdifference Ġof Ġthe Ġpower Ġspect ra Ġbetween Ġfrontal Ġleft Ġand Ġright Ġhom\n",
      "ologue Ġelectrodes Ġpairs . ĠFor Ġeach Ġeye - tracking Ġbased ĠEEG Ġfeature ,\n",
      "Ġall Ġchannels Ġwere Ġsubject Ġto Ġan Ġartifact Ġrejection Ġcriterion Ġof Ġ$ 90\n",
      "\\ mu ĠV $ Ġto Ġexclude Ġtrials Ġwith Ġtransient Ġnoise . </s> Data ĠVal idation\n",
      "</s> The Ġaim Ġof Ġthe Ġtechnical Ġvalidation Ġof Ġthe Ġdata Ġis Ġto Ġguarantee\n",
      "Ġgood Ġrecording Ġquality Ġand Ġto Ġreplicate Ġfindings Ġof Ġprevious Ġstudies\n",
      "Ġinvestigating Ġco - reg istration Ġof ĠEEG Ġand Ġeye Ġmovement Ġdata Ġduring\n",
      "Ġnatural Ġreading Ġtasks Ġ( e . g . Ġdim igen 2011 core g istration ). ĠWe Ġalso\n",
      "Ġcompare Ġthe Ġresults Ġto ĠZ u Co Ġ1 . 0 ĠB IB REF 1 , Ġwhich Ġallows Ġa Ġmore\n",
      "Ġdirect Ġcomparison Ġdue Ġto Ġthe Ġanalogous Ġrecording Ġprocedure . </s> Data\n",
      "ĠVal idation Ġ: :: ĠEye - tracking </s> We Ġvalidated Ġthe Ġrecorded Ġeye -\n",
      "tracking Ġdata Ġby Ġanalyzing Ġthe Ġfix ations Ġmade Ġby Ġall Ġsubjects Ġthrough\n",
      "Ġtheir Ġreading Ġspeed Ġand Ġomission Ġrate Ġon Ġsentence Ġlevel . ĠThe\n",
      "Ġomission Ġrate Ġis Ġdefined Ġas Ġthe Ġpercentage Ġof Ġwords Ġthat Ġis Ġnot Ġfix\n",
      "ated Ġin Ġa Ġsentence . ĠFigure ĠFIG REF 10 Ġ( middle ) Ġshows Ġthe Ġmean\n",
      "Ġreading Ġspeed Ġover Ġall Ġsubjects , Ġmeasured Ġin Ġseconds Ġper Ġsentence\n",
      "Ġand ĠFigure ĠFIG REF 10 Ġ( right ) Ġshows Ġthe Ġmean Ġomission Ġrates Ġaggreg\n",
      "ated Ġover Ġall Ġsubjects Ġfor Ġeach Ġtask . ĠClearly , Ġthe Ġparticipants Ġmade\n",
      "Ġless Ġfix ations Ġduring Ġthe Ġtask - specific Ġreading , Ġwhich Ġlead Ġto\n",
      "Ġfaster Ġreading Ġspeed . </s> Moreover , Ġwe Ġcorrobor ated Ġthese Ġsentence -\n",
      "level Ġmetrics Ġby Ġvisual izing Ġthe Ġskipping Ġproportion Ġon Ġword Ġlevel Ġ(\n",
      "Figure ĠFIG REF 13 ). ĠThe Ġskipping Ġproportion Ġis Ġthe Ġaverage Ġrate Ġof\n",
      "Ġwords Ġbeing Ġskipped Ġ( i . e . Ġnot Ġbeing Ġfix ated ) Ġin Ġa Ġsentence . ĠAs\n",
      "Ġexpected , Ġthis Ġalso Ġincreases Ġin Ġthe Ġtask - specific Ġreading . </s>\n",
      "Although Ġthe Ġreading Ġmaterial Ġis Ġfrom Ġthe Ġsame Ġsource Ġand Ġof Ġthe\n",
      "Ġsame Ġlength Ġrange Ġ( see ĠFigure ĠFIG REF 10 Ġ( left )), Ġin Ġthe Ġfirst\n",
      "Ġtask Ġ( NR ) Ġpassive Ġreading Ġwas Ġrecorded , Ġwhile Ġin Ġthe Ġsecond Ġtask\n",
      "Ġ( TS R ) Ġthe Ġsubjects Ġhad Ġto Ġannot ate Ġa Ġspecific Ġrelation Ġtype Ġin\n",
      "Ġeach Ġsentence . ĠThus , Ġthe Ġtask - specific Ġannotation Ġreading Ġlead Ġto\n",
      "Ġshorter Ġpasses Ġbecause Ġthe Ġgoal Ġwas Ġmerely Ġto Ġrecognize Ġa Ġrelation\n",
      "Ġin Ġthe Ġtext , Ġbut Ġnot Ġnecessarily Ġto Ġprocess Ġthe Ġevery Ġword Ġin Ġeach\n",
      "Ġsentence . ĠThis Ġdistinct Ġreading Ġbehavior Ġis Ġshown Ġin ĠFigure ĠFIG REF\n",
      "15 , Ġwhere Ġfix ations Ġoccur Ġuntil Ġthe Ġend Ġof Ġthe Ġsentence Ġduring\n",
      "Ġnormal Ġreading , Ġwhile Ġduring Ġtask - specific Ġreading Ġthe Ġfix ations\n",
      "Ġstop Ġafter Ġthe Ġdecisive Ġwords Ġto Ġdetect Ġa Ġgiven Ġrelation Ġtype .\n",
      "ĠFinally , Ġwe Ġalso Ġanalyzed Ġthe Ġaverage Ġreading Ġtimes Ġfor Ġeach Ġof Ġthe\n",
      "Ġextracted Ġeye - tracking Ġfeatures . ĠThe Ġmeans Ġand Ġdistributions Ġfor\n",
      "Ġboth Ġtasks Ġare Ġshown Ġin ĠFigure ĠFIG REF 21 . ĠThese Ġresults Ġare Ġin\n",
      "Ġline Ġwith Ġthe Ġrecorded Ġdata Ġin ĠZ u Co Ġ1 . 0 , Ġas Ġwell Ġas Ġwith Ġthe\n",
      "Ġfeatures Ġextracted Ġin Ġthe ĠG EC O Ġcorpus ĠB IB REF 4 . </s> Data ĠVal\n",
      "idation Ġ: :: ĠEEG </s> As Ġa Ġfirst Ġvalidation Ġstep , Ġwe Ġextracted\n",
      "Ġfixation - related Ġpotential s Ġ( FR Ps ), Ġwhere Ġthe ĠEEG Ġsignal Ġduring\n",
      "Ġall Ġfix ations Ġof Ġone Ġtask Ġare Ġaveraged . ĠFigure ĠFIG REF 24 Ġshows Ġthe\n",
      "Ġtime - series Ġof Ġthe Ġresulting ĠFR Ps Ġfor Ġtwo Ġelectrodes Ġ( PO 8 Ġand ĠC\n",
      "z ), Ġas Ġwell Ġas Ġtop ographies Ġof Ġthe Ġvoltage Ġdistributions Ġacross Ġthe\n",
      "Ġscalp Ġat Ġselected Ġpoints Ġin Ġtime . ĠThe Ġfive Ġcomponents Ġ( for Ġwhich\n",
      "Ġthe Ġscalp Ġtop ographies Ġare Ġplotted ) Ġare Ġhighly Ġsimilar Ġin Ġthe Ġtime\n",
      "- course Ġof Ġthe Ġchosen Ġelectrodes Ġto Ġdim igen 2011 core g istration Ġas\n",
      "Ġwell Ġas Ġto ĠZ u Co Ġ1 . 0 . </s> Moreover , Ġthese Ġprevious Ġstudies Ġwere\n",
      "Ġable Ġto Ġshow Ġan Ġeffect Ġof Ġfixation Ġduration Ġon Ġthe Ġresulting ĠFR Ps .\n",
      "ĠTo Ġshow Ġthis Ġdependency Ġwe Ġfollowed Ġtwo Ġapproaches . ĠFirst , Ġfor Ġeach\n",
      "Ġreading Ġtask , Ġall Ġsingle - trial ĠFR Ps Ġwere Ġordered Ġby Ġfixation\n",
      "Ġduration Ġand Ġa Ġvertical Ġsliding Ġtime - window Ġwas Ġused Ġto Ġsmooth Ġthe\n",
      "Ġdata ĠB IB REF 13 . ĠFigure ĠFIG REF 25 Ġ( bottom ) Ġshows Ġthe Ġresulting\n",
      "Ġplots . ĠIn Ġline Ġwith Ġthis Ġprevious Ġwork , Ġa Ġfirst Ġposit ivation Ġcan\n",
      "Ġbe Ġidentified Ġat Ġ100 Ġms Ġpost - fix ation Ġonset . ĠA Ġsecond Ġpositive\n",
      "Ġpeak Ġis Ġlocated Ġdependent Ġon Ġthe Ġduration Ġof Ġthe Ġfixation , Ġwhich\n",
      "Ġcan Ġbe Ġexplained Ġby Ġthe Ġtime - j ittered Ġsucceeding Ġfixation . ĠThe\n",
      "Ġsecond Ġapproach Ġis Ġbased Ġon Ġhe nd erson 2013 co Ġin Ġwhich Ġsingle Ġtrial\n",
      "ĠEEG Ġsegments Ġare Ġclustered Ġby Ġthe Ġduration Ġof Ġthe Ġcurrent Ġfixation .\n",
      "ĠAs Ġshown Ġin ĠFigure ĠFIG REF 25 Ġ( top ), Ġwe Ġchose Ġfour Ġclusters Ġand\n",
      "Ġaveraged Ġthe Ġdata Ġwithin Ġeach Ġcluster Ġto Ġfour Ġdistinct ĠFR Ps ,\n",
      "Ġdepending Ġon Ġthe Ġfixation Ġduration . ĠAgain , Ġthe Ġsame Ġposit ivation\n",
      "Ġpeaks Ġbecome Ġapparent . ĠBoth Ġfindings Ġare Ġconsistent Ġwith Ġthe Ġprevious\n",
      "Ġwork Ġmentioned Ġand Ġwith Ġour Ġfindings Ġfrom ĠZ u Co Ġ1 . 0 . </s>\n",
      "Conclusion </s> We Ġpresented Ġa Ġnew , Ġfreely Ġavailable Ġcorpus Ġof Ġeye\n",
      "Ġmovement Ġand Ġelectrical Ġbrain Ġactivity Ġrecordings Ġduring Ġnatural\n",
      "Ġreading Ġas Ġwell Ġas Ġduring Ġannotation . ĠThis Ġis Ġthe Ġfirst Ġdataset\n",
      "Ġthat Ġallows Ġfor Ġthe Ġcomparison Ġbetween Ġthese Ġtwo Ġreading Ġparad ig ms .\n",
      "ĠWe Ġdescribed Ġthe Ġmaterials Ġand Ġexperiment Ġdesign Ġin Ġdetail Ġand\n",
      "Ġconducted Ġan Ġextensive Ġvalidation Ġto Ġensure Ġthe Ġquality Ġof Ġthe\n",
      "Ġrecorded Ġdata . ĠSince Ġthis Ġcorpus Ġis Ġtailored Ġto Ġcognitive ly -\n",
      "inspired ĠN LP , Ġthe Ġapplications Ġand Ġre - use Ġpotential s Ġof Ġthis Ġdata\n",
      "Ġare Ġextensive . ĠThe Ġprovided Ġword - level Ġand Ġsentence - level Ġeye -\n",
      "tracking Ġand ĠEEG Ġfeatures Ġcan Ġbe Ġused Ġto Ġimprove Ġand Ġevaluate ĠN LP\n",
      "Ġand Ġmachine Ġlearning Ġmethods , Ġfor Ġinstance , Ġto Ġevaluate Ġlinguistic\n",
      "Ġphenomena Ġin Ġneural Ġmodels Ġvia Ġpsych ol ingu istic Ġdata . ĠIn Ġaddition ,\n",
      "Ġbecause Ġthe Ġsentences Ġcontains Ġsemantic Ġrelation Ġlabels Ġand Ġthe\n",
      "Ġannotations Ġof Ġall Ġparticipants , Ġit Ġcan Ġalso Ġbe Ġwidely Ġused Ġfor\n",
      "Ġrelation Ġextraction Ġand Ġclassification . ĠFinally , Ġthe Ġtwo Ġcarefully\n",
      "Ġconstructed Ġreading Ġparad ig ms Ġallow Ġfor Ġthe Ġcomparison Ġbetween Ġnormal\n",
      "Ġreading Ġand Ġreading Ġduring Ġannotation , Ġwhich Ġcan Ġbe Ġrelevant Ġto\n",
      "Ġimprove Ġthe Ġmanual Ġlab elling Ġprocess Ġas Ġwell Ġas Ġthe Ġquality Ġof Ġthe\n",
      "Ġannotations Ġfor Ġsupervised Ġmachine Ġlearning .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_wrap(' '.join(instance['question_with_context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Did they experiment with this new dataset?\n",
      "ANSWER: No\n"
     ]
    }
   ],
   "source": [
    "print('QUESTION:', instance['metadata']['question'])\n",
    "print('ANSWER:', instance['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1912.00903'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance['metadata']['article_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qasper.evaluator import token_f1_score, get_answers_and_evidence, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 4697 to 5120 to be a multiple of `config.attention_window`: 1024\n",
      "/Users/ag2435/anaconda3/envs/arxiv-agent/lib/python3.12/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", namely one word, i.e. we are examining the effects of attention delay on\n"
     ]
    }
   ],
   "source": [
    "qasper_answer = qasper.predict(instance)[0]\n",
    "print(qasper_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_f1_score(qasper_answer, instance['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the researchers experimented with the new dataset, Zurich Cognitive\n",
      "Language Processing Corpus (ZuCo) 2.0. This corpus includes simultaneous eye\n",
      "movement and brain activity recordings during natural reading and task-specific\n",
      "reading during annotation. The dataset consists of physiological data from 18\n",
      "subjects reading 739 English sentences from Wikipedia. The researchers conducted\n",
      "a detailed technical validation of the data to ensure the quality of the\n",
      "recordings.\n",
      "\n",
      "\n",
      "The corpus construction involved recording data from 19 participants, with data\n",
      "from one participant discarded due to technical issues. The participants read\n",
      "sentences selected from the Wikipedia corpus, and the dataset included both\n",
      "normal reading and task-specific reading paradigms. The experimental design\n",
      "included participants reading sentences at their own speed, using a control pad\n",
      "to move to the next sentence and answer control questions.\n",
      "\n",
      "\n",
      "Data acquisition took place in a controlled environment, with eye-tracking data\n",
      "recorded using an infrared video-based eye tracker and EEG data recorded with a\n",
      "high-density EEG system. Preprocessing and feature extraction were performed on\n",
      "both eye-tracking and EEG data to extract relevant features for analysis.\n",
      "\n",
      "\n",
      "The data was technically validated by analyzing fixations, reading speed,\n",
      "omission rates, and other metrics on both sentence and word levels. The\n",
      "researchers compared the results to ZuCo 1.0 and previous studies to ensure\n",
      "consistency and reliability of the data. The validation included analyzing\n",
      "fixation-related potentials and the effects of fixation duration on EEG signals.\n",
      "\n",
      "\n",
      "In conclusion, the researchers successfully collected and validated the data\n",
      "from ZuCo 2.0, enabling comparisons between normal reading and task-specific\n",
      "reading during annotation. The dataset provides valuable physiological data for\n",
      "analyzing language processing and can be used to improve NLP applications and\n",
      "machine learning algorithms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt35_answer = gpt35.predict(instance)\n",
    "print_wrap(gpt35_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_f1_score(gpt35_answer, instance['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, predicted):\n",
    "    max_answer_f1s = []\n",
    "    max_evidence_f1s = []\n",
    "    max_answer_f1s_by_type = {\n",
    "        \"extractive\": [],\n",
    "        \"abstractive\": [],\n",
    "        \"boolean\": [],\n",
    "        \"none\": [],\n",
    "    }\n",
    "    num_missing_predictions = 0\n",
    "    for question_id in gold:\n",
    "        if question_id not in predicted:\n",
    "            num_missing_predictions += 1\n",
    "            max_answer_f1s.append(0.0)\n",
    "            max_evidence_f1s.append(0.0)\n",
    "            continue\n",
    "        answer_f1s_and_types = [\n",
    "            (token_f1_score(predicted[question_id][\"answer\"], reference[\"answer\"]),\n",
    "             reference[\"type\"])\n",
    "            for reference in gold[question_id]\n",
    "        ]\n",
    "        max_answer_f1, answer_type = sorted(answer_f1s_and_types, key=lambda x: x[0], reverse=True)[0]\n",
    "        max_answer_f1s.append(max_answer_f1)\n",
    "        max_answer_f1s_by_type[answer_type].append(max_answer_f1)\n",
    "        # evidence_f1s = [\n",
    "        #     paragraph_f1_score(predicted[question_id][\"evidence\"], reference[\"evidence\"])\n",
    "        #     for reference in gold[question_id]\n",
    "        # ]\n",
    "        # max_evidence_f1s.append(max(evidence_f1s))\n",
    "\n",
    "    mean = lambda x: sum(x) / len(x) if x else 0.0\n",
    "    return {\n",
    "        \"Answer F1\": mean(max_answer_f1s),\n",
    "        \"Answer F1 by type\": {key: mean(value) for key, value in max_answer_f1s_by_type.items()},\n",
    "        # \"Evidence F1\": mean(max_evidence_f1s),\n",
    "        \"Missing predictions\": num_missing_predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_data = json.load(open(args.gold))\n",
    "gold_answers_and_evidence = get_answers_and_evidence(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54', 'f5e6f43454332e0521a778db0b769481e23e7682', '9a05a5f4351db75da371f7ac12eb0b03607c4b87', '5eda469a8a77f028d0c5f1acd296111085614537', '18c5d366b1da8447b5404eab71f4cc658ba12e6f', 'b5e4866f0685299f1d7af267bbcc4afe2aab806f', '1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590', 'b6ae8e10c6a0d34c834f18f66ab730b670fb528c', 'a87a009c242d57c51fc94fe312af5e02070f898b', 'ef4dba073d24042f24886580ae77add5326f2130', '2df4a045a9cd7b44874340b6fdf9308d3c55327a', 'a313e98994fc039a82aa2447c411dda92c65a470', '37861be6aecd9242c4fdccdfcd06e48f3f1f8f81', '7e62a53823aba08bc26b2812db016f5ce6159565', '9eabb54c2408dac24f00f92cf1061258c7ea2e1a', '3d013f15796ae7fed5272183a166c45f16e24e39', '9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc', 'd3aa0449708cc861a51551b128d73e11d62207d2', 'cfbec1ef032ac968560a7c76dec70faf1269b27c', 'c0e341c4d2253eb42c8840381b082aae274eddad', '1ec152119cf756b16191b236c85522afeed11f59', '891c2001d6baaaf0da4e65b647402acac621a7d2', '66c96c297c2cffdf5013bab5e95b59101cb38655', '6b53e1f46ae4ba9b75117fc6e593abded89366be', 'c0bee6539eb6956a7347daa9d2419b367bd02064', '3de0487276bb5961586acc6e9f82934ef8cb668c', '113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab', '0752d71a0a1f73b3482a888313622ce9e9870d6e', '55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3', '4eaf9787f51cd7cdc45eb85cf223d752328c6ee4', 'fb2b536dc8e442dffab408db992b971e86548158', '31735ec3d83c40b79d11df5c34154849aeb3fb47', '10d450960907091f13e0be55f40bcb96f44dd074', 'b5608076d91450b0d295ad14c3e3a90d7e168d0e', 'c21b87c97d1afac85ece2450ee76d01c946de668', 'd087539e6a38c42f0a521ff2173ef42c0733878e', 'efe9bad55107a6be7704ed97ecce948a8ca7b1d2', '71e4ba4e87e6596aeca187127c0d088df6570c57', '7561a968470a8936d10e1ba722d2f38b5a9a4d38', '6d4400f45bd97b812e946b8a682b018826e841f1', '26c2e1eb12143d985e4fb50543cf0d1eb4395e67', 'f17ca24b135f9fe6bb25dc5084b13e1637ec7744', 'bd5bd1765362c2d972a762ca12675108754aa437', 'd9b6c61fc6d29ad399d27b931b6cb7b1117b314a', 'd27438b11bc70e706431dda0af2b1c0b0d209f96', '8d4ac4afbf5b14f412171729ceb5e822afcfa3f4', '3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f', '07d15501a599bae7eb4a9ead63e9df3d55b3dc35', '99e78c390932594bd833be0f5c890af5c605d808', '861187338c5ad445b9acddba8f2c7688785667b1', 'f161e6d5aecf8fae3a26374dcb3e4e1b40530c95', '12c50dea84f9a8845795fa8b8c1679328bd66246', '0810b43404686ddfe4ca84783477ae300fdd2ea4', '455d4ef8611f62b1361be4f6387b222858bb5e56', 'bc16ce6e9c61ae13d46970ebe6c4728a47f8f425', '1ff0fccf0dca95a6630380c84b0422bed854269a', '3d7d865e905295d11f1e85af5fa89b210e3e9fdf', '2ad4d3d222f5237ed97923640bc8e199409cbe52', '3fad42be0fb2052bb404b989cc7d58b440cd23a0', 'ee417fea65f9b1029455797671da0840c8c1abbe', 'ca5a82b54cb707c9b947aa8445aac51ea218b23a', 'da55bd769721b878dd17f07f124a37a0a165db02', 'feb448860918ef5b905bb25d7b855ba389117c1f', '4bc2784be43d599000cb71d31928908250d4cef3', '75df70ce7aa714ec4c6456d0c51f82a16227f2cb', '6424e442b34a576f904d9649d63acf1e4fdefdfc', '5eabfc6cc8aa8a99e6e42514ef9584569cb75dec', '887c6727e9f25ade61b4853a869fe712fe0b703d', '6236762b5631d9e395f81e1ebccc4bf3ab9b24ac', '31d695ba855d821d3e5cdb7bea638c7dbb7c87c7', 'b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab', 'a99fdd34422f4231442c220c97eafc26c76508dd', '2c78993524ca62bf1f525b60f2220a374d0e3535', 'd604f5fb114169f75f9a38fab18c1e866c5ac28b', '1d3e914d0890fc09311a70de0b20974bf7f0c9fe', '16535db1d73a9373ffe9d6eedaa2369cefd91ac4', 'de0b650022ad8693465242ded169313419eed7d9', '2b3cac7af10d358d4081083962d03ea2798cf622', '897ba53ef44f658c128125edd26abf605060fb13', '41ac23e32bf208b69414f4b687c4f324c6132464', 'e97186c51d4af490dba6faaf833d269c8256426c', '5bb3c27606c59d73fd6944ba7382096de4fa58d8', '8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b', '85590bb26fed01a802241bc537d85ba5ef1c6dc2', '75ff6e425ce304a35f18c0230c0d13d3913a31a9', '5cb610d3d5d7d447b4cd5736d6a7d8262140af58', 'c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a', 'b9d168da5321a7d7b812c52bb102a05210fe45bd', '0c234db3b380c27c4c70579a5d6948e1e3b24ff1', 'fa527becb8e2551f4fd2ae840dbd4a68971349e0', '32a3c248b928d4066ce00bbb0053534ee62596e7', 'c9b8d3858c112859eabee54248b874331c48f71b', '45e9533586199bde19313cd43b3d0ecadcaf7a33', 'd3dbb5c22ef204d85707d2d24284cc77fa816b6c', 'a5e49cdb91d9fd0ca625cc1ede236d3d4672403c', 'aefa333b2cf0a4000cd40566149816f5b36135e7', 'c5abe97625b9e1c8de8208e15d59c704a597b88c', 'eb2d5edcdfe18bd708348283f92a32294bb193a5', '88ab7811662157680144ed3fdd00939e36552672', 'cb196725edc9cdb2c54b72364f3bbf7c76471490', '286078813136943dfafb5155ee15d2429e7601d9', '8f16dc7d7be0d284069841e456ebb2c69575b32b', 'a7d020120a45c39bee624f65443e09b895c10533', '585626d18a20d304ae7df228c2128da542d248ff', 'bfc2dc913e7b78f3bd45e5449d71383d0aa4a890', '6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de', 'b46c0015a122ee5fb95c2a45691cb97f80de1bb6', '5b7a4994bfdbf8882f391adf1cd2218dbc2255a0', '9176d2ba1c638cdec334971c4c7f1bb959495a8e', '0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c', '5e324846a99a5573cd2e843d1657e87f4eb22fa6', '2ccc26e11df4eb26fcccdd1f446dc749aff5d572', 'f318a2851d7061f05a5b32b94251f943480fbd15', '6bbbb9933aab97ce2342200447c6322527427061', '2007bfb8f66e88a235c3a8d8c0a3b3dd88734706', 'd859cc37799a508bbbe4270ed291ca6394afce2c', '50e80cfa84200717921840fddcf3b051a9216ad8', 'b1bc9ae9d40e7065343c12f860a461c7c730a612', '63a1cbe66fd58ff0ead895a8bac1198c38c008aa', '509af1f11bd6f3db59284258e18fdfebe86cae47', '23e16c1173b7def2c5cb56053b57047c9971e3bb', 'd78f7f84a76a07b777d4092cb58161528ca3803c', '9da1e124d28b488b0d94998d32aa2fa8a5ebec51', '37be0d479480211291e068d0d3823ad0c13321d3', 'a3d9b101765048f4b61cbd3eaa2439582ebb5c77', '009ce6f2bea67e7df911b3f93443b23467c9f4a1', '55569d0a4586d20c01268a80a7e31a17a18198e2', '7cd22ca9e107d2b13a7cc94252aaa9007976b338', 'adbf33c6144b2f5c40d0c6a328a92687a476f371', 'f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24', 'a0543b4afda15ea47c1e623c7f00d4aaca045be0', '1591068b747c94f45b948e12edafe74b5e721047', '193ee49ae0f8827a6e67388a10da59e137e7769f', 'ed2eb4e54b641b7670ab5a7060c7b16c628699ab', 'beac555c4aea76c88f19db7cc901fa638765c250', '91e326fde8b0a538bc34d419541b5990d8aae14b', '044f922604b4b3f42ae381419fd5cd5624fa0637', 'f94b53db307685d572aefad52cd55f53d23769c2', 'aa7d327ef98f9f9847b447d4def04889b4508d7a', 'b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0', '551457ed34ca7fc0878c85bc664b135c21059b58', '0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8', '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'a4d115220438c0ded06a91ad62337061389a6747', '2c7e94a65f5f532aa31d3e538dcab0468a43b264', '149da739b1c19a157880d9d4827f0b692006aa2c', '27de1d499348e17fec324d0ef00361a490659988', 'cfcdd73e712caf552ba44d0aa264d8dace65a589', '23b2901264bda91045258b5d4120879ae292e950', 'b5bc34e1e381dbf972d0b594fe8c66ff75305d71', '72f7ef55e150e16dcf97fe443aff9971a32414ef', '20e38438471266ce021817c6364f6a46d01564f2', '28067da818e3f61f8b5152c0d42a531bf0f987d4', 'bf3b27a4f4be1f9ae31319877fd0c75c03126fd5', 'ffa7f91d6406da11ddf415ef094aaf28f3c3872d', 'b634ff1607ce5756655e61b9a6f18bc736f84c83', '2f901dab6b757e12763b23ae8b37ae2e517a2271', 'b591853e938984e6069d738371500ebdec50d256', 'a130306c6662ff489df13fb3f8faa7cba8c52a21', 'b1cf5739467ba90059add58d11b73d075a11ec86', '2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd', '4f253dfced6a749bf57a1b4984dc962ce9550184', 'dc1cec824507fc85ac1ba87882fe1e422ff6cffb', 'f428618ca9c017e0c9c2a23515dab30a7660f65f', '8ce11515634236165cdb06ba80b9a36a8b9099a2', '6024039bbd1118c5dab86c41cce1175d99f10a25', 'de5b6c25e35b3a6c5e40e350fc5e52c160b33490', 'b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f', '6bfba3ddca5101ed15256fca75fcdc95a53cece7', 'df5a4505edccc0ee11349ed6e7958cf6b84c9ed4', 'fd753ab5177d7bd27db0e0afc12411876ee607df', '88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42', '4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3', '8b3d3953454c88bde88181897a7a2c0c8dd87e23', '784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f', '7705dd04acedaefee30d8b2c9978537afb2040dc', '44497509fdf5e87cff05cdcbe254fbd288d857ad', '0ee73909ac638903da4a0e5565c8571fc794ab96', '1f07e837574519f2b696f3d6fa3230af0b931e5d', '5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76', '729694a9fe1e05d329b7a4078a596fe606bc5a95', '1c997c268c68149ae6fb43d83ffcd53f0e7fe57e', '5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde', 'f9bf6bef946012dd42835bf0c547c0de9c1d229f', '6a633811019e9323dc8549ad540550d27aa6d972', '6b9b9e5d154cb963f6d921093539490daa5ebbae', 'bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2', 'd46c0ea1ba68c649cc64d2ebb6af20202a74a3c7', '6844683935d0d8f588fa06530f5068bf3e1ed0c0', '8acab64ba72831633e8cc174d5469afecccf3ae9', '53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa', '72755c2d79210857cfff60bfbcb55f83c71ada51', '7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569', 'bd6dc38a9ac8d329114172194b0820766458dacc', '3ddff6b707767c3dd54d7104fe88b628765cae58', '0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7', '06be47e2f50b902b05ebf1ff1c66051925f5c247', '003d6f9722ddc2ee13e879fefafc315fb8e87cb9', 'c88a846197b72d25e04ec55f00ee3e72f655504c', '4d28c99750095763c81bcd5544491a0ba51d9070', '78292bc57ee68fdb93ed45430d80acca25a9e916', '443d2448136364235389039cbead07e80922ec5c', 'aa6d956c2860f58fc9baea74c353c9d985b05605', '4c18081ae3b676cc7831403d11bc070c10120f8e', 'fb3d30d59ed49e87f63d3735b876d45c4c6b8939', '197b276d0610ebfacd57ab46b0b29f3033c96a40', 'e025061e199b121f2ac8f3d9637d9bf987d65cd5', '61652a3da85196564401d616d251084a25ab4596', '14b74ad5a6f5b0506511c9b454e9c464371ef8c4', '5f2bade0881c719ab026bc2e2962e2ada96cdb25', '5c88d601e8fca96bffebfa9ef22331ecf31c6d75', '71bd5db79635d48a0730163a9f2e8ef19a86cd66', '9ecde59ffab3c57ec54591c3c7826a9188b2b270', '005cca3c8ab6c3a166e315547a2259020f318ffb', 'bcc0cd4e262f2db4270429ab520971bcf39414cf', 'f641f561ad2ea2794a52e4e4bdd62e1f353ab797', 'af34051bf3e628c1e2a00b110bb84e5f018b419f', '022c365a14fdec406c7a945a1a18e7e79df37f08', '5260cb56b7d127772425583c5c28958c37cb9bea', '9b97805a0c093df405391a85e4d3ab447671c86a', '38f58f13c7f23442d5952c8caf126073a477bac0', '7ee5c45b127fb284a4a9e72bb9b980a602f7445a', 'ddf5e1f600b9ce2e8f63213982ef4209bab01fd8', '27275fe9f6a9004639f9ac33c3a5767fea388a98', 'ef3567ce7301b28e34377e7b62c4ec9b496f00bf', '7595260c5747aede0b32b7414e13899869209506', 'c2d1387e08cf25cb6b1f482178cca58030e85b70', '5a22293b055f5775081d6acdc0450f7bd5f5de04', '03c967763e51ef2537793db7902e2c9c17e43e95', '26327ccebc620a73ba37a95aabe968864e3392b2', 'ababb79dd3c301f4541beafa181f6a6726839a10', 'c2b8ee872b99f698b3d2082d57f9408a91e1b4c1', '8eefa116e3c3d3db751423cc4095d1c4153d3a5f', '133eb4aa4394758be5f41744c60c99901b2bc01c', '3fff37b9f68697d080dbd9d9008a63907137644e', 'a778b8204a415b295f73b93623d09599f242f202', '642e8cf1d39faa1cd985d16750cdc6696c52db2f', '493e971ee3f57a821ef1f67ef3cd47ade154e7c4', '8dd8e5599fc56562f2acbc16dd8544689cddd938', 'abe2393415e533cb06311e74ed1c5674cff8571f', '00c57e45ac6afbdfa67350a57e81b4fad0ed2885', '22714f6cad2d5c54c28823e7285dc85e8d6bc109', '82642d3111287abf736b781043d49536fe48c350', '5a81732d52f64e81f1f83e8fd3514251227efbc7', '9a8b9ea3176d30da2453cac6e9347737c729a538', '4477bb513d56e57732fba126944073d414d1f75f', '1b23c4535a6c10eb70bbc95313c465e4a547db5e', '0a75a52450ed866df3a304077769e1725a995bb7', 'fd0a3e9c210163a55d3ed791e95ae3875184b8f8', 'c37f65c9f0d543a35c784263b79236ccf1c44fac', '584af673429c7f8621c6bf83362a37048daa0e5d', '1be54c5b3ea67d837ffba2290a40c1e720d9587f', 'b08f88d1facefceb87e134ba2c1fa90035018e83', 'b06512c17d99f9339ffdab12cedbc63501ff527e', 'fd8e23947095fe2230ffe1a478945829b09c8c95', '8bf7f1f93d0a2816234d36395ab40c481be9a0e0', '3611a72f754de1e256fbd25b012197e1c24e8470', '4c07c33dfaf4f3e6db55e377da6fa69825d0ba15', 'b1ce129678e37070e69f01332f1a8587e18e06b0', '7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1', '0689904db9b00a814e3109fb1698086370a28fa2', 'cc354c952b5aaed2d4d1e932175e008ff2d801dd', '0f12dc077fe8e5b95ca9163cea1dd17195c96929', '2ddb51b03163d309434ee403fef42d6b9aecc458', 'e587559f5ab6e42f7d981372ee34aebdc92b646e', 'f68508adef6f4bcdc0cc0a3ce9afc9a2b6333cc5', '5563a3538d311c979c2fb83c1cc9afc66ff6fffc', 'ceb4bac1f6c506b5dab37607354530faee02ccac', '91bc8c0bc1634045177065536dd311f89134630b', 'fe1dcd6ef1f8618bbceee418f07cafe63a8efe08', '53f74250948015c394e7b8438a2041fdeb330911', 'bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a', '7b4fb6da74e6bd1baea556788a02969134cf0800', 'bc31a3d2f7c608df8c019a64d64cb0ccc5669210', 'f67b9bda14ec70feba2e0d10c400b2b2025a0a6a', '1cfed6b0c9b5a079a51166209649a987e7553e4e', '761de1610e934189850e8fda707dc5239dd58092', 'f8da63df16c4c42093e5778c01a8e7e9b270142e', 'c09a92e25e6a81369fcc4ae6045491f2690ccc10', '63c3550c6fb42f41a0c93133e9fca12ac00df9b3', '603fee7314fa65261812157ddfc2c544277fcf90', '09a1173e971e0fcdbf2fbecb1b077158ab08f497', '70e9210fe64f8d71334e5107732d764332a81cb1', '051df74dc643498e95d16e58851701628fdfd43e', '33554065284110859a8ea3ca7346474ab2cab100', '57f23dfc264feb62f45d9a9e24c60bd73d7fe563', '54830abe73fef4e629a36866ceeeca10214bd2c8', '2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6', 'ef7212075e80bf35b7889dc8dd52fcbae0d1400a', '567dc9bad8428ea9a2658c88203a0ed0f8da0dc3', 'd51dc36fbf6518226b8e45d4c817e07e8f642003', 'd8627ba08b7342e473b8a2b560baa8cdbae3c7fd', 'cb77d6a74065cb05318faf57e7ceca05e126a80d', '8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d', 'a1b3e2107302c5a993baafbe177684ae88d6f505', 'bb2de20ee5937da7e3e6230e942bec7b6e8f61ee', '1170e4ee76fa202cabac9f621e8fbeb4a6c5f094', '1462eb312944926469e7cee067dfc7f1267a2a8c', 'f59f1f5b528a2eec5cfb1e49c87699e0c536cc45', '9bd080bb2a089410fd7ace82e91711136116af6c', '6d1217b3d9cfb04be7fcd2238666fa02855ce9c5', '1e775cf30784e6b1c2b573294a82e145a3f959bb', '392fb87564c4f45d0d8d491a9bb217c4fce87f03', '203337c15bd1ee05763c748391d295a1f6415b9b', 'd004ca2e999940ac5c1576046e30efa3059832fa', '21548433abd21346659505296fb0576e78287a74', 'f0b2289cb887740f9255909018f400f028b1ef26', '51b1142c1d23420dbf6d49446730b0e82b32137c', '58355e2a782bf145b61ee2a3e0e426119985c179', '25c1c4a91f5dedd4e06d14121af3b5921db125e9', 'f88036174b4a0dbf4fe70ddad884d16082c5748d', 'a267d620af319b48e56c191aa4c433ea3870f6fb', '899ed05c460bf2aa0aa65101cad1986d4f622652', 'd53299fac8c94bd0179968eb868506124af407d1', '29f2954098f055fb19d9502572f085862d75bf61', '6bf93968110c6e3e3640360440607744007a5228', '37a79be0148e1751ffb2daabe4c8ec6680036106', '518dae6f936882152c162058895db4eca815e649', 'e44a6bf67ce3fde0c6608b150030e44d87eb25e3', '6a31db1aca57a818f36bba9002561724655372a7', 'e330e162ec29722f5ec9f83853d129c9e0693d65', 'd3093062aebff475b4deab90815004051e802aa6', '4944cd597b836b62616a4e37c045ce48de8c82ca', 'a29c071065d26e5ee3c3bcd877e7f215c59d1d33', '7f207549c75f5c4388efc15ed28822672b845663', '596aede2b311deb8cb0a82d2e7de314ef6e83e4e', '2e89ebd2e4008c67bb2413699589ee55f59c4f36', 'e2db361ae9ad9dbaa9a85736c5593eb3a471983d', '252a645af9876241fb166e5822992ce17fec6eb6', 'ed67359889cf61fa11ee291d6c378cccf83d599d', '425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82', '955de9f7412ba98a0c91998919fa048d339b1d48', '3b371ea554fa6639c76a364060258454e4b931d4', 'ddb23a71113cbc092cbc158066d891cae261e2c6', 'e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38', 'c7486d039304ca9d50d0571236429f4f6fbcfcf7', 'f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9', 'a103636c8d1dbfa53341133aeb751ffec269415c', '55139fcfe04ce90aad407e2e5a0067a45f31e07e', 'fbaf060004f196a286fef67593d2d76826f0304e', '7ae38f51243cb80b16a1df14872b72a1f8a2048f', 'deb89bca0925657e0f91ab5daca78b9e548de2bd', '9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570', 'e6583c60b13b87fc37af75ffc975e7e316d4f4e0', 'c7b6e6cb997de1660fd24d31759fe6bb21c7863f', 'f9f59c171531c452bd2767dc332dc74cadee5120', '4ac2c3c259024d7cd8e449600b499f93332dab60', 'bc730e4d964b6a66656078e2da130310142ab641', '3941401a182a3d6234894a5c8a75d48c6116c45c', '67e9e147b2cab5ba43572ce8a17fc863690172f0', 'a74190189a6ced2a2d5b781e445e36f4e527e82a', '43f074bacabd0a355b4e0f91a1afd538c0a6244f', '58ef2442450c392bfc55c4dc35f216542f5f2dbb', '78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d', '375b281e7441547ba284068326dd834216e55c07', '05c49b9f84772e6df41f530d86c1f7a1da6aa489', '6ecb69360449bb9915ac73c0a816c8ac479cbbfc', '68df324e5fa697baed25c761d0be4c528f7f5cf7', '77c34f1033702278f7f044806c1eba0c6ecb8b04', '2ee715c7c6289669f11a79743a6b2b696073805d', '61a9ea36ddc37c60d1a51dabcfff9445a2225725', 'cc850bc8245a7ae790e1f59014371d4f35cd46d7', '984fc3e726848f8f13dfe72b89e3770d00c3a1af', 'fb1227b3681c69f60eb0539e16c5a8cd784177a7', '8df35c24af9efc3348d3b8d746df116480dfe661', '277a7e916e65dfefd44d2d05774f95257ac946ae', '2916bbdb95ef31ab26527ba67961cf5ec94d6afe', 'f2e8497aa16327aa297a7f9f7d156e485fe33945', '9b76f428b7c8c9fc930aa88ee585a03478bff9b3', 'dd6b378d89c05058e8f49e48fd48f5c458ea2ebc', 'e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851', 'c00ce1e3be14610fb4e1f0614005911bb5ff0302', '71fe5822d9fccb1cb391c11283b223dc8aa1640c', '97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3', '1062a0506c3691a93bb914171c2701d2ae9621cb', '8e12b5c459fa963b3e549deadb864c244879fe82', '483a699563efcb8804e1861b18809279f21c7610', 'd3ff2986ca8cb85a9a5cec039c266df756947b43', '3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4', '2317ca8d475b01f6632537b95895608dc40c4415', '3e88fb3d28593309a307eb97e875575644a01463', '0767ca8ff1424f7a811222ca108a33b6411aaa8a', 'e8f969ffd637b82d04d3be28c51f0f3ca6b3883e', '46227b4265f1d300a5ed71bf40822829de662bc2', 'a6a48de63c1928238b37c2a01c924b852fe752f8', 'b65a83a24fc66728451bb063cf6ec50134c8bfb0', '8c852fc29bda014d28c3ee5b5a7e449ab9152d35', '682e26262abba473412f68cbeb5f69aa3b9968d7', '5daeb8d4d6f3b8543ec6309a7a35523e160437eb', '74fb77a624ea9f1821f58935a52cca3086bb0981', 'd015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751', '55bd59076a49b19d3283af41c5e3ccb875f3eb0c', '521280a87c43fcdf9f577da235e7072a23f0673e', '5a8cc8f80509ea77d8213ed28c5ead501c68c725', '290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30', '1b72aa2ec3ce02131e60626639f0cf2056ec23ca', 'c49ee6ac4dc812ff84d255886fd5aff794f53c39', '3f856097be2246bde8244add838e83a2c793bd17', 'bf52c01bf82612d0c7bbf2e6a5bb2570c322936f', '74e866137b3452ec50fb6feaf5753c8637459e62', '184b0082e10ce191940c1d24785b631828a9f714', 'c59078efa7249acfb9043717237c96ae762c0a8c', '73bddaaf601a4f944a3182ca0f4de85a19cdc1d2', 'd4e5e3f37679ff68914b55334e822ea18e60a6cf', '5f60defb546f35d25a094ff34781cddd4119e400', '90d946ccc3abf494890e147dd85bd489b8f3f0e8', 'b962cc817a4baf6c56150f0d97097f18ad6cd9ed', 'fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f', '52f8a3e3cd5d42126b5307adc740b71510a6bdf5', '2236386729105f5cf42f73cc055ce3acdea2d452', '18942ab8c365955da3fd8fc901dfb1a3b65c1be1', '7b4992e2d26577246a16ac0d1efc995ab4695d24', 'ab9b0bde6113ffef8eb1c39919d21e5913a05081', '9a9d225f9ac35ed35ea02f554f6056af3b42471d', 'ea56148a8356a1918bedcf0a99ae667c27792cfe', 'cd32a38e0f33b137ab590e1677e8fb073724df7f', '2c6b50877133a499502feb79a682f4023ddab63e', 'f651cd144b7749e82aa1374779700812f64c8799', '4625cfba3083346a96e573af5464bc26c34ec943', '326588b1de9ba0fd049ab37c907e6e5413e14acd', 'ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb', '55507f066073b29c1736b684c09c045064053ba9', 'e838275bb0673fba0d67ac00e4307944a2c17be3', '8dda1ef371933811e2a25a286529c31623cca0c6', 'b3de9357c569fb1454be8f2ac5fcecaea295b967', '59e58c6fc63cf5b54b632462465bfbd85b1bf3dd', '5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83', '3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f', '14b8ae5656e7d4ee02237288372d9e682b24fdb8', 'e3a2d8886f03e78ed5e138df870f48635875727e', '62f27fe08ddb67f16857fab2a8a721926ecbb6fb', '9ca447c8959a693a3f7bdd0a2c516f4b86f95718', '05887a8466e0a2f0df4d6a5ffc5815acd7d9066a', 'c87fcc98625e82fdb494ff0f5309319620d69040', '500a8ec1c56502529d6e59ba6424331f797f31f0', 'ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc', 'f2155dc4aeab86bf31a838c8ff388c85440fce6e', 'ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc', '4d706ce5bde82caf40241f5b78338ea5ee5eb01e', '86bf75245358f17e35fc133e46a92439ac86d472', '9132d56e26844dc13b3355448d0f14b95bd2178a', 'f3c204723da53c7c8ef4dc1018ffbee545e81056', '0602a974a879e6eae223cdf048410b5a0111665e', '56b034c303983b2e276ed6518d6b080f7b8abe6a', '15e481e668114e4afe0c78eefb716ffe1646b494', '3d7a982c718ea6bc7e770d8c5da564fbb9d11951', '692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1', '935d6a6187e6a0c9c0da8e53a42697f853f5c248', '3b77b4defc8a139992bd0b07b5cf718382cb1a5f', '01a41c0a4a7365cd37d28690735114f2ff5229f2', 'cd2878c5a52542ddf080b20bec005d9a74f2d916', 'fd2c6c26fd0ab3c10aae4f2550c5391576a77491', '6b6d498546f856ac20958f666fc3fd55811347e2', 'de3b1145cb4111ea2d4e113f816b537d052d9814', '132f752169adf6dc5ade3e4ca773c11044985da4', '1d9aeeaa6efa1367c22be0718f5a5635a73844bd', '012b8a89aea27485797373adbcda32f16f9d7b54', 'c598028815066089cc1e131b96d6966d2610467a', 'ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed', '0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50', '92dfacbbfa732ecea006e251be415a6f89fb4ec6', 'c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9', '307e8ab37b67202fe22aedd9a98d9d06aaa169c5', '6415f38a06c2f99e8627e8ba6251aa4b364ade2d', 'e5c8e9e54e77960c8c26e8e238168a603fcdfcc6', '50be4a737dc0951b35d139f51075011095d77f2a', '6becff2967fe7c5256fe0b00231765be5b9db9f1', '76121e359dfe3f16c2a352bd35f28005f2a40da3', '02428a8fec9788f6dc3a86b5d5f3aa679935678d', '7793805982354947ea9fc742411bec314a6998f6', '007b13f05d234d37966d1aa7d85b5fd78564ff45', '2ceced87af4c8fdebf2dc959aa700a5c95bd518f', '72ed5fed07ace5e3ffe9de6c313625705bc8f0c7', '2e37e681942e28b5b05639baaff4cd5129adb5fb', 'b49598b05358117ab1471b8ebd0b042d2f04b2a4', '932b39fd6c47c6a880621a62e6a978491d881d60', 'b36f867fcda5ad62c46d23513369337352aa01d2', 'c6a0b9b5dabcefda0233320dd1548518a0ae758e', '1e185a3b8cac1da939427b55bf1ba7e768c5dae4', '26e2d4d0e482e6963a76760323b8e1c26b6eee91', 'b80a3fbeb49a8968e149955bdcf199556478eeff', 'badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9', '67b66fe67a3cb2ce043070513664203e564bdcbd', 'f56d07f73b31a9c72ea737b40103d7004ef6a079', '38e4aaeabf06a63a067b272f8950116733a7895c', '1d197cbcac7b3f4015416f0152a6692e881ada6c', '92294820ac0d9421f086139e816354970f066d8a', '477d9d3376af4d938bb01280fe48d9ae7c9cf7f7', 'f225a9f923e4cdd836dd8fe097848da06ec3e0cc', 'ff338921e34c15baf1eae0074938bf79ee65fdd2', 'e807d347742b2799bc347c0eff19b4c270449fee', '31b92c03d5b9be96abcc1d588d10651703aff716', '9ec1f88ceec84a10dc070ba70e90a792fba8ce71', '384bf1f55c34b36cb03f916f50bbefade6c86a75', 'aef607d2ac46024be17b1ddd0ed3f13378c563a6', '93beae291b455e5d3ecea6ac73b83632a3ae7ec7', '6c91d44d5334a4ac80100eead4e105d34e99a284', 'a69a59b6c0ab27bcee1a780d6867df21e30aec08', 'b3d01ac226ee979e188a4141877a6d2a5482de98', 'af5730d82535464cedfa707a03415ac2e7a21295', 'ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2', 'b249b60a8c94d0e40d65f1ffdfcac527dab57516', '0f567251a6566f65170a1329eeeb5105932036b2', '4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f', '60ce4868af45753c9e124e64e518c32376f12694', '1b1a30e9e68a9ae76af467e60cefb180d135e285', '2c85865a65acd429508f50b5e4db9674813d67f2', '73a7acf33b26f5e9475ee975ba00d14fd06f170f', 'dd53baf26dad3d74872f2d8956c9119a27269bd5', '218bc82796eb8d91611996979a4a42500131a936', 'b21bc09193699dc9cfad523f3d5542b0b2ff1b8e', '352bc6de5c5068c6c19062bad1b8f644919b1145', 'd667731ea20605580c398a1224a0094d1155ebbb', '8bb0011ad1d63996d5650770f3be18abdd9f7fc6', 'b0dbe75047310fec4d4ce787be5c32935fc4e37b', 'd64383e39357bd4177b49c02eb48e12ba7ffd4fb', '52f9cd05d8312ae3c7a43689804bac63f7cac34b', 'dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596', '955cbea7e5ead36fb89cd6229a97ccb3febcf8bc', '04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39', '15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8', 'ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797', '6ca938324dc7e1742a840d0a54dc13cc207394a1', '4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802', '4d47bef19afd70c10bbceafd1846516546641a2f', '506d21501d54a12d0c9fd3dbbf19067802439a04', '701571680724c05ca70c11bc267fb1160ea1460a', '600b097475b30480407ce1de81c28c54a0b3b2f8', 'ee7e9a948ee6888aa5830b1a3d0d148ff656d864', '5fda8539a97828e188ba26aad5cda1b9dd642bc8', '709feae853ec0362d4e883db8af41620da0677fe', '186b7978ee33b563a37139adff1da7d51a60f581', 'fabcd71644bb63559d34b38d78f6ef87c256d475', 'da9c0637623885afaf023a319beee87898948fe9', '8a1c0ef69b6022a0642ca131a8eacb5c97016640', '48088a842f7a433d3290eb45eb0d4c6ab1d8f13c', '4907096cf16d506937e592c50ae63b642da49052', '8748e8f64af57560d124c7b518b853bf2711c13e', '893ec40b678a72760b6802f6abf73b8f487ae639', 'c81f215d457bdb913a5bade2b4283f19c4ee826c', 'e101e38efaa4b931f7dd75757caacdc945bb32b4', 'afb77b11da41cd0edcaa496d3f634d18e48d7168', '41b2355766a4260f41b477419d44c3fd37f3547d', '96a4091f681872e6d98d0efee777d9e820cb8dae', '81a35b9572c9d574a30cc2164f47750716157fc8', 'f4496316ddd35ee2f0ccc6475d73a66abf87b611', 'e8a32460fba149003566969f92ab5dd94a8754a4', '2a6003a74d051d0ebbe62e8883533a5f5e55078b', '1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b', '9c44df7503720709eac933a15569e5761b378046', 'b7381927764536bd97b099b6a172708125364954', 'df95b3cb6aa0187655fd4856ae2b1f503d533583', 'f7ed3b9ed469ed34f46acde86b8a066c52ecf430', 'c7eb71683f53ab7acffd691a36cad6edc7f5522e', '17a1eff7993c47c54eddc7344e7454fbe64191cd', 'a5e5cda1f6195ab1336855f1e39a609d61326d62', '32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9', 'eda4869c67fe8bbf83db632275f053e7e0241e8c', '2c7494d47b2a69f182e83455fe4c75ae3b2893e9', '4d7ff4e5d06902de85b0e9a364dc455196d06a7d', 'ecc63972b2783ee39b3e522653cfb6dc5917d522', '8d074aabf4f51c8455618c5bf7689d3f62c4da1d', 'fe2666ace293b4bfac3182db6d0c6f03ea799277', '70a1b0f9f26f1b82c14783f1b76dfb5400444aa4', 'd3ca5f1814860a88ff30761fec3d860d35e39167', 'dd20d93166c14f1e57644cd7fa7b5e5738025cd0', 'dc2a2c177cd5df6da5d03e6e74262bf424850ec9', 'ae90c5567746fe25af2fcea0cc5f355751e05c71', 'd7644c674887ca9708eb12107acd964ae53b216d', 'a3bb9a936f61bafb509fa12ac0a61f91abcc5106', 'df6d327e176740da9edcc111a06374c54c8e809c', '49764eee7fb523a6a28375cc699f5e0220b81766', '3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1', 'bb3267c3f0a12d8014d51105de5d81686afe5f1b', '114934e1a1e818630ff33ac5c4cd4be6c6f75bb2', '2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72', 'b8d0e4e0e820753ffc107c1847fe1dfd48883989', '5aa12b4063d6182a71870c98e4e1815ff3dc8a72', '22815878083ebd2f9e08bc33a5e733063dac7a0f', '220d11a03897d85af91ec88a9b502815c7d2b6f3', 'd509081673f5667060400eb325a8050fa5db7cc8', 'c2e475adeddcdc4d637ef0d4f5065b6a9b299827', 'cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5', '6cd25c637c6b772ce29e8ee81571e8694549c5ab', '1088255980541382a2aa2c0319427702172bbf84', '0d9fcc715dee0ec85132b3f4a730d7687b6a06f4', '8910ee2236a497c92324bbbc77c596dba39efe46', '2c59528b6bc5b5dc28a7b69b33594b274908cca6', '6b367775a081f4d2423dc756c9b65b6eef350345', 'bc01853512eb3c11528e33003ceb233d7c1d7038', '67ec8ef85844e01746c13627090dc2706bb2a4f3', 'ba539cab80d25c3e20f39644415ed48b9e4e4185', '6bf5620f295b5243230bc97b340fae6e92304595', '4986f420884f917d1f60d3cea04dc8e64d3b5bf1', '747b847d687f703cc20a87877c5b138f26ff137d', '111afb77cfbf4c98e0458606378fa63a0e965e36', '6568a31241167f618ef5ede939053feaa2fb0d7e', '50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed', '0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c', '4dc268e3d482e504ca80d2ab514e68fd9b1c3af1', 'ab54cd2dc83141bad3cb3628b3f0feee9169a556', '249c805ee6f2ebe4dbc972126b3d82fb09fa3556', 'b4f881331b975e6e4cab1868267211ed729d782d', '79413ff5d98957c31866f22179283902650b5bb6', '29c014baf99fb9f40b5171aab3e2c7f12a748f79', '09c86ef78e567033b725fc56b85c5d2602c1a7c3', 'd67c01d9b689c052045f3de1b0918bab18c3f174', 'e5bc73974c79d96eee2b688e578a9de1d0eb38fd', '2cd37743bcc7ea3bd405ce6d91e79e5339d7642e', 'eac9dae3492e17bc49c842fb566f464ff18c049b', '7697baf8d8d582c1f664a614f6332121061f87db', '1cb100182508cf55b3509283c0e2bbcd527d625e', '206739417251064b910ae9e5ff096e867ee10fb8', 'd6401cece55a14d2a35ba797a0878dfe2deabedc', 'ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff', 'fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c', 'b1a068c1050e2bed12d5c9550c73e59cd5b1f78d', 'f9edd8f9c13b54d8b1253ed30e7decc1999602da', 'd93c0e78a3fe890cd534a11276e934be68583f4b', '30af1926559079f59b0df055da76a3a34df8336f', 'ceb767e33fde4b927e730f893db5ece947ffb0d8', 'c2cb6c4500d9e02fc9a1bdffd22c3df69655189f', 'c571deefe93f0a41b60f9886db119947648e967c', '06eb9f2320451df83e27362c22eb02f4a426a018', 'e54257585cc75564341eb02bdc63ff8111992f82', '2a3e36c220e7b47c1b652511a4fdd7238a74a68f', '9658b5ffb5c56e5a48a3fea0342ad8fc99741908', '46c9e5f335b2927db995a55a18b7c7621fd3d051', 'ce0e2a8675055a5468c4c54dbb099cfd743df8a7', '3a6e843c6c81244c14730295cfb8b865cd7ede46', 'fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf', '1beb4a590fa6127a138f4ed1dd13d5d51cc96809', '5c5aeee83ea3b34f5936404f5855ccb9869356c1', 'f8c1b17d265a61502347c9a937269b38fc3fcab1', '5913930ce597513299e4b630df5e5153f3618038', '81d193672090295e687bc4f4ac1b7a9c76ea35df', 'cf171fad0bea5ab985c53d11e48e7883c23cdc44', '2a564b092916f2fabbfe893cf13de169945ef2e1', '0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d', '73e83c54251f6a07744413ac8b8bed6480b2294f', '3355918bbdccac644afe441f085d0ffbbad565d7', 'e48e750743aef36529fbea4328b8253dbe928b4d', 'c08aab979dcdc8f4fe8ec1337c3c8290ab13414e', '8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f', 'cc608df2884e1e82679f663ed9d9d67a4b6c03f3', '3e432d71512ffbd790a482c716e7079ee78ce732', 'dd76130ec5fac477123fe8880472d03fbafddef6', '43eecc576348411b0634611c81589f618cd4fddf', '79f9468e011670993fd162543d1a4b3dd811ac5d', 'c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf', '902b3123aec0f3a39319ffa9d05ab8e08a2eb567', '1038542243efe5ab3e65c89385e53c4831cd9981', 'e2b0cd30cf56a4b13f96426489367024310c3a05', 'e831041d50f3922265330fcbee5a980d0e2586dd', '7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93', 'ac7f6497be4bcca64e75f28934b207c9e8097576', '87bb3105e03ed6ac5abfde0a7ca9b8de8985663c', 'd9980676a83295dda37c20cfd5d58e574d0a4859', '9225b651e0fed28d4b6261a9f6b443b52597e401', '565189b672efee01d22f4fc6b73cd5287b2ee72c', 'b6f7fadaa1bb828530c2d6780289f12740229d84', '7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613', '4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f', '6c96e910bd98c9fd58ba2050f99b9c9bac69840a', '9af3142630b350c93875441e1e1767312df76d17', 'e374169ee10f835f660ab8403a5701114586f167', '82595ca5d11e541ed0c3353b41e8698af40a479b', 'd4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f', '53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e', '869feb7f47606105005efdb6bea1c549824baea0', 'c497e8701060583d91bb64b9f9202d40047effc4', '8060a773f6a136944f7b59758d08cc6f2a59693b', '1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306', 'c0af8b7bf52dc15e0b33704822c4a34077e09cd1', '9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e', 'e0122fc7b0143d5cbcda2120be87a012fb987627', '5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4', '37edc25e39515ffc2d92115d2fcd9e6ceb18898b', 'e431661f17347607c3d3d9764928385a8f3d9650', '876700622bd6811d903e65314ac75971bbe23dcc', '312e9cc11b9036a6324bdcb64eca6814053ffa17', '1c0ba6958da09411deded4a14dfea5be55687619', '1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b', 'd915b401bb96c9f104a0353bef9254672e6f5a47', '79a44a68bb57b375d8a57a0a7f522d33476d9f33', '664db503509b8236bc4d3dc39cebb74498365750', '64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8', 'b0a18628289146472aa42f992d0db85c200ec64b', '72ce05546c81ada05885026470f4c8c218805055', 'f20a389ace2267aa61eddcc235535452ccdae0e6', '6411622cc8b2fbedbfa468859d453596d3bd2f03', 'fc77d70c305fa80447b191248aba93da63ac3704', '5b551ba47d582f2e6467b1b91a8d4d6a30c343ec', '3cf1edfa6d53a236cf4258afd87c87c0a477e243', '9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa', '34dc0838632d643f33c8dbfe7bd4b656586582a2', 'c77359fb9d3ef96965a9af0396b101f82a0a9de6', '1bdc990c7e948724ab04e70867675a334fdd3051', '78536da059b884d6ad04680baeb894895458055c', '96b07373756d7854bccc3c12e8d41454ab8741f5', '511517efc96edcd3e91e7783821c9d6d5a6562af', '9122de265577e8f6b5160cd7d28be9e22da752b2', 'e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d', '45be665a4504f0c7f458cf3f75a95d5a75eefd42', '22b740cc3c8598247ee102279f96575bdb10d53f', '74b4779de437c697fe702e51f23e2b0538b0f631', '435570723b37ee1f5898c1a34ef86a0b2e8701bb', 'aa2948209cc33b071dbf294822e72bb136678345', 'd9412dda3279729e95fcb35cbed09e61577a896e', '41b70699514703820435b00efbc3aac4dd67560a', 'e3c9e4bc7bb93461856e1f4354f33010bc7d28d5', '06cc8fcafc0880cf69a2514bb7341642b9833041', 'd650101712e36594bd77b45930a990402a455222', 'cb384dc5366b693f28680374d31ff45356af0461', 'd41e20ec716b5904a272938e5a8f5f3f15a7779e', '0682bf049f96fa603d50f0fdad0b79a5c55f6c97', '97d1ac71eed13d4f51f29aac0e1a554007907df8', 'c17b609b0b090d7e8f99de1445be04f8f66367d4', '53014cfb506f6fffb22577bf580ae6f4d5317ce5', 'fa30a938b58fc05131c3854f12efe376cbad887f', 'f875337f2ecd686cd7789e111174d0f14972638d', 'de53af4eddbc30c808d90b8a11a29217d377569e', 'dac087e1328e65ca08f66d8b5307d6624bf3943f', 'a1645d0ba50e4c29f0feb806521093e7b1459081', '3cd185b7adc835e1c4449eff81222f5fc15c8500', 'f03112b868b658c954db62fc64430bebbaa7d9e0', '5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4', 'a6d3e57de796172c236e33a6ceb4cca793dc2315', '395b61d368e8766014aa960fde0192e4196bcb85', '92bb41cf7bd1f7886784796a8220ed5aa07bc49b', '4ef11518b40cc55d86c485f14e24732123b0d907', '6a219d7c58451842aa5d6819a7cdf51c55e9fc0f', 'cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad', 'f8f4e4a50d2b3fbd193327e79ea32d8d057e1414', 'bc84c5a58c57038910f7720d7a784560054d3e1a', '29923a824c98b3ba85ced964a0e6a2af35758abe', '559c68802ee2bb8b11e2188127418ca3a6155ba7', '8dc707a0daf7bff61a97d9d854283e65c0c85064', 'ffde866b1203a01580eb33237a0bb9da71c75ecf', '6cd8bad8a031ce6d802ded90f9754088e0c8d653', '30eacb4595014c9c0e5ee9669103d003cfdfe1e5', '0f7867f888109b9e000ef68965df4dde2511a55f', 'e2e977d7222654ee8d983fd8ba63b930e9a5a691', '0cfe0e33fbb100751fc0916001a5a19498ae8cb5', '35b3ce3a7499070e9b280f52e2cb0c29b0745380', '71ba1b09bb03f5977d790d91702481cc406b3767', '612c3675b6c55b60ae6d24265ed8e20f62cb117e', 'bd40f33452da7711b65faaa248aca359b27fddb6', '787c4d4628eac00dbceb1c96020bff0090edca46', '559920ebe19e99e43418c2f0455a0ffdc8edaaa2', '43f56301c5d2f50b6449b582652f2351cbe90e70', '68cd8433a75eee7d100dbbfedebbf53873a21720', '0a01341ddca3fe4c8727be7cf5841091c6ca3d0b', 'a8c862d8c5da3f060eedf3395a3d0a97fd8a12d8', '3c3807f226ba72fc41f59f0338f12a49a0c35605', 'c70bafc35e27be9d1efae60596bc0dd390c124c0', '81d607fc206198162faa54a796717c2805282d9b', '51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad', 'f0848e7a339da0828278f6803ed7990366c975f0', 'b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8', '792f6d76d2befba2af07198584aac1b189583ae4', '127d5ddfabec5c58832e5865cbd8ed0978c25a13', 'b91671715ad4fad56c67c28ce6f29e180fe08595', 'a6d37b5975050da0b1959232ae756fc09e5f87e8', 'e82fa03f1638a8c59ceb62bb9a6b41b498950e1f', '7ab9c0b4ceca1c142ff068f85015a249b14282d0', '00050f7365e317dc0487e282a4c33804b58b1fb3', 'c5b0ed5db65051eebd858beaf303809aa815e8e5', '10fb7dc031075946153baf0a0599e126de29e3a4', 'e438445cf823893c841b2bc26cdce32ccc3f5cbe', '12f7fac818f0006cf33269c9eafd41bbb8979a48', 'd5a8fd8bb48dd1f75927e874bdea582b4732a0cd', '1097768b89f8bd28d6ef6443c94feb04c1a1318e', 'fc1679c714eab822431bbe96f0e9cf4079cd8b8d', '23e2971c962bb6486bc0a66ff04242170dd22a1d', 'c9bc6f53b941863e801280343afa14248521ce43', '07b70b2b799b9efa630e8737df8b1dd1284f032c', '71a0c4f19be4ce1b1bae58a6e8f2a586e125d074', 'c2eb743c9d0baf1781c3c0df9533fab588250af3', 'c35806cf68220b2b9bb082b62f493393b9bdff86', 'f7d0fa52017a642a9f70091a252857fccca31f12', '01209a3bead7c87bcdc628be2a5a26b41abde9d1', '2740e3d7d33173664c1c5ab292c7ec75ff6e0802', 'db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21', '48bd71477d5f89333fa7ce5c4556e4d950fb16ed', '76ed74788e3eb3321e646c48ae8bf6cdfe46dca1', 'ad1be65c4f0655ac5c902d17f05454c0d4c4a15d', '2eb9280d72cde9de3aabbed993009a98a5fe0990', '154a721ccc1d425688942e22e75af711b423e086', '84bad9a821917cb96584cf5383c6d2a035358d7c', 'c9305e5794b65b33399c22ac8e4e024f6b757a30', '56b7319be68197727baa7d498fa38af0a8440fe4', '2268c9044e868ba0a16e92d2063ada87f68b5d03', '6b7354d7d715bad83183296ce2f3ddf2357cb449', 'e949b28f6d1f20e18e82742e04d68158415dc61e', 'a1ac2a152710335519c9a907eec60d9f468b19db', 'ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25', '49eb52b3ec0647e165a5e41488088c80a20cc78f', '9bb7ae50bff91571a945c1af025ed2e67714a788', '81dbe9a9ddaa5d02b02e01a306d898015a56ffb6', '348886b4762db063711ef8b7a10952375fbdcb57', '1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b', 'f9aa055bf73185ba939dfb03454384810eb17ad1', 'd571e0b0f402a3d36fb30d70cdcd2911df883bc7', 'ce2b921e4442a21555d65d8ce4ef7e3bde931dfc', '2275b0e195cd9cb25f50c5c570da97a4cce5dca8', '37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6', 'd01c51155e4719bf587d114bcd403b273c77246f', '9b4dc790e4ff49562992aae4fad3a38621fadd8b', 'a1dac888f63c9efaf159d9bdfde7c938636f07b1', '1e4dbfc556cf237accb8b370de2f164fa723687b', 'fff5c24dca92bc7d5435a2600e6764f039551787', 'b2ecfd5480a2a4be98730e2d646dfb84daedab17', 'a3efe43a72b76b8f5e5111b54393d00e6a5c97ab', 'f1e90a553a4185a4b0299bd179f4f156df798bce', '19b7312cfdddb02c3d4eaa40301a67143a72a35a', '22744c3bc68f120669fc69490f8e539b09e34b94', 'dcea88698949da4a1bd00277c06df06c33f6a5ff', 'd7b60abb0091246e29d1a9c28467de598e090c20', 'bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e', '5a6926de13a8cc25ce687c22741ba97a6e63d4ee', 'dcc1115aeaf87118736e86f3e3eb85bf5541281c', 'c74185bced810449c5f438f11ed6a578d1e359b4', '88e5d37617e14d6976cc602a168332fc23644f19', '45f7c03a686b68179cadb1413c5f3c1d373328bd', 'a2015f02dfb376bf9b218d1c897018f4e70424d7', 'f697d00a82750b14376fe20a5a2b249e98bebe9b', 'e0e379e546f1da9da874a2e90c79b41c60feb817', '70148c8d0f345ea36200d5ba19d021924d98e759', '27cf16bc9ef71761b9df6217f00f39f21130ce15', '627b8d7b5b985394428c974aca5ba0c1bbbba377', '126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0', '7e4ef0a4debc048b244b61b4f7dc2518b5b466c0', 'b68f72aed961d5ba152e9dc50345e1e832196a76', 'cf874cd9023d901e10aa8664b813d32501e7e4d2', '42084c41343e5a6ae58a22e5bfc5ce987b5173de', 'b637d6393ef3af7462917b81861531022b291933', '8b9c12df9f89040f1485b3847a29f11b5c9262e0', '72e4e26d0dd79c590c28b10938952a9f9497ff1e', '63b92dcc701ec77fdb3355ede5d37d2fbf057bcc', '58ee0cbf1d8e3711c617b1cd3d7aca8620e26187', 'f71b52e00e0be80c926f153b9fe0a06dd93af11e', '8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe', '54e945ea4b014e11ed4e1e61abc2aa9e68fea310', 'df0257ab04686ddf1c6c4d9b0529a7632330b98e', '568fb7989a133564d84911e7cb58e4d8748243ef', '2c947447d81252397839d58c75ebcc71b34379b5', 'c01784b995f6594fdb23d7b62f20a35ae73eaa77', '3415762847ed13acc3c90de60e3ef42612bc49af', '223dc2b9ea34addc0f502003c2e1c1141f6b36a7', 'e1ab11885f72b4658263a60751d956ba661c1d61', 'c85b6f9bafc4c64fc538108ab40a0590a2f5768e', '8e52637026bee9061f9558178eaec08279bf7ac6', '0f6216b9e4e59252b0c1adfd1a848635437dfcdc', '22ccee453e37536ddb0c1c1d17b0dbac04c6c607', 'd00bbeda2a45495e6261548710afa6b21ea32870', '71b1af123fe292fd9950b8439db834212f0b0e32', 'a616a3f0d244368ec588f04dfbc37d77fda01b4c', '8e44c02c2d9fa56fb74ace35ee70a5add50b52ae', '1522ccedbb1f668958f24cca070f640274bc2549', '97466a37525536086ed5d6e5ed143df085682318', 'e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a', 'd6191c4643201262a770947fc95a613f57bedb6b', 'ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8', 'fc4ae12576ea3a85ea6d150b46938890d63a7d18', '19cf7884c0c509c189b1e74fe92c149ff59e444b', 'ecd5770cf8cb12cb34285e26ab834301c17c53e1', '4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729', '5529f26f72ce47440c2a64248063a6d5892b9fde', 'f85ca6135b101736f5c16c5b5d40895280016023', '5fa36dc8f7c4e65acb962fc484989d20b8fdaeec', 'd98847340e46ffe381992f1a594e75d3fb8d385e', '7006c66a15477b917656f435d66f63760d33a304', 'a15bc19674d48cd9919ad1cf152bf49c88f4417d', '440faf8d0af8291d324977ad0f68c8d661fe365e', '0ec56e15005a627d0b478a67fd627a9d85c3920e', 'a712718e6596ba946f29a99838d82f95b9ebb1ce', '3116453e35352a3a90ee5b12246dc7f2e60cfc59', 'dfca00be3284cc555a6a4eac4831471fb1f5875b', 'a9a532399237b514c1227f2d6be8601474e669be', '26126068d72408555bcb52977cd669faf660bdf7', '660284b0a21fe3801e64dc9e0e51da5400223fe3', 'c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd', 'f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3', 'a253749e3b4c4f340778235f640ce694642a4555', '1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891', '777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47', '2da4c3679111dd92a1d0869dae353ebe5989dfd2', 'b7c3f3942a07c118e57130bc4c3ec4adc431d725', 'a5505e25ee9ae84090e1442034ddbb3cedabcf04', '1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf', '7fa3c2c0cf7f559d43e84076a9113a390c5ba03a', '9a7ba5ed1779c664d2cac92494a43517d3e87c96', '662870a90890c620a964720b2ca122a1139410ea', '92d1a6df3041667dc662376938bc65527a5a1c3c', '12159f04e0427fe33fa05af6ba8c950f1a5ce5ea', 'a4a1fcef760b133e9aa876ac28145ad98a609927', '63bb2040fa107c5296351c2b5f0312336dad2863', '01f4a0a19467947a8f3bdd7ec9fac75b5222d710', '7784d321ccc64db5141113b6783e4ba92fdd4b20', '218615a005f7f00606223005fef22c07057d9d77', '867290103f762e1ddfa6f2ea30dd0a327f595182', '907b3af3cfaf68fe188de9467ed1260e52ec6cf1', '56a8826cbee49560592b2d4b47b18ada236a12b9', '968b7c3553a668ba88da105eff067d57f393c63f', 'f03df5d99b753dc4833ef27b32bb95ba53d790ee', 'a8f51b4e334a917702422782329d97304a2fe139', 'dca86fbe1d57b44986055b282a03c15ef7882e51', '6f024d4c952fbb53b6f8cf27020659aa4d9a6590', 'd2a0142150cc3788475572f82458ef88087bd7ac', '27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa', 'b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72', '808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc', '36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd', 'f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6', '357eb9f0c07fa45e482d998a8268bd737beb827f', 'ad08b215dca538930ef1f50b4e49cd25527028ad', '31101dc9937f108e27e08a5f34be44f0090b8b6b', 'e4a315e9c190cf96493eefe04ce4ba6ae6894550', '6263b2cba18207474786b303852d2f0d7068d4b6', 'c1c44fd96c3fa6e16949ae8fa453e511c6435c68', 'd28d86524292506d4b24ae2d486725a6d57a3db3', 'feafcc1c4026d7f55a2c8ce7850d7e12030b5c22', '63488da6c7aff9e374561a24ba224e9ce7f65e40', 'c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c', 'a9337636b52de375c852682a2561af2c1db5ec63', '45a5961a4e1d1c22874c4918e5c98bd3c0a670b3', '30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1', '5c6fa86757410aee6f5a0762328637de03a569e9', '7e38e0279a620d3df05ab9b5e2795044f18d4471', '8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92', '03ebb29c08375afc42a957c7b2dc1a42bed7b713', '9cf070d6671ee4a6353f79a165aa648309e01295', '87bc6f83f7f90df3c6c37659139b92657c3f7a38', '01e2d10178347d177519f792f86f25575106ddc7', '021bfb7e180d67112b74f05ecb3fa13acc036c86', 'd201b9992809142fe59ae74508bc576f8ca538ff', 'c4628d965983934d7a2a9797a2de6a411629d5bc', 'bd419f4094186a5ce74ba6ac1622b24e29e553f4', '11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af', '1269c5d8f61e821ee0029080c5ba2500421d5fa6', 'e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb', 'b9ea841b817ba23281c95c7a769873b840dee8d5', '219af68afeaecabdfd279f439f10ba7c231736e4', 'a66a275a817f980c36e0b67d2e00bd823f63abf8', 'b6f466e0fdcb310ecd212fd90396d9d13e0c0504', '62ea141d0fb342dfb97c69b49d1c978665b93b3c', 'a32c792a0cef03218bf66322245677fc2d5e5a31', '0101ebfbaba75fd47868ad0c796ac44ebc19c566', '50cb50657572e315fd452a89f3e0be465094b66f', '981fd79dd69581659cb1d4e2b29178e82681eb4d', '03e9ac1a2d90152cd041342a11293a1ebd33bcc3', 'ef396a34436072cb3c40b0c9bc9179fee4a168ae', '04bde1d2b445f971e97bb46ade2d0290981c7a32', 'bfbd6040cb95b179118557352e8e3899ef25c525', 'd6e353e0231d09fd5dcba493544d53706f3fe1ab', '7bd6a6ec230e1efb27d691762cc0674237dc7967', '6aaf12505add25dd133c7b0dafe8f4fe966d1f1d', '73906462bd3415f23d6378590a5ba28709b17605', '5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11', '88bf368491f9613767f696f84b4bb1f5a7d7cb48', '0737954caf66f2b4c898b356d2a3c43748b9706b', '664b3eadc12c8dde309e8bbd59e9af961a433cde', 'b3307d5b68c57a074c483636affee41054be06d1', 'bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31', '12d7055baf5bffb6e9e95e977c000ef2e77a4362', '498c0229f831c82a5eb494cdb3547452112a66a0', '8c48c726bb17a17d70ab29db4d65a93030dd5382', '89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f', '06b5272774ec43ee5facfa7111033386f06cf448', '08b57deb237f15061e4029b6718f1393fa26acce', '9b7655d39c7a19a23eb8944568eb5618042b9026', 'cd06d775f491b4a17c9d616a8729fd45aa2e79bf', '1329280df5ee9e902b2742bde4a97bc3e6573ff3', '58c6737070ef559e9220a8d08adc481fdcd53a24', '0af16b164db20d8569df4ce688d5a62c861ace0b', '78a4ec72d76f0a736a4a01369a42b092922203b6', '6a14379fee26a39631aebd0e14511ce3756e42ad', '81588e0e207303c2867c896f3911a54a1ef7c874', 'dd09db5eb321083dba16c2550676e60682f9a0cd', '40c0f97c3547232d6aa039fcb330f142668dea4b', '777217e025132ddc173cf33747ee590628a8f62f', '2dbf6fe095cd879a9bf40f110b7b72c8bdde9475', '7d483077ed7f2f504d59f4fc2f162741fa5ac23b', 'de830c534c23f103288c198eb19174c76bfd38a1', 'b0d66760829f111b8fad0bd81ca331ddd943ef41', 'ae7c93646aa5f3206cd759904965b4d484d12f83', 'd1ec42b2b5a3c956ff528543636e024bfde5e5ba', '1dac4bc5af239024566fcb0f43bb9ff1c248ecec', '3bf0306e9bd044f723e38170c13455877b2aeec3', '2858620e0498db2f2224bfbed5263432f0570832', '545e92833b0ad4ba32eac5997edecf97a366a244', 'cb12c19f9d14bef7b2f778892d9071eea2d6c63d', '9193006f359c53eb937deff1248ee3317978e576', 'bc67b91dd73acded2d52fd4fee732b7a9722ea8b', '49c32a2a64eb41381e5f12ccea4150cac9f3303d', 'bbb77f2d6685c9257763ca38afaaef29044b4018', '22732cb9476e521452bf0538f3fdb94cf3867651', '4e748cb2b5e74d905d9b24b53be6cfdf326e8054', '74b338d5352fe1a6fd592e38269a4c81fe79b866', 'd6ea7a30b0b61ae126b00b59d2a14fff2ef887bf', 'f903396d943541a8cc65edefb04ca37814ed30dd', 'ba28ce9a2f7e8524243adf288cc3f11055e667bb', '975e60535724f4149c7488699a199ba2920a062c', 'b970f48d30775d3468952795bc72976baab3438e'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_answers_and_evidence.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'BIBREF19, BIBREF20',\n",
       "  'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "  'type': 'extractive'},\n",
       " {'answer': 'multilingual NMT (MNMT) BIBREF19',\n",
       "  'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "  'type': 'extractive'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_answers_and_evidence['b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers_and_evidence = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:07<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "for instance in tqdm(instances):\n",
    "    question_id = instance[\"metadata\"][\"question_id\"]\n",
    "\n",
    "    if question_id in predicted_answers_and_evidence: # keep this to conserve API requests\n",
    "        continue\n",
    "\n",
    "    # prediction_data = json.loads(line)\n",
    "    # pred_answer = qasper.predict(instance)[0]\n",
    "    pred_answer = gpt35.predict(instance)\n",
    "\n",
    "    predicted_answers_and_evidence[question_id] = {\n",
    "        \"answer\": pred_answer,\n",
    "        # \"evidence\": prediction_data[\"predicted_evidence\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "import json\n",
    "with open('output/gpt35-predictions.json', 'w') as f:\n",
    "    json.dump(predicted_answers_and_evidence, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output = evaluate(\n",
    "    {k:v for k, v in gold_answers_and_evidence.items() \\\n",
    "        if k in predicted_answers_and_evidence}, \n",
    "    predicted_answers_and_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_answers_and_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Answer F1': 0.06489175253739486,\n",
       " 'Answer F1 by type': {'extractive': 0.08545044887734807,\n",
       "  'abstractive': 0.08790371215230747,\n",
       "  'boolean': 0.0013445793337097684,\n",
       "  'none': 0.0},\n",
       " 'Missing predictions': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(evaluation_output, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
